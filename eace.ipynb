{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cacaf10e-8e01-4932-8438-9ae9ffedbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files:\n",
      "  eon_paths_timeseries.csv  ->  OK\n",
      "  eon_links_timeseries.csv  ->  OK\n",
      "  eon_target_fewshot.csv  ->  OK\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ⬇️ Change this if your CSVs are in another folder\n",
    "DATA_DIR = Path(\".\")            # e.g., Path(\"/path/to/your/folder\")\n",
    "\n",
    "PATHS_CSV   = DATA_DIR / \"eon_paths_timeseries.csv\"\n",
    "LINKS_CSV   = DATA_DIR / \"eon_links_timeseries.csv\"\n",
    "FEWSHOT_CSV = DATA_DIR / \"eon_target_fewshot.csv\"\n",
    "\n",
    "OUT_DIR = Path(\"./outputs\")\n",
    "(OUT_DIR / \"figs\").mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"metrics\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Looking for files:\")\n",
    "for p in [PATHS_CSV, LINKS_CSV, FEWSHOT_CSV]:\n",
    "    print(f\"  {p}  ->  {'OK' if p.exists() else 'NOT FOUND'}\")\n",
    "\n",
    "# Hard stop if any file is missing\n",
    "assert PATHS_CSV.exists() and LINKS_CSV.exists() and FEWSHOT_CSV.exists(), \"Place all 3 CSVs in DATA_DIR.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb70ed39-9543-48aa-af64-1b3df62d056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shapes:\n",
      "  paths : (10800, 24)\n",
      "  links : (5310, 16)\n",
      "  fewshot: (600, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topology</th>\n",
       "      <th>day</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>path</th>\n",
       "      <th>hops</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>avg_utilization</th>\n",
       "      <th>min_osnr_db</th>\n",
       "      <th>...</th>\n",
       "      <th>symbol_rate_gbaud</th>\n",
       "      <th>bitrate_gbps</th>\n",
       "      <th>req_osnr_db</th>\n",
       "      <th>osnr_margin_db</th>\n",
       "      <th>est_ber</th>\n",
       "      <th>qot_ok</th>\n",
       "      <th>failure_present</th>\n",
       "      <th>failure_type</th>\n",
       "      <th>fail_link</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSFNET</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1-&gt;8-&gt;10-&gt;11-&gt;12-&gt;13</td>\n",
       "      <td>5</td>\n",
       "      <td>1403.889557</td>\n",
       "      <td>7.019448</td>\n",
       "      <td>0.787454</td>\n",
       "      <td>22.537527</td>\n",
       "      <td>...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>2.737527</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train_source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSFNET</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>14-&gt;11-&gt;10-&gt;8-&gt;1</td>\n",
       "      <td>4</td>\n",
       "      <td>1032.394401</td>\n",
       "      <td>5.161972</td>\n",
       "      <td>0.795083</td>\n",
       "      <td>22.537527</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>10.037527</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train_source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSFNET</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5-&gt;7-&gt;8</td>\n",
       "      <td>2</td>\n",
       "      <td>811.524544</td>\n",
       "      <td>4.057623</td>\n",
       "      <td>0.636687</td>\n",
       "      <td>23.277962</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>10.777962</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train_source</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  topology  day  src  dst                  path  hops  distance_km  \\\n",
       "0   NSFNET    0    1   13  1->8->10->11->12->13     5  1403.889557   \n",
       "1   NSFNET    0   14    1      14->11->10->8->1     4  1032.394401   \n",
       "2   NSFNET    0    5    8               5->7->8     2   811.524544   \n",
       "\n",
       "   latency_ms  avg_utilization  min_osnr_db  ...  symbol_rate_gbaud  \\\n",
       "0    7.019448         0.787454    22.537527  ...               64.0   \n",
       "1    5.161972         0.795083    22.537527  ...               32.0   \n",
       "2    4.057623         0.636687    23.277962  ...               40.0   \n",
       "\n",
       "   bitrate_gbps  req_osnr_db osnr_margin_db   est_ber  qot_ok  \\\n",
       "0         256.0         19.8       2.737527  0.003510       1   \n",
       "1          64.0         12.5      10.037527  0.000012       1   \n",
       "2          80.0         12.5      10.777962  0.000007       1   \n",
       "\n",
       "   failure_present  failure_type  fail_link         split  \n",
       "0                0          none        NaN  train_source  \n",
       "1                0          none        NaN  train_source  \n",
       "2                0          none        NaN  train_source  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topology</th>\n",
       "      <th>day</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>edge_id</th>\n",
       "      <th>length_km</th>\n",
       "      <th>osnr_db</th>\n",
       "      <th>snr_db</th>\n",
       "      <th>signal_dbm</th>\n",
       "      <th>noise_dbm</th>\n",
       "      <th>center_freq_offset_ghz</th>\n",
       "      <th>filter_bw_scale</th>\n",
       "      <th>bandwidth_utilization</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>failure_active</th>\n",
       "      <th>failure_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSFNET</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1-2</td>\n",
       "      <td>474.507123</td>\n",
       "      <td>23.855619</td>\n",
       "      <td>20.355619</td>\n",
       "      <td>-1.05308</td>\n",
       "      <td>-24.908699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.787887</td>\n",
       "      <td>2.372536</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSFNET</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1-2</td>\n",
       "      <td>474.507123</td>\n",
       "      <td>23.923705</td>\n",
       "      <td>20.423705</td>\n",
       "      <td>-1.05308</td>\n",
       "      <td>-24.976784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920905</td>\n",
       "      <td>2.372536</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSFNET</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1-2</td>\n",
       "      <td>474.507123</td>\n",
       "      <td>24.722090</td>\n",
       "      <td>21.222090</td>\n",
       "      <td>-1.05308</td>\n",
       "      <td>-25.775170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.782731</td>\n",
       "      <td>2.372536</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topology  day  u  v edge_id   length_km    osnr_db     snr_db  signal_dbm  \\\n",
       "0   NSFNET    0  1  2     1-2  474.507123  23.855619  20.355619    -1.05308   \n",
       "1   NSFNET    1  1  2     1-2  474.507123  23.923705  20.423705    -1.05308   \n",
       "2   NSFNET    2  1  2     1-2  474.507123  24.722090  21.222090    -1.05308   \n",
       "\n",
       "   noise_dbm  center_freq_offset_ghz  filter_bw_scale  bandwidth_utilization  \\\n",
       "0 -24.908699                     0.0              1.0               0.787887   \n",
       "1 -24.976784                     0.0              1.0               0.920905   \n",
       "2 -25.775170                     0.0              1.0               0.782731   \n",
       "\n",
       "   latency_ms  failure_active failure_type  \n",
       "0    2.372536               0         none  \n",
       "1    2.372536               0         none  \n",
       "2    2.372536               0         none  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topology</th>\n",
       "      <th>day</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>path</th>\n",
       "      <th>hops</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>avg_utilization</th>\n",
       "      <th>min_osnr_db</th>\n",
       "      <th>...</th>\n",
       "      <th>symbol_rate_gbaud</th>\n",
       "      <th>bitrate_gbps</th>\n",
       "      <th>req_osnr_db</th>\n",
       "      <th>osnr_margin_db</th>\n",
       "      <th>est_ber</th>\n",
       "      <th>qot_ok</th>\n",
       "      <th>failure_present</th>\n",
       "      <th>failure_type</th>\n",
       "      <th>fail_link</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GEANT2</td>\n",
       "      <td>62</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>7-&gt;19-&gt;14-&gt;13</td>\n",
       "      <td>3</td>\n",
       "      <td>776.881395</td>\n",
       "      <td>3.884407</td>\n",
       "      <td>0.768215</td>\n",
       "      <td>21.194588</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-3.805412</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test_target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEANT2</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>14-&gt;13</td>\n",
       "      <td>1</td>\n",
       "      <td>104.679224</td>\n",
       "      <td>0.523396</td>\n",
       "      <td>0.676073</td>\n",
       "      <td>26.576763</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.776763</td>\n",
       "      <td>0.007348</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test_target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEANT2</td>\n",
       "      <td>63</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>23-&gt;19-&gt;14-&gt;5-&gt;4-&gt;22</td>\n",
       "      <td>5</td>\n",
       "      <td>1153.974412</td>\n",
       "      <td>5.769872</td>\n",
       "      <td>0.728540</td>\n",
       "      <td>21.304553</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-3.695447</td>\n",
       "      <td>0.009914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test_target</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  topology  day  src  dst                  path  hops  distance_km  \\\n",
       "0   GEANT2   62    7   13         7->19->14->13     3   776.881395   \n",
       "1   GEANT2    6   14   13                14->13     1   104.679224   \n",
       "2   GEANT2   63   23   22  23->19->14->5->4->22     5  1153.974412   \n",
       "\n",
       "   latency_ms  avg_utilization  min_osnr_db  ...  symbol_rate_gbaud  \\\n",
       "0    3.884407         0.768215    21.194588  ...               32.0   \n",
       "1    0.523396         0.676073    26.576763  ...               40.0   \n",
       "2    5.769872         0.728540    21.304553  ...               32.0   \n",
       "\n",
       "   bitrate_gbps  req_osnr_db osnr_margin_db   est_ber  qot_ok  \\\n",
       "0         192.0         25.0      -3.805412  0.009921       0   \n",
       "1         240.0         25.8       0.776763  0.007348       0   \n",
       "2         192.0         25.0      -3.695447  0.009914       0   \n",
       "\n",
       "   failure_present  failure_type  fail_link        split  \n",
       "0                0          none        NaN  test_target  \n",
       "1                0          none        NaN  test_target  \n",
       "2                0          none        NaN  test_target  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "paths = pd.read_csv(PATHS_CSV)\n",
    "links = pd.read_csv(LINKS_CSV)\n",
    "few   = pd.read_csv(FEWSHOT_CSV)\n",
    "\n",
    "print(\"Loaded shapes:\")\n",
    "print(\"  paths :\", paths.shape)\n",
    "print(\"  links :\", links.shape)\n",
    "print(\"  fewshot:\", few.shape)\n",
    "\n",
    "display(paths.head(3))\n",
    "display(links.head(3))\n",
    "display(few.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd920b9-9bc7-4aa8-ad9e-04b1586a4000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\devon\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\devon\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.0 MB 375.2 kB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 604.1 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 689.4 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.4/11.0 MB 983.5 kB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 3.9/11.0 MB 1.4 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.0/11.0 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.5/11.0 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.0/11.0 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.3/11.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.4/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.9/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 2.1 MB/s  0:00:05\n",
      "Downloading numpy-2.3.2-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/12.8 MB 5.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.6/12.8 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.8 MB 6.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.0/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.0/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.5/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.0/12.8 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.3/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.9/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.2/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 4.6 MB/s  0:00:02\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ---------------------------------------- 4/4 [pandas]\n",
      "\n",
      "Successfully installed numpy-2.3.2 pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f5a812-4321-4d07-810b-f253dda7267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split counts (paths):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_target</th>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_source</th>\n",
       "      <td>4560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_source</th>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rows\n",
       "split             \n",
       "test_target   5400\n",
       "train_source  4560\n",
       "val_source     840"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance (paths):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qot_ok': {1: 8323, 0: 2477},\n",
       " 'failure_present': {0: 9577, 1: 1223},\n",
       " 'failure_type': {'none': 9577, 'shift': 661, 'tighten': 562}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def split_counts(df):\n",
    "    return df['split'].value_counts().to_frame('rows')\n",
    "\n",
    "def class_balance(df):\n",
    "    return {\n",
    "        'qot_ok': df['qot_ok'].value_counts().to_dict(),\n",
    "        'failure_present': df['failure_present'].value_counts().to_dict(),\n",
    "        'failure_type': df['failure_type'].value_counts().to_dict(),\n",
    "    }\n",
    "\n",
    "print(\"Split counts (paths):\")\n",
    "display(split_counts(paths))\n",
    "\n",
    "print(\"Class balance (paths):\")\n",
    "display(class_balance(paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05325fe-fe9f-417a-8cda-ecaa7761f5b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping sklearn as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\devon\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.7 MB 1.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/8.7 MB 1.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.6/8.7 MB 1.7 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.8/8.7 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.4/8.7 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.1/8.7 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.9/8.7 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.7/8.7 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.5/8.7 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.0/8.7 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.6/8.7 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.3/8.7 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.7 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 2.7 MB/s  0:00:03\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.1-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/38.5 MB 6.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.4/38.5 MB 5.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.9/38.5 MB 6.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 5.2/38.5 MB 6.4 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 6.8/38.5 MB 6.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 8.4/38.5 MB 6.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 10.0/38.5 MB 6.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.5/38.5 MB 6.9 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 12.1/38.5 MB 6.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 12.6/38.5 MB 6.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 13.9/38.5 MB 6.1 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 15.2/38.5 MB 6.1 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 16.8/38.5 MB 6.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 18.1/38.5 MB 6.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 18.9/38.5 MB 6.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 18.9/38.5 MB 6.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 20.2/38.5 MB 5.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 21.2/38.5 MB 5.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 23.3/38.5 MB 5.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 24.9/38.5 MB 6.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 25.7/38.5 MB 5.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 27.0/38.5 MB 5.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.0/38.5 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 29.9/38.5 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.5/38.5 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.3/38.5 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.6/38.5 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 35.9/38.5 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.7/38.5 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.5 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.5/38.5 MB 6.1 MB/s  0:00:06\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ---------------------------------------- 4/4 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y sklearn\n",
    "%pip install -U scikit-learn\n",
    "\n",
    "# verify\n",
    "import sklearn, sys\n",
    "print(\"sklearn version:\", sklearn.__version__)\n",
    "print(\"python exe:\", sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f57adff-4d85-4789-9016-1bb8718ee752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QoT baseline (NSFNET→GEANT2 zero-shot):\n",
      "  Accuracy: 0.9792592592592593\n",
      "  F1 score: 0.972386542384192\n",
      "  AUC: 0.99828865139197\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# --- Split helper\n",
    "def get_splits(df):\n",
    "    train = df[df['split']==\"train_source\"].copy()\n",
    "    val   = df[df['split']==\"val_source\"].copy()\n",
    "    test  = df[df['split']==\"test_target\"].copy()\n",
    "    return train, val, test\n",
    "\n",
    "# --- Feature maker (numerics + modulation code)\n",
    "NUM_COLS = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps','req_osnr_db','osnr_margin_db'\n",
    "]\n",
    "CAT_COLS = ['modulation']\n",
    "\n",
    "def make_xy(df):\n",
    "    X = df[NUM_COLS + CAT_COLS].copy()\n",
    "    X['modulation'] = X['modulation'].astype('category').cat.codes\n",
    "    y = df['qot_ok'].astype(int).values\n",
    "    return X, y\n",
    "\n",
    "# --- Train/eval\n",
    "train, val, test = get_splits(paths)\n",
    "Xtr, ytr = make_xy(train)\n",
    "Xva, yva = make_xy(val)\n",
    "Xte, yte = make_xy(test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xtr = scaler.fit_transform(Xtr)\n",
    "Xva = scaler.transform(Xva)\n",
    "Xte = scaler.transform(Xte)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                    alpha=1e-4, learning_rate_init=1e-3,\n",
    "                    max_iter=200, random_state=42)\n",
    "clf.fit(Xtr, ytr)\n",
    "\n",
    "yte_pred = clf.predict(Xte)\n",
    "yte_proba = clf.predict_proba(Xte)[:,1]\n",
    "\n",
    "print(\"QoT baseline (NSFNET→GEANT2 zero-shot):\")\n",
    "print(\"  Accuracy:\", accuracy_score(yte, yte_pred))\n",
    "print(\"  F1 score:\", f1_score(yte, yte_pred, average=\"macro\"))\n",
    "print(\"  AUC:\", roc_auc_score(yte, yte_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31af3160-527c-48de-8421-27915f285a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure Detection (NSFNET→GEANT2 zero-shot):\n",
      "val_source: acc=1.0000, macro-F1=1.0000, bal-acc=1.0000\n",
      "test_target (GEANT2): acc=0.9954, macro-F1=0.9907, bal-acc=0.9915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devon\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:534: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# NSFNET→GEANT2 zero-shot failure_present (0/1) with safer preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# --- helper: splits from `paths`\n",
    "def get_splits(df):\n",
    "    train = df[df['split']==\"train_source\"].copy()\n",
    "    val   = df[df['split']==\"val_source\"].copy()\n",
    "    test  = df[df['split']==\"test_target\"].copy()\n",
    "    return train, val, test\n",
    "\n",
    "# --- features\n",
    "NUM_COLS = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps','req_osnr_db','osnr_margin_db'\n",
    "]\n",
    "CAT_COLS = ['modulation']\n",
    "\n",
    "# --- split\n",
    "train, val, test = get_splits(paths)\n",
    "\n",
    "Xtr, ytr = train[NUM_COLS + CAT_COLS], train['failure_present'].astype(int).values\n",
    "Xva, yva = val[NUM_COLS + CAT_COLS],   val['failure_present'].astype(int).values\n",
    "Xte, yte = test[NUM_COLS + CAT_COLS],  test['failure_present'].astype(int).values\n",
    "\n",
    "# --- preprocessors (no leakage; OHE is robust to unseen categories)\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), NUM_COLS),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), CAT_COLS),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# --- model (early stopping uses an internal split on the training data)\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 256),\n",
    "    activation=\"relu\",\n",
    "    alpha=1e-4,\n",
    "    learning_rate_init=1e-3,\n",
    "    max_iter=400,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20,\n",
    "    validation_fraction=0.15\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", clf)])\n",
    "\n",
    "# (optional) handle class imbalance on train via sample weights\n",
    "wtr = compute_sample_weight(class_weight=\"balanced\", y=ytr)\n",
    "\n",
    "# --- fit on source (NSFNET)\n",
    "pipe.fit(Xtr, ytr, clf__sample_weight=wtr)\n",
    "\n",
    "# --- evaluate on source-val and target-test (GEANT2)\n",
    "def eval_and_print(name, X, y):\n",
    "    y_pred = pipe.predict(X)\n",
    "    print(f\"{name}: \"\n",
    "          f\"acc={accuracy_score(y, y_pred):.4f}, \"\n",
    "          f\"macro-F1={f1_score(y, y_pred, average='macro'):.4f}, \"\n",
    "          f\"bal-acc={balanced_accuracy_score(y, y_pred):.4f}\")\n",
    "\n",
    "print(\"Failure Detection (NSFNET→GEANT2 zero-shot):\")\n",
    "eval_and_print(\"val_source\", Xva, yva)\n",
    "eval_and_print(\"test_target (GEANT2)\", Xte, yte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d76342b-769b-4ac2-a7ad-85e9322c6685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance on GEANT2 test (failure_present):\n",
      "{0: 4610, 1: 790} \n",
      "\n",
      "A) With fingerprints  -> Acc=0.9969  Macro-F1=0.9937\n",
      "Confusion matrix (A):\n",
      " [[4606    4]\n",
      " [  13  777]] \n",
      "\n",
      "B) Heuristic rule     -> Acc=0.9807  Macro-F1=0.9634\n",
      "Confusion matrix (B):\n",
      " [[4506  104]\n",
      " [   0  790]] \n",
      "\n",
      "C) Drop fingerprints  -> Acc=0.8385  Macro-F1=0.4880\n",
      "Confusion matrix (C):\n",
      " [[4498  112]\n",
      " [ 760   30]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devon\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: why detection is ~perfect, and how it changes if we drop fingerprint features.\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- splits\n",
    "def get_splits(df):\n",
    "    tr = df[df['split']==\"train_source\"].copy()\n",
    "    va = df[df['split']==\"val_source\"].copy()\n",
    "    te = df[df['split']==\"test_target\"].copy()\n",
    "    return tr, va, te\n",
    "\n",
    "train, val, test = get_splits(paths)\n",
    "\n",
    "# ----- helpers\n",
    "NUM_COLS_ALL = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps','req_osnr_db','osnr_margin_db'\n",
    "]\n",
    "CAT_COLS = ['modulation']\n",
    "\n",
    "def make_xy(df, drop_fingerprints=False):\n",
    "    cols = NUM_COLS_ALL.copy()\n",
    "    if drop_fingerprints:\n",
    "        # remove direct failure fingerprints\n",
    "        cols.remove('max_center_offset_ghz')\n",
    "        cols.remove('min_filter_bw_scale')\n",
    "    X = df[cols + CAT_COLS].copy()\n",
    "    X['modulation'] = X['modulation'].astype('category').cat.codes\n",
    "    y = df['failure_present'].astype(int).values\n",
    "    return X, y\n",
    "\n",
    "def train_eval(drop_fingerprints=False):\n",
    "    Xtr, ytr = make_xy(train, drop_fingerprints)\n",
    "    Xva, yva = make_xy(val, drop_fingerprints)\n",
    "    Xte, yte = make_xy(test, drop_fingerprints)\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(Xtr)\n",
    "    Xte = scaler.transform(Xte)\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                        alpha=1e-4, learning_rate_init=1e-3,\n",
    "                        max_iter=200, random_state=42)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    ypred = clf.predict(Xte)\n",
    "    acc = accuracy_score(yte, ypred)\n",
    "    f1  = f1_score(yte, ypred, average='macro')\n",
    "    return acc, f1, yte, ypred\n",
    "\n",
    "print(\"Class balance on GEANT2 test (failure_present):\")\n",
    "print(test['failure_present'].value_counts().to_dict(), \"\\n\")\n",
    "\n",
    "# (A) Model WITH fingerprints (what you ran)\n",
    "accA, f1A, y_true, y_predA = train_eval(drop_fingerprints=False)\n",
    "print(\"A) With fingerprints  -> Acc=%.4f  Macro-F1=%.4f\" % (accA, f1A))\n",
    "print(\"Confusion matrix (A):\\n\", confusion_matrix(y_true, y_predA), \"\\n\")\n",
    "\n",
    "# (B) Simple rule baseline (offset>0 OR scale<0.999)\n",
    "rule_pred = ((test['max_center_offset_ghz']>0).astype(int) | (test['min_filter_bw_scale']<0.999).astype(int)).values\n",
    "accR = accuracy_score(y_true, rule_pred)\n",
    "f1R  = f1_score(y_true, rule_pred, average='macro')\n",
    "print(\"B) Heuristic rule     -> Acc=%.4f  Macro-F1=%.4f\" % (accR, f1R))\n",
    "print(\"Confusion matrix (B):\\n\", confusion_matrix(y_true, rule_pred), \"\\n\")\n",
    "\n",
    "# (C) Model WITHOUT fingerprints (harder, more realistic if telemetry is incomplete)\n",
    "accC, f1C, _, y_predC = train_eval(drop_fingerprints=True)\n",
    "print(\"C) Drop fingerprints  -> Acc=%.4f  Macro-F1=%.4f\" % (accC, f1C))\n",
    "print(\"Confusion matrix (C):\\n\", confusion_matrix(y_true, y_predC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23e198f3-1cf0-4760-8e02-d38ff0637d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure Type (NSFNET→GEANT2 zero-shot):\n",
      "\n",
      "(A) Easy mode (WITH fingerprints)\n",
      "WITH fingerprints ⇒ Macro-F1=0.9085\n",
      "Confusion matrix (rows=GT, cols=Pred) order ['none', 'shift', 'tighten']\n",
      "[[4605    1    4]\n",
      " [   2  379   98]\n",
      " [   9    0  302]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        none     0.9976    0.9989    0.9983      4610\n",
      "       shift     0.9974    0.7912    0.8824       479\n",
      "     tighten     0.7475    0.9711    0.8448       311\n",
      "\n",
      "    accuracy                         0.9789      5400\n",
      "   macro avg     0.9142    0.9204    0.9085      5400\n",
      "weighted avg     0.9832    0.9789    0.9791      5400\n",
      "\n",
      "\n",
      "(B) Hard mode (DROP fingerprints)\n",
      "DROP fingerprints ⇒ Macro-F1=0.3272\n",
      "Confusion matrix (rows=GT, cols=Pred) order ['none', 'shift', 'tighten']\n",
      "[[4254   54  302]\n",
      " [ 444    0   35]\n",
      " [ 279    0   32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        none     0.8547    0.9228    0.8875      4610\n",
      "       shift     0.0000    0.0000    0.0000       479\n",
      "     tighten     0.0867    0.1029    0.0941       311\n",
      "\n",
      "    accuracy                         0.7937      5400\n",
      "   macro avg     0.3138    0.3419    0.3272      5400\n",
      "weighted avg     0.7347    0.7937    0.7630      5400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devon\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predict failure_type: {'none','shift','tighten'} on GEANT2 using a model trained on NSFNET.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# --- splits\n",
    "def get_splits(df):\n",
    "    tr = df[df['split']==\"train_source\"].copy()\n",
    "    va = df[df['split']==\"val_source\"].copy()\n",
    "    te = df[df['split']==\"test_target\"].copy()\n",
    "    return tr, va, te\n",
    "\n",
    "train, val, test = get_splits(paths)\n",
    "\n",
    "# --- target as categorical codes with fixed order\n",
    "ORDER = ['none','shift','tighten']\n",
    "cat_type = pd.api.types.CategoricalDtype(categories=ORDER, ordered=False)\n",
    "\n",
    "# --- features\n",
    "NUM_ALL = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps','req_osnr_db','osnr_margin_db'\n",
    "]\n",
    "CAT = ['modulation']\n",
    "\n",
    "def make_xy(df, drop_fingerprints=False):\n",
    "    cols = NUM_ALL.copy()\n",
    "    if drop_fingerprints:\n",
    "        cols.remove('max_center_offset_ghz')\n",
    "        cols.remove('min_filter_bw_scale')\n",
    "    X = df[cols + CAT].copy()\n",
    "    X['modulation'] = X['modulation'].astype('category').cat.codes\n",
    "    y = df['failure_type'].astype(cat_type).cat.codes.values  # 0=none,1=shift,2=tighten\n",
    "    return X, y\n",
    "\n",
    "def run(drop_fingerprints=False, seed=42):\n",
    "    Xtr, ytr = make_xy(train, drop_fingerprints)\n",
    "    Xva, yva = make_xy(val, drop_fingerprints)\n",
    "    Xte, yte = make_xy(test, drop_fingerprints)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(Xtr); Xva = scaler.transform(Xva); Xte = scaler.transform(Xte)\n",
    "\n",
    "    # class-balanced sample weights (important due to 'none' majority)\n",
    "    sw = compute_sample_weight(class_weight='balanced', y=ytr)\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                        alpha=1e-4, learning_rate_init=1e-3,\n",
    "                        max_iter=200, random_state=seed)\n",
    "    clf.fit(Xtr, ytr, sample_weight=sw)\n",
    "\n",
    "    ypred = clf.predict(Xte)\n",
    "    macro_f1 = f1_score(yte, ypred, average='macro')\n",
    "    print((\"WITH\" if not drop_fingerprints else \"DROP\") + \" fingerprints ⇒ Macro-F1=%.4f\" % macro_f1)\n",
    "    print(\"Confusion matrix (rows=GT, cols=Pred) order\", ORDER)\n",
    "    print(confusion_matrix(yte, ypred))\n",
    "    print(classification_report(yte, ypred, target_names=ORDER, digits=4))\n",
    "\n",
    "print(\"Failure Type (NSFNET→GEANT2 zero-shot):\")\n",
    "print(\"\\n(A) Easy mode (WITH fingerprints)\")\n",
    "run(drop_fingerprints=False)\n",
    "\n",
    "print(\"\\n(B) Hard mode (DROP fingerprints)\")\n",
    "run(drop_fingerprints=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdc4f7bb-b06a-43ca-94b1-df1c374be38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated rows (GEANT2 failures): 790  (skipped due to missing joins: 0)\n",
      "Top-1 accuracy: 1.000 | Top-3 accuracy: 1.000 | Mean hop-error: 0.00\n",
      "GT equals per-row max penalty (fraction): 1.000\n"
     ]
    }
   ],
   "source": [
    "# === GEANT2 heuristic localization (compact single cell) ===\n",
    "# Assumes DataFrames: paths, links\n",
    "\n",
    "import re, numpy as np, pandas as pd\n",
    "\n",
    "# ---- config (edit if needed) ----\n",
    "OFFSET_SCALE = 20.0      # GHz scale for offset term\n",
    "TIME_COL     = None      # e.g., 'time_idx' if both tables have it; else keep None\n",
    "LAG          = 0         # set 1 to use telemetry at t-1 (avoids peeking)\n",
    "ZSCORE       = False     # set True to z-score per (topology,day[,time])\n",
    "\n",
    "# ---- helpers ----\n",
    "def norm_eid(e):\n",
    "    s = str(e).strip().replace(\"(\",\"\").replace(\")\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    a,b = map(int, re.findall(r\"\\d+\", s)[:2])\n",
    "    return f\"{min(a,b)}-{max(a,b)}\"\n",
    "\n",
    "def path_to_eids(p):\n",
    "    ns = list(map(int, re.findall(r\"\\d+\", str(p))))\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(ns[:-1], ns[1:])]\n",
    "\n",
    "def zscore_in_groups(df, cols, by):\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        g = out.groupby(by)[c]\n",
    "        mu = g.transform(\"mean\")\n",
    "        sd = g.transform(\"std\").replace(0, np.nan).fillna(1.0)\n",
    "        out[c] = (out[c] - mu) / sd\n",
    "    return out\n",
    "\n",
    "# ---- prep links ----\n",
    "lk = links.copy()\n",
    "if \"edge_id\" not in lk.columns:\n",
    "    assert {\"u\",\"v\"}.issubset(lk.columns), \"links must have edge_id or (u,v)\"\n",
    "    lk[\"edge_id\"] = lk.apply(lambda r: norm_eid((r[\"u\"], r[\"v\"])), axis=1)\n",
    "\n",
    "if lk[\"bandwidth_utilization\"].max(skipna=True) > 1.0:\n",
    "    lk[\"bandwidth_utilization\"] = lk[\"bandwidth_utilization\"].astype(float) / 100.0\n",
    "\n",
    "keys = [\"topology\",\"day\"]\n",
    "if TIME_COL and (TIME_COL in lk.columns) and (TIME_COL in paths.columns):\n",
    "    if LAG: lk = lk.copy(); lk[TIME_COL] = lk[TIME_COL] + int(LAG)\n",
    "    keys.append(TIME_COL)\n",
    "\n",
    "# ---- filter GEANT2 failures ----\n",
    "df = paths.loc[\n",
    "    (paths[\"split\"]==\"test_target\") &\n",
    "    (paths[\"failure_present\"]==1) &\n",
    "    (paths[\"fail_link\"].astype(str).str.len()>0)\n",
    "].copy().reset_index(drop=True)\n",
    "\n",
    "df[\"gt_eid\"] = df[\"fail_link\"].map(norm_eid)\n",
    "df[\"eids\"]   = df[\"path\"].map(path_to_eids)\n",
    "df[\"row_id\"] = df.index\n",
    "\n",
    "# ---- explode edges & join telemetry ----\n",
    "cand = (df[[\"row_id\"]+keys+[\"eids\"]]\n",
    "        .explode(\"eids\").rename(columns={\"eids\":\"edge_id\"}))\n",
    "cand = cand.merge(\n",
    "    lk[keys+[\"edge_id\",\"center_freq_offset_ghz\",\"filter_bw_scale\",\"bandwidth_utilization\"]],\n",
    "    on=keys+[\"edge_id\"], how=\"left\", validate=\"many_to_one\"\n",
    ")\n",
    "\n",
    "# ---- penalty ----\n",
    "cand[\"offset\"] = cand[\"center_freq_offset_ghz\"].astype(float)\n",
    "cand[\"scale\"]  = cand[\"filter_bw_scale\"].astype(float).clip(lower=1e-6)\n",
    "cand[\"util\"]   = cand[\"bandwidth_utilization\"].astype(float).clip(0,1)\n",
    "\n",
    "if ZSCORE:\n",
    "    tmp = cand.copy()\n",
    "    tmp[\"invscale_minus1\"] = 1.0/tmp[\"scale\"] - 1.0\n",
    "    tmp = zscore_in_groups(tmp, [\"offset\",\"invscale_minus1\"], by=keys)\n",
    "    cand[\"pen\"] = (tmp[\"offset\"])**2 + tmp[\"invscale_minus1\"].clip(lower=0)*cand[\"util\"]\n",
    "else:\n",
    "    cand[\"pen\"] = (cand[\"offset\"]/float(OFFSET_SCALE))**2 + (1.0/cand[\"scale\"] - 1.0).clip(lower=0)*cand[\"util\"]\n",
    "\n",
    "# ---- pick Top-1 / Top-3 per path ----\n",
    "cand[\"rank\"]  = cand.groupby(\"row_id\")[\"pen\"].rank(ascending=False, method=\"first\")\n",
    "pred_top1     = cand.loc[cand[\"rank\"]==1, [\"row_id\",\"edge_id\"]].rename(columns={\"edge_id\":\"pred\"})\n",
    "top3_list     = cand.loc[cand[\"rank\"]<=3].groupby(\"row_id\")[\"edge_id\"].apply(list).rename(\"top3\")\n",
    "\n",
    "res = df[[\"row_id\",\"gt_eid\",\"eids\"]].merge(pred_top1, on=\"row_id\", how=\"left\").merge(top3_list, on=\"row_id\", how=\"left\")\n",
    "\n",
    "# exclude rows where no telemetry matched (no prediction)\n",
    "res_eval = res.dropna(subset=[\"pred\"]).copy()\n",
    "misses   = len(res) - len(res_eval)\n",
    "\n",
    "# ---- metrics ----\n",
    "top1 = (res_eval[\"pred\"]==res_eval[\"gt_eid\"]).mean()\n",
    "top3 = res_eval.apply(lambda r: r[\"gt_eid\"] in (r[\"top3\"] or []), axis=1).mean()\n",
    "def hop_err(r):\n",
    "    try: return abs(r[\"eids\"].index(r[\"pred\"]) - r[\"eids\"].index(r[\"gt_eid\"]))\n",
    "    except: return np.nan\n",
    "res_eval[\"hop_err\"] = res_eval.apply(hop_err, axis=1)\n",
    "mhe = float(np.nanmean(res_eval[\"hop_err\"]))\n",
    "\n",
    "# diagnostic: is GT the per-row max penalty? (just to verify no \"fixing\")\n",
    "mx   = cand.groupby(\"row_id\")[\"pen\"].max()\n",
    "gt_p = (cand.merge(res_eval[[\"row_id\",\"gt_eid\"]], left_on=[\"row_id\",\"edge_id\"], right_on=[\"row_id\",\"gt_eid\"], how=\"inner\"))[\"pen\"]\n",
    "gt_frac = float(np.mean(np.isclose(gt_p.values, mx.loc[res_eval[\"row_id\"]].values))) if len(gt_p) else float(\"nan\")\n",
    "\n",
    "print(f\"Evaluated rows (GEANT2 failures): {len(res_eval)}  (skipped due to missing joins: {misses})\")\n",
    "print(f\"Top-1 accuracy: {top1:.3f} | Top-3 accuracy: {top3:.3f} | Mean hop-error: {mhe:.2f}\")\n",
    "print(f\"GT equals per-row max penalty (fraction): {gt_frac:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a624d95-3010-4eb5-a6cf-750214030db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with ties at max penalty: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devon\\AppData\\Local\\Temp\\ipykernel_15148\\498411556.py:2: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ties = cand.groupby(\"row_id\").apply(lambda g: (g[\"pen\"]==g[\"pen\"].max()).sum()).gt(1).sum()\n"
     ]
    }
   ],
   "source": [
    "# 1) Ties check: kitni rows me max penalty par tie tha?\n",
    "ties = cand.groupby(\"row_id\").apply(lambda g: (g[\"pen\"]==g[\"pen\"].max()).sum()).gt(1).sum()\n",
    "print(\"Rows with ties at max penalty:\", int(ties))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "878a3cf7-3e0f-4145-b0d5-2ae6947b865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerouting policy: QoT-guided with up to K=3 alternate paths\n",
      "Demands predicted-bad (considered): 431\n",
      "Salvaged (predicted-good after reroute): 96\n",
      "Salvage rate: 22.3%\n",
      "Avg extra distance (km): 438.2\n",
      "Avg extra latency (ms): 2.191\n"
     ]
    }
   ],
   "source": [
    "# Simple QoT-guided rerouting on GEANT2 (test_target)\n",
    "# Requirements: networkx\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "try:\n",
    "    import networkx as nx\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Please install networkx: pip install networkx\")\n",
    "\n",
    "# --- 1) Ensure we have a QoT model (reuse if already trained earlier)\n",
    "def ensure_qot_model(paths_df):\n",
    "    NUM_COLS = [\n",
    "        'hops','distance_km','latency_ms',\n",
    "        'avg_utilization','min_osnr_db','min_snr_db',\n",
    "        'max_center_offset_ghz','min_filter_bw_scale',\n",
    "        'symbol_rate_gbaud','bitrate_gbps','req_osnr_db','osnr_margin_db'\n",
    "    ]\n",
    "    CAT_COLS = ['modulation']\n",
    "\n",
    "    def get_splits(df):\n",
    "        tr = df[df['split']==\"train_source\"].copy()\n",
    "        va = df[df['split']==\"val_source\"].copy()\n",
    "        te = df[df['split']==\"test_target\"].copy()\n",
    "        return tr, va, te\n",
    "\n",
    "    def make_xy(df):\n",
    "        X = df[NUM_COLS + CAT_COLS].copy()\n",
    "        X['modulation'] = X['modulation'].astype('category').cat.codes\n",
    "        y = df['qot_ok'].astype(int).values\n",
    "        return X, y\n",
    "\n",
    "    # try to reuse existing clf/scaler\n",
    "    if 'clf' in globals() and 'scaler' in globals():\n",
    "        return globals()['clf'], globals()['scaler'], NUM_COLS, CAT_COLS\n",
    "\n",
    "    train, val, test = get_splits(paths_df)\n",
    "    Xtr, ytr = make_xy(train)\n",
    "\n",
    "    scaler_ = StandardScaler()\n",
    "    Xtr = scaler_.fit_transform(Xtr)\n",
    "\n",
    "    clf_ = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                         alpha=1e-4, learning_rate_init=1e-3,\n",
    "                         max_iter=200, random_state=42)\n",
    "    clf_.fit(Xtr, ytr)\n",
    "\n",
    "    return clf_, scaler_, NUM_COLS, CAT_COLS\n",
    "\n",
    "clf, scaler, NUM_COLS, CAT_COLS = ensure_qot_model(paths)\n",
    "\n",
    "# --- 2) Utility: build per-day GEANT2 graph from links CSV\n",
    "def build_graph_for_day(day):\n",
    "    sub = links[(links['topology']=='GEANT2') & (links['day']==day)]\n",
    "    G = nx.Graph()\n",
    "    for _, r in sub.iterrows():\n",
    "        u, v = int(r['u']), int(r['v'])\n",
    "        G.add_edge(u, v,\n",
    "                   edge_id=f\"{min(u,v)}-{max(u,v)}\",\n",
    "                   length_km=float(r['length_km']),\n",
    "                   latency_ms=float(r['latency_ms']))\n",
    "    return G, sub.set_index('edge_id')\n",
    "\n",
    "# --- 3) Aggregate features for an arbitrary candidate path (keeping tx settings same as original)\n",
    "def agg_features_for_path(eids, day, tx_row, links_idx):\n",
    "    rows = links_idx.loc[eids]\n",
    "    # Aggregates from link telemetry (same day)\n",
    "    hops = len(eids)\n",
    "    distance_km = float(rows['length_km'].sum())\n",
    "    latency_ms  = float(rows['latency_ms'].sum())\n",
    "    avg_util    = float(rows['bandwidth_utilization'].mean())\n",
    "    min_osnr    = float(rows['osnr_db'].min())\n",
    "    min_snr     = float(rows['snr_db'].min())\n",
    "    max_shift   = float(rows['center_freq_offset_ghz'].max())\n",
    "    min_scale   = float(rows['filter_bw_scale'].min())\n",
    "\n",
    "    # Keep original transponder settings (assumption)\n",
    "    mod  = tx_row['modulation']\n",
    "    srb  = tx_row['symbol_rate_gbaud']\n",
    "    br   = tx_row['bitrate_gbps']\n",
    "    req  = tx_row['req_osnr_db']\n",
    "    margin = min_osnr - req\n",
    "\n",
    "    # Build feature row in the model's schema\n",
    "    X = pd.DataFrame([{\n",
    "        'hops':hops,'distance_km':distance_km,'latency_ms':latency_ms,\n",
    "        'avg_utilization':avg_util,'min_osnr_db':min_osnr,'min_snr_db':min_snr,\n",
    "        'max_center_offset_ghz':max_shift,'min_filter_bw_scale':min_scale,\n",
    "        'symbol_rate_gbaud':srb,'bitrate_gbps':br,'req_osnr_db':req,'osnr_margin_db':margin,\n",
    "        'modulation':mod\n",
    "    }])\n",
    "    # encode modulation\n",
    "    X['modulation'] = X['modulation'].astype('category').cat.codes\n",
    "    return X\n",
    "\n",
    "# --- 4) Reroute simulation on a sample of GEANT2 test rows\n",
    "test_gea = paths[(paths['split']=='test_target') & (paths['topology']=='GEANT2')].copy()\n",
    "\n",
    "# sample to keep it fast; you can increase n\n",
    "sample = test_gea.sample(n=min(500, len(test_gea)), random_state=7).reset_index(drop=True)\n",
    "\n",
    "K = 3  # try up to K shortest alternate paths\n",
    "salvaged = 0\n",
    "considered = 0\n",
    "extra_dist, extra_lat = [], []\n",
    "\n",
    "def edge_ids_from_nodes(nodes):\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(nodes[:-1], nodes[1:])]\n",
    "\n",
    "for _, row in sample.iterrows():\n",
    "    day = int(row['day'])\n",
    "    G, links_idx = build_graph_for_day(day)\n",
    "\n",
    "    # original path as nodes & eids\n",
    "    nodes = [int(x) for x in row['path'].split('->')]\n",
    "    orig_eids = edge_ids_from_nodes(nodes)\n",
    "\n",
    "    # original prediction\n",
    "    Xorig = pd.DataFrame([row[NUM_COLS + CAT_COLS]])\n",
    "    Xorig['modulation'] = Xorig['modulation'].astype('category').cat.codes\n",
    "    Xorig_scaled = scaler.transform(Xorig)\n",
    "    pred_orig = int(clf.predict(Xorig_scaled)[0])\n",
    "\n",
    "    if pred_orig == 1:\n",
    "        continue  # already good; no reroute needed\n",
    "\n",
    "    considered += 1\n",
    "\n",
    "    # k-shortest paths by length; skip identical to original\n",
    "    try:\n",
    "        # precompute edge weights from link length\n",
    "        for u, v, d in G.edges(data=True):\n",
    "            pass\n",
    "        kpaths_nodes = []\n",
    "        for k, p in enumerate(nx.shortest_simple_paths(G, nodes[0], nodes[-1], weight=lambda u,v,d: d['length_km'])):\n",
    "            if k >= K+1: break\n",
    "            if p == nodes:\n",
    "                continue\n",
    "            kpaths_nodes.append(p)\n",
    "    except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "        continue\n",
    "\n",
    "    rerouted = False\n",
    "    for cand_nodes in kpaths_nodes:\n",
    "        cand_eids = edge_ids_from_nodes(cand_nodes)\n",
    "        # features for candidate using same TX settings as original row\n",
    "        Xcand = agg_features_for_path(cand_eids, day, row, links_idx)\n",
    "        Xcand_scaled = scaler.transform(Xcand[NUM_COLS + CAT_COLS].assign(modulation= Xcand['modulation']))\n",
    "        pred_cand = int(clf.predict(Xcand_scaled)[0])\n",
    "        if pred_cand == 1:\n",
    "            # compute overheads\n",
    "            orig_dist = float(links_idx.loc[orig_eids]['length_km'].sum())\n",
    "            cand_dist = float(links_idx.loc[cand_eids]['length_km'].sum())\n",
    "            orig_lat  = float(links_idx.loc[orig_eids]['latency_ms'].sum())\n",
    "            cand_lat  = float(links_idx.loc[cand_eids]['latency_ms'].sum())\n",
    "            extra_dist.append(cand_dist - orig_dist)\n",
    "            extra_lat.append(cand_lat - orig_lat)\n",
    "            salvaged += 1\n",
    "            rerouted = True\n",
    "            break\n",
    "\n",
    "# --- 5) Summary\n",
    "print(\"Rerouting policy: QoT-guided with up to K=%d alternate paths\" % K)\n",
    "print(\"Demands predicted-bad (considered):\", considered)\n",
    "print(\"Salvaged (predicted-good after reroute):\", salvaged)\n",
    "rate = salvaged / considered if considered>0 else 0.0\n",
    "print(\"Salvage rate: %.1f%%\" % (100*rate))\n",
    "if extra_dist:\n",
    "    print(\"Avg extra distance (km): %.1f\" % (np.mean(extra_dist)))\n",
    "    print(\"Avg extra latency (ms): %.3f\" % (np.mean(extra_lat)))\n",
    "else:\n",
    "    print(\"No successful reroutes → no overhead stats.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "418eab99-73dd-47d8-8239-dcb64dbb0e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.8/2.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.0 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.0 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.0 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.4 MB/s  0:00:01\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-3.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "971d25ed-0cc6-495a-82c9-ac00e0494cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EASY (with margin & req)] QoT few-shot (k=100) on GEANT2 held-out\n",
      "Zero-shot : {'acc': 0.9796, 'f1': 0.9725, 'auc': 0.9983}\n",
      "k-shot    : {'acc': 0.9836, 'f1': 0.9783, 'auc': 0.9988}\n",
      "Δ (k - 0) : {'acc': 0.004, 'f1': 0.0058, 'auc': 0.0005}\n",
      "\n",
      "[HARD (drop margin & req)] QoT few-shot (k=100) on GEANT2 held-out\n",
      "Zero-shot : {'acc': 0.9808, 'f1': 0.9748, 'auc': 0.9984}\n",
      "k-shot    : {'acc': 0.9811, 'f1': 0.9748, 'auc': 0.998}\n",
      "Δ (k - 0) : {'acc': 0.0004, 'f1': 0.0, 'auc': -0.0003}\n"
     ]
    }
   ],
   "source": [
    "# Few-shot QoT adaptation on GEANT2 held-out (compare zero-shot vs k-shot)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- config ----------\n",
    "k = 100          # change to 10 / 40 / 200 etc.\n",
    "seed = 42\n",
    "\n",
    "# ---------- build held-out test by removing few-shot rows ----------\n",
    "key_cols = ['topology','day','src','dst','path','hops','distance_km']\n",
    "few_k = few.sample(n=min(k, len(few)), random_state=seed).copy()\n",
    "few_k['__key__'] = few_k[key_cols].astype(str).agg('|'.join, axis=1)\n",
    "\n",
    "test_all = paths[paths['split']=='test_target'].copy()\n",
    "test_all['__key__'] = test_all[key_cols].astype(str).agg('|'.join, axis=1)\n",
    "heldout = test_all[~test_all['__key__'].isin(set(few_k['__key__']))].copy()\n",
    "\n",
    "train = paths[paths['split']=='train_source'].copy()  # NSFNET only\n",
    "\n",
    "# ---------- feature builders ----------\n",
    "NUM_EASY = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps','req_osnr_db','osnr_margin_db'\n",
    "]\n",
    "NUM_HARD = [c for c in NUM_EASY if c not in ['req_osnr_db','osnr_margin_db']]\n",
    "CAT = ['modulation']\n",
    "\n",
    "def make_xy(df, hard=False):\n",
    "    cols = (NUM_HARD if hard else NUM_EASY) + CAT\n",
    "    X = df[cols].copy()\n",
    "    X['modulation'] = X['modulation'].astype('category').cat.codes\n",
    "    y = df['qot_ok'].astype(int).values\n",
    "    return X, y, cols\n",
    "\n",
    "def run_variant(hard=False, tag=\"HARD\"):\n",
    "    # train on NSFNET\n",
    "    Xtr, ytr, cols = make_xy(train, hard)\n",
    "    Xho, yho, _    = make_xy(heldout, hard)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(Xtr)\n",
    "    Xho = scaler.transform(Xho)\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                        alpha=1e-4, learning_rate_init=1e-3,\n",
    "                        max_iter=200, random_state=seed)\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    # zero-shot on held-out GEANT2\n",
    "    y0_pred = clf.predict(Xho)\n",
    "    metrics_zero = {\n",
    "        \"acc\": accuracy_score(yho, y0_pred),\n",
    "        \"f1\":  f1_score(yho, y0_pred, average='macro')\n",
    "    }\n",
    "    try:\n",
    "        y0_prob = clf.predict_proba(Xho)[:,1]\n",
    "        metrics_zero[\"auc\"] = roc_auc_score(yho, y0_prob)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # few-shot fine-tune on GEANT2 labeled k rows\n",
    "    Xfs, yfs, _ = make_xy(few_k, hard)\n",
    "    Xfs = scaler.transform(Xfs)\n",
    "    clf.partial_fit(Xfs, yfs)\n",
    "\n",
    "    # re-test on held-out\n",
    "    yA_pred = clf.predict(Xho)\n",
    "    metrics_k = {\n",
    "        \"acc\": accuracy_score(yho, yA_pred),\n",
    "        \"f1\":  f1_score(yho, yA_pred, average='macro')\n",
    "    }\n",
    "    try:\n",
    "        yA_prob = clf.predict_proba(Xho)[:,1]\n",
    "        metrics_k[\"auc\"] = roc_auc_score(yho, yA_prob)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"\\n[{tag}] QoT few-shot (k={len(few_k)}) on GEANT2 held-out\")\n",
    "    print(\"Zero-shot :\", {k: round(v,4) for k,v in metrics_zero.items()})\n",
    "    print(\"k-shot    :\", {k: round(v,4) for k,v in metrics_k.items()})\n",
    "    print(\"Δ (k - 0) :\", {m: round(metrics_k[m]-metrics_zero.get(m,0), 4) for m in metrics_k})\n",
    "\n",
    "# ---------- Run both variants ----------\n",
    "run_variant(hard=False, tag=\"EASY (with margin & req)\")\n",
    "run_variant(hard=True,  tag=\"HARD (drop margin & req)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70dded46-e0a4-4728-9edc-65d2c7de26a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EASY (with fingerprints)] Failure detection few-shot (k=100)\n",
      "Zero-shot : {'acc': 0.9968, 'f1': 0.9933}\n",
      "k-shot    : {'acc': 0.9968, 'f1': 0.9933}\n",
      "Δ (k - 0) : {'acc': 0.0, 'f1': 0.0}\n",
      "\n",
      "[HARD (drop fingerprints)] Failure detection few-shot (k=100)\n",
      "Zero-shot : {'acc': 0.8447, 'f1': 0.4906}\n",
      "k-shot    : {'acc': 0.7874, 'f1': 0.5146}\n",
      "Δ (k - 0) : {'acc': -0.0574, 'f1': 0.024}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devon\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Few-shot adaptation for failure_present (0/1) on GEANT2 held-out\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# --- config ---\n",
    "k = 100        # try 10 / 40 / 100 / 200\n",
    "seed = 42\n",
    "\n",
    "# --- build held-out test by removing few-shot rows ---\n",
    "key_cols = ['topology','day','src','dst','path','hops','distance_km']\n",
    "few_k = few.sample(n=min(k, len(few)), random_state=seed).copy()\n",
    "few_k['__key__'] = few_k[key_cols].astype(str).agg('|'.join, axis=1)\n",
    "\n",
    "test_all = paths[paths['split']=='test_target'].copy()\n",
    "test_all['__key__'] = test_all[key_cols].astype(str).agg('|'.join, axis=1)\n",
    "heldout = test_all[~test_all['__key__'].isin(set(few_k['__key__']))].copy()\n",
    "\n",
    "train = paths[paths['split']=='train_source'].copy()  # NSFNET\n",
    "\n",
    "# --- features ---\n",
    "NUM_EASY = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps','req_osnr_db','osnr_margin_db'\n",
    "]\n",
    "NUM_HARD = [c for c in NUM_EASY if c not in ['max_center_offset_ghz','min_filter_bw_scale']]\n",
    "CAT = ['modulation']\n",
    "\n",
    "def make_xy(df, hard=False):\n",
    "    cols = (NUM_HARD if hard else NUM_EASY) + CAT\n",
    "    X = df[cols].copy()\n",
    "    X['modulation'] = X['modulation'].astype('category').cat.codes\n",
    "    y = df['failure_present'].astype(int).values\n",
    "    return X, y, cols\n",
    "\n",
    "def run_variant(hard=False, tag=\"HARD\"):\n",
    "    # train on NSFNET\n",
    "    Xtr, ytr, cols = make_xy(train, hard)\n",
    "    Xho, yho, _    = make_xy(heldout, hard)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(Xtr)\n",
    "    Xho = scaler.transform(Xho)\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                        alpha=1e-4, learning_rate_init=1e-3,\n",
    "                        max_iter=200, random_state=seed)\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    # zero-shot on held-out\n",
    "    y0 = clf.predict(Xho)\n",
    "    m0 = {\"acc\": accuracy_score(yho, y0), \"f1\": f1_score(yho, y0, average='macro')}\n",
    "\n",
    "    # few-shot fine-tune on GEANT2 labeled k rows\n",
    "    Xfs, yfs, _ = make_xy(few_k, hard)\n",
    "    Xfs = scaler.transform(Xfs)\n",
    "    clf.partial_fit(Xfs, yfs)\n",
    "\n",
    "    # re-test on held-out\n",
    "    yA = clf.predict(Xho)\n",
    "    mA = {\"acc\": accuracy_score(yho, yA), \"f1\": f1_score(yho, yA, average='macro')}\n",
    "\n",
    "    print(f\"\\n[{tag}] Failure detection few-shot (k={len(few_k)})\")\n",
    "    print(\"Zero-shot :\", {k: round(v,4) for k,v in m0.items()})\n",
    "    print(\"k-shot    :\", {k: round(v,4) for k,v in mA.items()})\n",
    "    print(\"Δ (k - 0) :\", {m: round(mA[m]-m0[m], 4) for m in mA})\n",
    "\n",
    "# Run both variants\n",
    "run_variant(hard=False, tag=\"EASY (with fingerprints)\")\n",
    "run_variant(hard=True,  tag=\"HARD (drop fingerprints)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "623261b9-0df5-4197-9012-6b2db2f9a7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EASY (with fingerprints)] Failure Type 3-way few-shot (k=100) — Macro-F1\n",
      "Zero-shot : 0.9085\n",
      "k-shot    : 0.9536\n",
      "Δ (k-0)   : 0.0451\n",
      "\n",
      "Confusion matrix (rows=GT, cols=Pred) — order ['none', 'shift', 'tighten']\n",
      "[[4554    2    4]\n",
      " [   1  411   39]\n",
      " [  11    0  278]]\n",
      "\n",
      "Per-class report after k-shot:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        none     0.9974    0.9987    0.9980      4560\n",
      "       shift     0.9952    0.9113    0.9514       451\n",
      "     tighten     0.8660    0.9619    0.9115       289\n",
      "\n",
      "    accuracy                         0.9892      5300\n",
      "   macro avg     0.9529    0.9573    0.9536      5300\n",
      "weighted avg     0.9900    0.9892    0.9893      5300\n",
      "\n",
      "\n",
      "[HARD (drop fingerprints)] Failure Type 3-way few-shot (k=100) — Macro-F1\n",
      "Zero-shot : 0.3287\n",
      "k-shot    : 0.3111\n",
      "Δ (k-0)   : -0.0176\n",
      "\n",
      "Confusion matrix (rows=GT, cols=Pred) — order ['none', 'shift', 'tighten']\n",
      "[[3777  126  657]\n",
      " [ 386    1   64]\n",
      " [ 239    4   46]]\n",
      "\n",
      "Per-class report after k-shot:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        none     0.8580    0.8283    0.8429      4560\n",
      "       shift     0.0076    0.0022    0.0034       451\n",
      "     tighten     0.0600    0.1592    0.0871       289\n",
      "\n",
      "    accuracy                         0.7215      5300\n",
      "   macro avg     0.3085    0.3299    0.3111      5300\n",
      "weighted avg     0.7421    0.7215    0.7302      5300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devon\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Few-shot for failure_type: {'none','shift','tighten'} on GEANT2 (held-out)\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# ---- config ----\n",
    "k = 100        # try 10 / 40 / 100 / 200\n",
    "seed = 42\n",
    "ORDER = ['none','shift','tighten']\n",
    "cat_type = pd.api.types.CategoricalDtype(categories=ORDER, ordered=False)\n",
    "\n",
    "# ---- build held-out test by removing few-shot rows ----\n",
    "key_cols = ['topology','day','src','dst','path','hops','distance_km']\n",
    "few_k = few.sample(n=min(k, len(few)), random_state=seed).copy()\n",
    "few_k['__key__'] = few_k[key_cols].astype(str).agg('|'.join, axis=1)\n",
    "\n",
    "test_all = paths[paths['split']=='test_target'].copy()\n",
    "test_all['__key__'] = test_all[key_cols].astype(str).agg('|'.join, axis=1)\n",
    "heldout = test_all[~test_all['__key__'].isin(set(few_k['__key__']))].copy()\n",
    "\n",
    "train = paths[paths['split']=='train_source'].copy()  # NSFNET\n",
    "\n",
    "# ---- features ----\n",
    "NUM_EASY = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps','req_osnr_db','osnr_margin_db'\n",
    "]\n",
    "# HARD: drop direct fingerprints (offset/scale) so typing isn't trivial\n",
    "NUM_HARD = [c for c in NUM_EASY if c not in ['max_center_offset_ghz','min_filter_bw_scale']]\n",
    "CAT = ['modulation']\n",
    "\n",
    "def make_xy(df, hard=False):\n",
    "    cols = (NUM_HARD if hard else NUM_EASY) + CAT\n",
    "    X = df[cols].copy()\n",
    "    X['modulation'] = X['modulation'].astype('category').cat.codes\n",
    "    y = df['failure_type'].astype(cat_type).cat.codes.values  # 0=none, 1=shift, 2=tighten\n",
    "    return X, y, cols\n",
    "\n",
    "def run_variant(hard=False, tag=\"HARD\"):\n",
    "    # Train on NSFNET\n",
    "    Xtr, ytr, cols = make_xy(train, hard)\n",
    "    Xho, yho, _    = make_xy(heldout, hard)\n",
    "    Xfs, yfs, _    = make_xy(few_k, hard)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(Xtr)\n",
    "    Xho = scaler.transform(Xho)\n",
    "    Xfs = scaler.transform(Xfs)\n",
    "\n",
    "    # Class-imbalance handling via sample weights (train only)\n",
    "    sw = compute_sample_weight(class_weight='balanced', y=ytr)\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                        alpha=1e-4, learning_rate_init=1e-3,\n",
    "                        max_iter=200, random_state=seed)\n",
    "\n",
    "    clf.fit(Xtr, ytr, sample_weight=sw)\n",
    "\n",
    "    # Zero-shot on GEANT2 held-out\n",
    "    y0 = clf.predict(Xho)\n",
    "    m0 = f1_score(yho, y0, average='macro')\n",
    "\n",
    "    # Few-shot fine-tune on GEANT2 labeled k rows\n",
    "    # (partial_fit without weights keeps it simple & robust)\n",
    "    clf.partial_fit(Xfs, yfs)\n",
    "\n",
    "    # Re-test on held-out\n",
    "    yA = clf.predict(Xho)\n",
    "    mA = f1_score(yho, yA, average='macro')\n",
    "\n",
    "    print(f\"\\n[{tag}] Failure Type 3-way few-shot (k={len(few_k)}) — Macro-F1\")\n",
    "    print(\"Zero-shot :\", round(m0,4))\n",
    "    print(\"k-shot    :\", round(mA,4))\n",
    "    print(\"Δ (k-0)   :\", round(mA - m0, 4))\n",
    "\n",
    "    # Optional quick breakdown (helps spot which class improved)\n",
    "    print(\"\\nConfusion matrix (rows=GT, cols=Pred) — order\", ORDER)\n",
    "    print(confusion_matrix(yho, yA))\n",
    "    print(\"\\nPer-class report after k-shot:\")\n",
    "    print(classification_report(yho, yA, target_names=ORDER, digits=4))\n",
    "\n",
    "# Run both variants\n",
    "run_variant(hard=False, tag=\"EASY (with fingerprints)\")\n",
    "run_variant(hard=True,  tag=\"HARD (drop fingerprints)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "031a8f9a-3a52-4502-8d27-36bb8469e23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot (HARD features): {'acc': 0.9809, 'f1': 0.9754, 'auc': 0.9984}\n",
      "TTA-lite (BN-adapt + aug): {'acc': 0.9828, 'f1': 0.9775, 'auc': 0.9987}\n",
      "Δ (TTA - zero): {'acc': 0.0019, 'f1': 0.0021, 'auc': 0.0003}\n"
     ]
    }
   ],
   "source": [
    "# Label-free TTA-lite for QoT on GEANT2 (held-out): BN-adapt + test-time augmentations\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---------- 1) Features (HARD: drop margin/req) ----------\n",
    "NUM_HARD = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "CAT = ['modulation']\n",
    "\n",
    "def make_xy_qot(df):\n",
    "    X = df[NUM_HARD + CAT].copy()\n",
    "    X['modulation'] = X['modulation'].astype('category').cat.codes\n",
    "    y = df['qot_ok'].astype(int).values\n",
    "    return X, y\n",
    "\n",
    "# ---------- 2) Train on NSFNET; evaluate zero-shot on GEANT2 ----------\n",
    "train = paths[paths['split']=='train_source'].copy()\n",
    "test  = paths[paths['split']=='test_target'].copy()  # GEANT2 entire test domain\n",
    "\n",
    "Xtr, ytr = make_xy_qot(train)\n",
    "Xte, yte = make_xy_qot(test)\n",
    "\n",
    "sc_src = StandardScaler()\n",
    "Xtr_s = sc_src.fit_transform(Xtr)\n",
    "Xte_s = sc_src.transform(Xte)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                    alpha=1e-4, learning_rate_init=1e-3,\n",
    "                    max_iter=200, random_state=42)\n",
    "clf.fit(Xtr_s, ytr)\n",
    "\n",
    "# zero-shot\n",
    "p0 = clf.predict_proba(Xte_s)[:,1]\n",
    "y0 = (p0 >= 0.5).astype(int)\n",
    "m0 = {\n",
    "    \"acc\": accuracy_score(yte, y0),\n",
    "    \"f1\" : f1_score(yte, y0, average='macro'),\n",
    "    \"auc\": roc_auc_score(yte, p0)\n",
    "}\n",
    "print(\"Zero-shot (HARD features):\", {k: round(v,4) for k,v in m0.items()})\n",
    "\n",
    "# ---------- 3) TTA-lite: BN-adapt + test-time augmentations ----------\n",
    "def bn_adapt_transform(X_source_scaled, X_raw_batch, sc_src, gamma=0.3):\n",
    "    \"\"\"\n",
    "    BN-adapt style: blend source scaler stats with *current batch* stats.\n",
    "    X_source_scaled is unused except for shape; we re-compute transform using blended stats.\n",
    "    gamma: weight for batch stats (0=no adapt, 1=only batch).\n",
    "    \"\"\"\n",
    "    mu_src = sc_src.mean_\n",
    "    sd_src = sc_src.scale_\n",
    "    mu_b   = X_raw_batch.mean(axis=0)\n",
    "    sd_b   = X_raw_batch.std(axis=0, ddof=0) + 1e-6\n",
    "\n",
    "    mu_new = (1-gamma)*mu_src + gamma*mu_b\n",
    "    sd_new = (1-gamma)*sd_src + gamma*sd_b\n",
    "    Xb_s   = (X_raw_batch - mu_new) / sd_new\n",
    "    return Xb_s\n",
    "\n",
    "def jitter_batch(Xb, strength=0.02, rng=None):\n",
    "    \"\"\"\n",
    "    Small, realistic jitter on numeric columns (not categorical codes).\n",
    "    We assume 'modulation' already encoded as int; do not jitter that column (last column).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState(7)\n",
    "    Xn = Xb.copy()\n",
    "    # do not jitter the last column (modulation code)\n",
    "    noise = rng.normal(loc=0.0, scale=strength, size=Xn[:,:-1].shape)\n",
    "    Xn[:,:-1] = Xn[:,:-1] * (1 + noise)\n",
    "    return Xn\n",
    "\n",
    "# batch params\n",
    "BATCH = 256\n",
    "GAMMA = 0.3      # BN blend weight\n",
    "AUG_N = 5        # augmentations per batch sample\n",
    "JSTRENGTH = 0.03\n",
    "\n",
    "pA = np.zeros_like(p0)\n",
    "for start in range(0, len(Xte), BATCH):\n",
    "    end = min(len(Xte), start+BATCH)\n",
    "    Xb_raw = Xte.iloc[start:end].values\n",
    "\n",
    "    # BN-adapt scaling for this batch\n",
    "    Xb_s = bn_adapt_transform(None, Xb_raw, sc_src, gamma=GAMMA)\n",
    "\n",
    "    # Monte-Carlo test-time augmentations\n",
    "    probs = []\n",
    "    for _ in range(AUG_N):\n",
    "        Xb_j = jitter_batch(Xb_s, strength=JSTRENGTH)\n",
    "        probs.append(clf.predict_proba(Xb_j)[:,1])\n",
    "    p_batch = np.mean(probs, axis=0)\n",
    "    pA[start:end] = p_batch\n",
    "\n",
    "yA = (pA >= 0.5).astype(int)\n",
    "mA = {\n",
    "    \"acc\": accuracy_score(yte, yA),\n",
    "    \"f1\" : f1_score(yte, yA, average='macro'),\n",
    "    \"auc\": roc_auc_score(yte, pA)\n",
    "}\n",
    "print(\"TTA-lite (BN-adapt + aug):\", {k: round(v,4) for k,v in mA.items()})\n",
    "print(\"Δ (TTA - zero):\", {k: round(mA[k]-m0[k],4) for k in m0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acc0fc8e-a5fc-4d6e-8f8f-64c8ddf62f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-edge samples: 1556 (train) | 3248 (test)\n",
      "Paths evaluated (GEANT2 failures): 790\n",
      "Graph-aware (edge LR) — Top-1: 0.980 | Top-3: 1.000 | Mean hop-error: 0.03\n",
      "Heuristic — Top-1: 1.000 | Top-3: 1.000 | Mean hop-error: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Train a per-edge classifier on NSFNET (fail_link vs others) and evaluate on GEANT2 (zero-shot).\n",
    "# Prediction per path = edge with highest failure probability.\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- Quick helpers ---\n",
    "lk_idx = links.set_index([\"topology\",\"day\",\"edge_id\"])\n",
    "\n",
    "def edge_ids_from_path(path_str):\n",
    "    ns = [int(x) for x in path_str.split(\"->\")]\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(ns[:-1], ns[1:])]\n",
    "\n",
    "def build_edge_dataset(df):\n",
    "    \"\"\"Expand each failed path row into per-edge samples with features + label(1 if fail_link).\"\"\"\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        eids = edge_ids_from_path(r[\"path\"])\n",
    "        feats = []\n",
    "        for eid in eids:\n",
    "            try:\n",
    "                rec = lk_idx.loc[(r[\"topology\"], r[\"day\"], eid)]\n",
    "            except KeyError:\n",
    "                feats = []  # missing join; skip whole path\n",
    "                break\n",
    "            feats.append(dict(\n",
    "                eid=eid,\n",
    "                center_shift=float(rec[\"center_freq_offset_ghz\"]),\n",
    "                tighten_intensity=max(0.0, 1.0/float(rec[\"filter_bw_scale\"]) - 1.0),\n",
    "                util=float(rec[\"bandwidth_utilization\"]),\n",
    "                osnr=float(rec[\"osnr_db\"]),\n",
    "                snr=float(rec[\"snr_db\"]),\n",
    "                length=float(rec[\"length_km\"]),\n",
    "                latency=float(rec[\"latency_ms\"])\n",
    "            ))\n",
    "        if not feats:\n",
    "            continue\n",
    "        # path-level relative features (bottleneck cues)\n",
    "        min_osnr = min(f[\"osnr\"] for f in feats)\n",
    "        for idx, f in enumerate(feats):\n",
    "            rows.append({\n",
    "                \"topology\": r[\"topology\"],\n",
    "                \"day\": int(r[\"day\"]),\n",
    "                \"path\": r[\"path\"],\n",
    "                \"eid\": f[\"eid\"],\n",
    "                # absolute edge fingerprints\n",
    "                \"center_shift\": f[\"center_shift\"],\n",
    "                \"tighten_intensity\": f[\"tighten_intensity\"],\n",
    "                \"util\": f[\"util\"],\n",
    "                \"osnr\": f[\"osnr\"],\n",
    "                \"snr\": f[\"snr\"],\n",
    "                \"length\": f[\"length\"],\n",
    "                \"latency\": f[\"latency\"],\n",
    "                # relative feature: how close to bottleneck\n",
    "                \"osnr_gap_to_min\": f[\"osnr\"] - min_osnr,\n",
    "                # label\n",
    "                \"y\": int(f[\"eid\"] == r[\"fail_link\"])\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- Build splits for localization (only failures with valid fail_link) ---\n",
    "def with_loc(df):\n",
    "    return df[(df[\"failure_present\"]==1) & (df[\"fail_link\"].astype(str)!=\"\")].copy()\n",
    "\n",
    "loc_train = with_loc(paths[paths[\"split\"]==\"train_source\"])\n",
    "loc_test  = with_loc(paths[paths[\"split\"]==\"test_target\"])\n",
    "\n",
    "# --- Expand to per-edge datasets ---\n",
    "train_edges = build_edge_dataset(loc_train)\n",
    "test_edges  = build_edge_dataset(loc_test)\n",
    "\n",
    "print(\"Per-edge samples:\", len(train_edges), \"(train) |\", len(test_edges), \"(test)\")\n",
    "\n",
    "# --- Train logistic regression (balanced) ---\n",
    "FEATS = [\"center_shift\",\"tighten_intensity\",\"util\",\"osnr\",\"snr\",\"length\",\"latency\",\"osnr_gap_to_min\"]\n",
    "X_tr, y_tr = train_edges[FEATS].values, train_edges[\"y\"].values\n",
    "\n",
    "clf_edge = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "clf_edge.fit(X_tr, y_tr)\n",
    "\n",
    "# --- Evaluate per path: choose edge with highest predicted prob ---\n",
    "from collections import defaultdict\n",
    "\n",
    "# group test edges by (topology, day, path)\n",
    "groups = defaultdict(list)\n",
    "for i, r in test_edges.iterrows():\n",
    "    key = (r[\"topology\"], r[\"day\"], r[\"path\"])\n",
    "    groups[key].append(r)\n",
    "\n",
    "top1_hits, top3_hits, hop_errors = 0, 0, []\n",
    "n_paths = 0\n",
    "\n",
    "for key, edge_list in groups.items():\n",
    "    dfp = pd.DataFrame(edge_list)\n",
    "    probs = clf_edge.predict_proba(dfp[FEATS].values)[:,1]\n",
    "    dfp = dfp.assign(prob=probs)\n",
    "\n",
    "    # sort by prob desc\n",
    "    dfp = dfp.sort_values(\"prob\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # ground-truth edge id\n",
    "    # fetch GT from original loc_test row\n",
    "    topo, day, path_str = key\n",
    "    gt = loc_test[(loc_test[\"topology\"]==topo) & (loc_test[\"day\"]==day) & (loc_test[\"path\"]==path_str)][\"fail_link\"].iloc[0]\n",
    "\n",
    "    # metrics\n",
    "    n_paths += 1\n",
    "    pred_top1 = dfp.loc[0, \"eid\"]\n",
    "    if pred_top1 == gt:\n",
    "        top1_hits += 1\n",
    "    if gt in dfp.head(3)[\"eid\"].tolist():\n",
    "        top3_hits += 1\n",
    "\n",
    "    # hop-error\n",
    "    eids = edge_ids_from_path(path_str)\n",
    "    try:\n",
    "        hop_errors.append(abs(eids.index(pred_top1) - eids.index(gt)))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "top1 = top1_hits / max(1, n_paths)\n",
    "top3 = top3_hits / max(1, n_paths)\n",
    "mhe  = float(np.mean(hop_errors)) if hop_errors else float(\"nan\")\n",
    "\n",
    "print(f\"Paths evaluated (GEANT2 failures): {n_paths}\")\n",
    "print(f\"Graph-aware (edge LR) — Top-1: {top1:.3f} | Top-3: {top3:.3f} | Mean hop-error: {mhe:.2f}\")\n",
    "\n",
    "# (Optional) Compare to heuristic from Step-5 quickly (recompute here):\n",
    "def heuristic_penalty(row):\n",
    "    # same as Step-5: (offset/20)^2 + max(0, 1/scale -1) * util\n",
    "    rec = lk_idx.loc[(row[\"topology\"], row[\"day\"], row[\"eid\"])]\n",
    "    offset = float(rec[\"center_freq_offset_ghz\"])\n",
    "    scale  = float(rec[\"filter_bw_scale\"])\n",
    "    util   = float(rec[\"bandwidth_utilization\"])\n",
    "    return (offset/20.0)**2 + max(0.0, (1.0/max(scale, 1e-6) - 1.0)) * util\n",
    "\n",
    "top1_h, top3_h, hop_h, n2 = 0, 0, [], 0\n",
    "for key, edge_list in groups.items():\n",
    "    dfp = pd.DataFrame(edge_list)\n",
    "    dfp[\"pen\"] = dfp.apply(heuristic_penalty, axis=1)\n",
    "    dfp = dfp.sort_values(\"pen\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    topo, day, path_str = key\n",
    "    gt = loc_test[(loc_test[\"topology\"]==topo) & (loc_test[\"day\"]==day) & (loc_test[\"path\"]==path_str)][\"fail_link\"].iloc[0]\n",
    "    n2 += 1\n",
    "    pred1 = dfp.loc[0, \"eid\"]\n",
    "    if pred1 == gt: top1_h += 1\n",
    "    if gt in dfp.head(3)[\"eid\"].tolist(): top3_h += 1\n",
    "\n",
    "    eids = edge_ids_from_path(path_str)\n",
    "    try:\n",
    "        hop_h.append(abs(eids.index(pred1) - eids.index(gt)))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "print(f\"Heuristic — Top-1: {top1_h/max(1,n2):.3f} | Top-3: {top3_h/max(1,n2):.3f} | Mean hop-error: {np.mean(hop_h) if hop_h else float('nan'):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d976939-5c11-4a47-a7bb-b41355c12062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge samples: train=1556, held-out=2825, few-shot=423  (k_fail=100)\n",
      "Zero-shot  : {'paths': 690, 'top1': 0.98, 'top3': 1.0, 'mhe': 0.032}\n",
      "k-shot     : {'paths': 690, 'top1': 0.994, 'top3': 1.0, 'mhe': 0.014}\n",
      "Δ (k-0)    : {'paths': 690, 'top1': 0.014, 'top3': 0.0, 'mhe': -0.017}\n"
     ]
    }
   ],
   "source": [
    "# Few-shot localization: train edge-level LR on NSFNET → evaluate on GEANT2 held-out,\n",
    "# then fine-tune with k few-shot GEANT2 failure paths and re-evaluate.\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ---------- config ----------\n",
    "k = 100          # try 10 / 40 / 100 / 200\n",
    "seed = 42\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "lk_idx = links.set_index([\"topology\",\"day\",\"edge_id\"])\n",
    "\n",
    "def edge_ids_from_path(path_str):\n",
    "    ns = [int(x) for x in path_str.split(\"->\")]\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(ns[:-1], ns[1:])]\n",
    "\n",
    "def with_loc(df):\n",
    "    return df[(df[\"failure_present\"]==1) & (df[\"fail_link\"].astype(str)!=\"\")].copy()\n",
    "\n",
    "def build_edge_dataset(df):\n",
    "    \"\"\"Expand failed paths into per-edge samples with features + label (1 if edge==fail_link).\"\"\"\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        eids = edge_ids_from_path(r[\"path\"])\n",
    "        feats = []\n",
    "        # collect per-edge telemetry\n",
    "        ok = True\n",
    "        for eid in eids:\n",
    "            try:\n",
    "                rec = lk_idx.loc[(r[\"topology\"], r[\"day\"], eid)]\n",
    "            except KeyError:\n",
    "                ok = False\n",
    "                break\n",
    "            feats.append(dict(\n",
    "                eid=eid,\n",
    "                center_shift=float(rec[\"center_freq_offset_ghz\"]),\n",
    "                tighten_intensity=max(0.0, 1.0/float(rec[\"filter_bw_scale\"]) - 1.0),\n",
    "                util=float(rec[\"bandwidth_utilization\"]),\n",
    "                osnr=float(rec[\"osnr_db\"]),\n",
    "                snr=float(rec[\"snr_db\"]),\n",
    "                length=float(rec[\"length_km\"]),\n",
    "                latency=float(rec[\"latency_ms\"])\n",
    "            ))\n",
    "        if not ok or not feats:\n",
    "            continue\n",
    "        min_osnr = min(f[\"osnr\"] for f in feats)  # per-path bottleneck ref\n",
    "        for f in feats:\n",
    "            rows.append({\n",
    "                \"topology\": r[\"topology\"], \"day\": int(r[\"day\"]), \"path\": r[\"path\"],\n",
    "                \"eid\": f[\"eid\"],\n",
    "                \"center_shift\": f[\"center_shift\"],\n",
    "                \"tighten_intensity\": f[\"tighten_intensity\"],\n",
    "                \"util\": f[\"util\"], \"osnr\": f[\"osnr\"], \"snr\": f[\"snr\"],\n",
    "                \"length\": f[\"length\"], \"latency\": f[\"latency\"],\n",
    "                \"osnr_gap_to_min\": f[\"osnr\"] - min_osnr,\n",
    "                \"y\": int(f[\"eid\"] == r[\"fail_link\"])\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def eval_localization(edge_df, loc_df, clf, feats):\n",
    "    \"\"\"Per-path: pick edge with highest P(fail). Return Top-1/Top-3/MHE and counts.\"\"\"\n",
    "    from collections import defaultdict\n",
    "    groups = defaultdict(list)\n",
    "    for _, r in edge_df.iterrows():\n",
    "        groups[(r[\"topology\"], r[\"day\"], r[\"path\"])].append(r)\n",
    "    top1=top3=0; hop_err=[]; n=0\n",
    "    for key, lst in groups.items():\n",
    "        dfp = pd.DataFrame(lst)\n",
    "        probs = clf.predict_proba(dfp[feats].values)[:,1]\n",
    "        dfp = dfp.assign(prob=probs).sort_values(\"prob\", ascending=False).reset_index(drop=True)\n",
    "        topo, day, path_str = key\n",
    "        gt = loc_df[(loc_df[\"topology\"]==topo) & (loc_df[\"day\"]==day) & (loc_df[\"path\"]==path_str)][\"fail_link\"]\n",
    "        if gt.empty: \n",
    "            continue\n",
    "        gt = gt.iloc[0]\n",
    "        n += 1\n",
    "        pred1 = dfp.loc[0,\"eid\"]\n",
    "        if pred1 == gt: top1 += 1\n",
    "        if gt in dfp.head(3)[\"eid\"].tolist(): top3 += 1\n",
    "        eids = edge_ids_from_path(path_str)\n",
    "        try:\n",
    "            hop_err.append(abs(eids.index(pred1) - eids.index(gt)))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return dict(\n",
    "        paths=n, top1=top1/max(1,n), top3=top3/max(1,n),\n",
    "        mhe=(float(np.mean(hop_err)) if hop_err else float(\"nan\"))\n",
    "    )\n",
    "\n",
    "FEATS = [\"center_shift\",\"tighten_intensity\",\"util\",\"osnr\",\"snr\",\"length\",\"latency\",\"osnr_gap_to_min\"]\n",
    "\n",
    "# ---------- splits ----------\n",
    "loc_train_paths = with_loc(paths[paths[\"split\"]==\"train_source\"])      # NSFNET (fail-only)\n",
    "loc_test_paths  = with_loc(paths[paths[\"split\"]==\"test_target\"])       # GEANT2 (all fail)\n",
    "\n",
    "# few-shot = only failure rows from the few-shot CSV\n",
    "few_fail = with_loc(few).reset_index(drop=True)\n",
    "k_eff = min(k, len(few_fail))\n",
    "few_k = few_fail.sample(n=k_eff, random_state=seed)\n",
    "\n",
    "# held-out = GEANT2 failures minus few_k (by composite key)\n",
    "key_cols = [\"topology\",\"day\",\"path\"]\n",
    "few_keys = set(few_k[key_cols].astype(str).agg(\"|\".join, axis=1))\n",
    "loc_test_paths = loc_test_paths.copy()\n",
    "loc_test_paths[\"__key__\"] = loc_test_paths[key_cols].astype(str).agg(\"|\".join, axis=1)\n",
    "loc_heldout_paths = loc_test_paths[~loc_test_paths[\"__key__\"].isin(few_keys)].copy()\n",
    "\n",
    "# ---------- build per-edge datasets ----------\n",
    "train_edges   = build_edge_dataset(loc_train_paths)\n",
    "heldout_edges = build_edge_dataset(loc_heldout_paths)\n",
    "few_edges     = build_edge_dataset(few_k)\n",
    "\n",
    "print(f\"Edge samples: train={len(train_edges)}, held-out={len(heldout_edges)}, few-shot={len(few_edges)}  (k_fail={k_eff})\")\n",
    "\n",
    "# ---------- train on NSFNET edges ----------\n",
    "clf_edge = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "clf_edge.fit(train_edges[FEATS].values, train_edges[\"y\"].values)\n",
    "\n",
    "# zero-shot eval on GEANT2 held-out failures\n",
    "m0 = eval_localization(heldout_edges, loc_heldout_paths, clf_edge, FEATS)\n",
    "print(\"Zero-shot  :\", {k: (round(v,3) if isinstance(v,float) else v) for k,v in m0.items()})\n",
    "\n",
    "# ---------- fine-tune by re-training on (train + few-shot) ----------\n",
    "if len(few_edges) > 0:\n",
    "    combo_X = pd.concat([train_edges[FEATS], few_edges[FEATS]], axis=0)\n",
    "    combo_y = pd.concat([train_edges[\"y\"],   few_edges[\"y\"]],   axis=0)\n",
    "    clf_edge_ft = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "    clf_edge_ft.fit(combo_X.values, combo_y.values)\n",
    "    mA = eval_localization(heldout_edges, loc_heldout_paths, clf_edge_ft, FEATS)\n",
    "    print(\"k-shot     :\", {k: (round(v,3) if isinstance(v,float) else v) for k,v in mA.items()})\n",
    "    print(\"Δ (k-0)    :\", {k: (round(mA[k]-m0[k],3) if isinstance(mA[k],float) else mA[k]) for k in m0})\n",
    "else:\n",
    "    print(\"No few-shot failure rows available → skipping fine-tune.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29a87f19-9074-409e-829d-a3eec70f24b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge samples: train=1556, held-out=2825, few-shot=423  (k_fail=100)\n",
      "Zero-shot  : {'paths': 690, 'top1': 0.98, 'top3': 1.0, 'mhe': 0.032}\n",
      "k-shot     : {'paths': 690, 'top1': 0.994, 'top3': 1.0, 'mhe': 0.014}\n",
      "Δ (k-0)    : {'paths': 690, 'top1': 0.014, 'top3': 0.0, 'mhe': -0.017}\n"
     ]
    }
   ],
   "source": [
    "# Few-shot localization: train edge-level LR on NSFNET → evaluate on GEANT2 held-out,\n",
    "# then fine-tune with k few-shot GEANT2 failure paths and re-evaluate.\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ---------- config ----------\n",
    "k = 100          # try 10 / 40 / 100 / 200\n",
    "seed = 42\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "lk_idx = links.set_index([\"topology\",\"day\",\"edge_id\"])\n",
    "\n",
    "def edge_ids_from_path(path_str):\n",
    "    ns = [int(x) for x in path_str.split(\"->\")]\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(ns[:-1], ns[1:])]\n",
    "\n",
    "def with_loc(df):\n",
    "    return df[(df[\"failure_present\"]==1) & (df[\"fail_link\"].astype(str)!=\"\")].copy()\n",
    "\n",
    "def build_edge_dataset(df):\n",
    "    \"\"\"Expand failed paths into per-edge samples with features + label (1 if edge==fail_link).\"\"\"\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        eids = edge_ids_from_path(r[\"path\"])\n",
    "        feats = []\n",
    "        # collect per-edge telemetry\n",
    "        ok = True\n",
    "        for eid in eids:\n",
    "            try:\n",
    "                rec = lk_idx.loc[(r[\"topology\"], r[\"day\"], eid)]\n",
    "            except KeyError:\n",
    "                ok = False\n",
    "                break\n",
    "            feats.append(dict(\n",
    "                eid=eid,\n",
    "                center_shift=float(rec[\"center_freq_offset_ghz\"]),\n",
    "                tighten_intensity=max(0.0, 1.0/float(rec[\"filter_bw_scale\"]) - 1.0),\n",
    "                util=float(rec[\"bandwidth_utilization\"]),\n",
    "                osnr=float(rec[\"osnr_db\"]),\n",
    "                snr=float(rec[\"snr_db\"]),\n",
    "                length=float(rec[\"length_km\"]),\n",
    "                latency=float(rec[\"latency_ms\"])\n",
    "            ))\n",
    "        if not ok or not feats:\n",
    "            continue\n",
    "        min_osnr = min(f[\"osnr\"] for f in feats)  # per-path bottleneck ref\n",
    "        for f in feats:\n",
    "            rows.append({\n",
    "                \"topology\": r[\"topology\"], \"day\": int(r[\"day\"]), \"path\": r[\"path\"],\n",
    "                \"eid\": f[\"eid\"],\n",
    "                \"center_shift\": f[\"center_shift\"],\n",
    "                \"tighten_intensity\": f[\"tighten_intensity\"],\n",
    "                \"util\": f[\"util\"], \"osnr\": f[\"osnr\"], \"snr\": f[\"snr\"],\n",
    "                \"length\": f[\"length\"], \"latency\": f[\"latency\"],\n",
    "                \"osnr_gap_to_min\": f[\"osnr\"] - min_osnr,\n",
    "                \"y\": int(f[\"eid\"] == r[\"fail_link\"])\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def eval_localization(edge_df, loc_df, clf, feats):\n",
    "    \"\"\"Per-path: pick edge with highest P(fail). Return Top-1/Top-3/MHE and counts.\"\"\"\n",
    "    from collections import defaultdict\n",
    "    groups = defaultdict(list)\n",
    "    for _, r in edge_df.iterrows():\n",
    "        groups[(r[\"topology\"], r[\"day\"], r[\"path\"])].append(r)\n",
    "    top1=top3=0; hop_err=[]; n=0\n",
    "    for key, lst in groups.items():\n",
    "        dfp = pd.DataFrame(lst)\n",
    "        probs = clf.predict_proba(dfp[feats].values)[:,1]\n",
    "        dfp = dfp.assign(prob=probs).sort_values(\"prob\", ascending=False).reset_index(drop=True)\n",
    "        topo, day, path_str = key\n",
    "        gt = loc_df[(loc_df[\"topology\"]==topo) & (loc_df[\"day\"]==day) & (loc_df[\"path\"]==path_str)][\"fail_link\"]\n",
    "        if gt.empty: \n",
    "            continue\n",
    "        gt = gt.iloc[0]\n",
    "        n += 1\n",
    "        pred1 = dfp.loc[0,\"eid\"]\n",
    "        if pred1 == gt: top1 += 1\n",
    "        if gt in dfp.head(3)[\"eid\"].tolist(): top3 += 1\n",
    "        eids = edge_ids_from_path(path_str)\n",
    "        try:\n",
    "            hop_err.append(abs(eids.index(pred1) - eids.index(gt)))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return dict(\n",
    "        paths=n, top1=top1/max(1,n), top3=top3/max(1,n),\n",
    "        mhe=(float(np.mean(hop_err)) if hop_err else float(\"nan\"))\n",
    "    )\n",
    "\n",
    "FEATS = [\"center_shift\",\"tighten_intensity\",\"util\",\"osnr\",\"snr\",\"length\",\"latency\",\"osnr_gap_to_min\"]\n",
    "\n",
    "# ---------- splits ----------\n",
    "loc_train_paths = with_loc(paths[paths[\"split\"]==\"train_source\"])      # NSFNET (fail-only)\n",
    "loc_test_paths  = with_loc(paths[paths[\"split\"]==\"test_target\"])       # GEANT2 (all fail)\n",
    "\n",
    "# few-shot = only failure rows from the few-shot CSV\n",
    "few_fail = with_loc(few).reset_index(drop=True)\n",
    "k_eff = min(k, len(few_fail))\n",
    "few_k = few_fail.sample(n=k_eff, random_state=seed)\n",
    "\n",
    "# held-out = GEANT2 failures minus few_k (by composite key)\n",
    "key_cols = [\"topology\",\"day\",\"path\"]\n",
    "few_keys = set(few_k[key_cols].astype(str).agg(\"|\".join, axis=1))\n",
    "loc_test_paths = loc_test_paths.copy()\n",
    "loc_test_paths[\"__key__\"] = loc_test_paths[key_cols].astype(str).agg(\"|\".join, axis=1)\n",
    "loc_heldout_paths = loc_test_paths[~loc_test_paths[\"__key__\"].isin(few_keys)].copy()\n",
    "\n",
    "# ---------- build per-edge datasets ----------\n",
    "train_edges   = build_edge_dataset(loc_train_paths)\n",
    "heldout_edges = build_edge_dataset(loc_heldout_paths)\n",
    "few_edges     = build_edge_dataset(few_k)\n",
    "\n",
    "print(f\"Edge samples: train={len(train_edges)}, held-out={len(heldout_edges)}, few-shot={len(few_edges)}  (k_fail={k_eff})\")\n",
    "\n",
    "# ---------- train on NSFNET edges ----------\n",
    "clf_edge = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "clf_edge.fit(train_edges[FEATS].values, train_edges[\"y\"].values)\n",
    "\n",
    "# zero-shot eval on GEANT2 held-out failures\n",
    "m0 = eval_localization(heldout_edges, loc_heldout_paths, clf_edge, FEATS)\n",
    "print(\"Zero-shot  :\", {k: (round(v,3) if isinstance(v,float) else v) for k,v in m0.items()})\n",
    "\n",
    "# ---------- fine-tune by re-training on (train + few-shot) ----------\n",
    "if len(few_edges) > 0:\n",
    "    combo_X = pd.concat([train_edges[FEATS], few_edges[FEATS]], axis=0)\n",
    "    combo_y = pd.concat([train_edges[\"y\"],   few_edges[\"y\"]],   axis=0)\n",
    "    clf_edge_ft = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "    clf_edge_ft.fit(combo_X.values, combo_y.values)\n",
    "    mA = eval_localization(heldout_edges, loc_heldout_paths, clf_edge_ft, FEATS)\n",
    "    print(\"k-shot     :\", {k: (round(v,3) if isinstance(v,float) else v) for k,v in mA.items()})\n",
    "    print(\"Δ (k-0)    :\", {k: (round(mA[k]-m0[k],3) if isinstance(mA[k],float) else mA[k]) for k in m0})\n",
    "else:\n",
    "    print(\"No few-shot failure rows available → skipping fine-tune.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c97dc0-64e9-4145-80c7-c431ef654a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figures →\n",
      "outputs\\figs\\qot_roc_zero_vs_tta.png\n",
      "outputs\\figs\\qot_reliability_zero_vs_tta.png\n",
      "outputs\\figs\\qot_risk_coverage_zero_vs_tta.png\n",
      "{'AUC_zero': 0.9983525503074555, 'AUC_tta': 0.9986583012633958, 'ECE_zero': 0.2534282407499026, 'ECE_tta': 0.2520535671302439}\n"
     ]
    }
   ],
   "source": [
    "# QoT calibration & reliability visuals (Zero-shot vs TTA-lite) on GEANT2 (HARD features)\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# ---------- 1) Features (HARD: drop margin/req) ----------\n",
    "NUM_HARD = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "CAT = ['modulation']\n",
    "\n",
    "def make_xy_qot(df):\n",
    "    X = df[NUM_HARD + CAT].copy()\n",
    "    X['modulation'] = X['modulation'].astype('category').cat.codes\n",
    "    y = df['qot_ok'].astype(int).values\n",
    "    return X, y\n",
    "\n",
    "# ---------- 2) Train on NSFNET; evaluate on GEANT2 ----------\n",
    "train = paths[paths['split']=='train_source'].copy()\n",
    "test  = paths[paths['split']=='test_target'].copy()\n",
    "\n",
    "Xtr, ytr = make_xy_qot(train)\n",
    "Xte, yte = make_xy_qot(test)\n",
    "\n",
    "sc_src = StandardScaler()\n",
    "Xtr_s = sc_src.fit_transform(Xtr)\n",
    "Xte_s = sc_src.transform(Xte)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                    alpha=1e-4, learning_rate_init=1e-3,\n",
    "                    max_iter=200, random_state=42)\n",
    "clf.fit(Xtr_s, ytr)\n",
    "\n",
    "# Zero-shot probs\n",
    "p0 = clf.predict_proba(Xte_s)[:,1]\n",
    "\n",
    "# ---------- 3) TTA-lite: BN-adapt + test-time augmentations (label-free) ----------\n",
    "def bn_adapt_transform(X_raw_batch, sc_src, gamma=0.3):\n",
    "    mu_src = sc_src.mean_\n",
    "    sd_src = sc_src.scale_\n",
    "    mu_b   = X_raw_batch.mean(axis=0)\n",
    "    sd_b   = X_raw_batch.std(axis=0, ddof=0) + 1e-6\n",
    "    mu_new = (1-gamma)*mu_src + gamma*mu_b\n",
    "    sd_new = (1-gamma)*sd_src + gamma*sd_b\n",
    "    return (X_raw_batch - mu_new) / sd_new\n",
    "\n",
    "def jitter_batch(Xb, strength=0.03, rng=None):\n",
    "    rng = rng or np.random.RandomState(7)\n",
    "    Xn = Xb.copy()\n",
    "    noise = rng.normal(0.0, strength, size=Xn[:,:-1].shape)\n",
    "    Xn[:,:-1] = Xn[:,:-1] * (1 + noise)   # don't jitter modulation code (last col)\n",
    "    return Xn\n",
    "\n",
    "BATCH = 256; GAMMA = 0.3; AUG_N = 5; JSTRENGTH = 0.03\n",
    "pA = np.zeros_like(p0)\n",
    "for s in range(0, len(Xte), BATCH):\n",
    "    e = min(len(Xte), s+BATCH)\n",
    "    Xb_raw = Xte.iloc[s:e].values\n",
    "    Xb_s   = bn_adapt_transform(Xb_raw, sc_src, gamma=GAMMA)\n",
    "    probs  = []\n",
    "    for _ in range(AUG_N):\n",
    "        Xb_j = jitter_batch(Xb_s, strength=JSTRENGTH)\n",
    "        probs.append(clf.predict_proba(Xb_j)[:,1])\n",
    "    pA[s:e] = np.mean(probs, axis=0)\n",
    "\n",
    "# ---------- 4) Metrics & Visuals ----------\n",
    "from pathlib import Path\n",
    "OUT = Path(\"./outputs/figs\"); OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# (a) ROC curves\n",
    "fpr0, tpr0, _ = roc_curve(yte, p0)\n",
    "fprA, tprA, _ = roc_curve(yte, pA)\n",
    "auc0, aucA = roc_auc_score(yte, p0), roc_auc_score(yte, pA)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr0, tpr0, label=f\"Zero-shot (AUC={auc0:.3f})\")\n",
    "plt.plot(fprA, tprA, label=f\"TTA-lite (AUC={aucA:.3f})\")\n",
    "plt.plot([0,1],[0,1],'--',linewidth=1)\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"QoT ROC — GEANT2 (HARD features)\")\n",
    "plt.legend(); \n",
    "plt.savefig(OUT/\"qot_roc_zero_vs_tta.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# (b) Reliability diagram + ECE\n",
    "def reliability_curve(y_true, p, n_bins=10):\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    idx  = np.digitize(p, bins) - 1\n",
    "    accs, confs, sizes = [], [], []\n",
    "    for b in range(n_bins):\n",
    "        mask = idx==b\n",
    "        if not np.any(mask): \n",
    "            accs.append(np.nan); confs.append((bins[b]+bins[b+1])/2); sizes.append(0)\n",
    "            continue\n",
    "        acc  = np.mean((p[mask]>=0.5)==y_true[mask])\n",
    "        conf = p[mask].mean()\n",
    "        accs.append(acc); confs.append(conf); sizes.append(mask.sum())\n",
    "    # Expected Calibration Error\n",
    "    N = len(y_true)\n",
    "    ece = np.nansum([ (sizes[i]/N)*abs(accs[i]-confs[i]) for i in range(n_bins) ])\n",
    "    return np.array(confs), np.array(accs), ece\n",
    "\n",
    "c0, a0, ece0 = reliability_curve(yte, p0, n_bins=10)\n",
    "cA, aA, eceA = reliability_curve(yte, pA, n_bins=10)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([0,1],[0,1],'--',linewidth=1)\n",
    "plt.plot(c0, a0, marker='o', label=f\"Zero-shot (ECE={ece0:.3f})\")\n",
    "plt.plot(cA, aA, marker='o', label=f\"TTA-lite (ECE={eceA:.3f})\")\n",
    "plt.xlabel(\"Confidence\"); plt.ylabel(\"Empirical Accuracy\"); \n",
    "plt.title(\"QoT Reliability — GEANT2 (HARD features)\")\n",
    "plt.legend()\n",
    "plt.savefig(OUT/\"qot_reliability_zero_vs_tta.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# (c) Risk-coverage curve (selective prediction by confidence)\n",
    "def risk_coverage(y_true, p):\n",
    "    conf = np.maximum(p, 1-p)  # confidence of predicted label\n",
    "    order = np.argsort(-conf)  # high → low confidence\n",
    "    yhat = (p>=0.5).astype(int)\n",
    "    accs, covs = [], []\n",
    "    for k in range(50, len(p)+1, max(1, len(p)//20)):\n",
    "        sel = order[:k]\n",
    "        accs.append( np.mean(yhat[sel]==y_true[sel]) )\n",
    "        covs.append( k/len(p) )\n",
    "    return np.array(covs), np.array(accs)\n",
    "\n",
    "cov0, acc0 = risk_coverage(yte, p0)\n",
    "covA, accA = risk_coverage(yte, pA)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cov0, 1-acc0, label=\"Zero-shot\")   # risk = 1-acc\n",
    "plt.plot(covA, 1-accA, label=\"TTA-lite\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Coverage\"); plt.ylabel(\"Risk (1 - Accuracy)\")\n",
    "plt.title(\"QoT Risk–Coverage — GEANT2 (HARD features)\")\n",
    "plt.legend()\n",
    "plt.savefig(OUT/\"qot_risk_coverage_zero_vs_tta.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "print(\"Saved figures →\",\n",
    "      str(OUT/\"qot_roc_zero_vs_tta.png\"),\n",
    "      str(OUT/\"qot_reliability_zero_vs_tta.png\"),\n",
    "      str(OUT/\"qot_risk_coverage_zero_vs_tta.png\"), sep=\"\\n\")\n",
    "print({\"AUC_zero\": float(auc0), \"AUC_tta\": float(aucA), \"ECE_zero\": float(ece0), \"ECE_tta\": float(eceA)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "675aa51a-e363-4dfd-8528-6e43e662a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.1-cp313-cp313-win_amd64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp313-cp313-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\devon\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\devon\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\devon\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\devon\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.5-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/8.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.6/8.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.6/8.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.5/8.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 3.6 MB/s  0:00:02\n",
      "Downloading contourpy-1.3.3-cp313-cp313-win_amd64.whl (226 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.1-cp313-cp313-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.3 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.3 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 4.7 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp313-cp313-win_amd64.whl (73 kB)\n",
      "Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/7.0 MB 7.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.6/7.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.4/7.0 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.4/7.0 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.5/7.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.0/7.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/7.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 4.0 MB/s  0:00:01\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ---------------------------------------- 0/7 [pyparsing]\n",
      "   ---------------------------------------- 0/7 [pyparsing]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----- ---------------------------------- 1/7 [pillow]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ----------------- ---------------------- 3/7 [fonttools]\n",
      "   ---------------------- ----------------- 4/7 [cycler]\n",
      "   ---------------------------- ----------- 5/7 [contourpy]\n",
      "   ---------------------------- ----------- 5/7 [contourpy]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [matplotlib]\n",
      "   ---------------------------------------- 7/7 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.1 kiwisolver-1.4.9 matplotlib-3.10.5 pillow-11.3.0 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcded1fc-0afa-4565-9301-a09e9fe0fa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched rows: 10800/10800 (dropped 0 due to missing link joins)\n",
      "Saved: outputs\\paths_graph_enriched.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topology</th>\n",
       "      <th>day</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>path</th>\n",
       "      <th>hops</th>\n",
       "      <th>gf_osnr_min</th>\n",
       "      <th>gf_osnr_var</th>\n",
       "      <th>gf_util_mean</th>\n",
       "      <th>gf_shift_max</th>\n",
       "      <th>gf_scale_min</th>\n",
       "      <th>gf_frac_shifted</th>\n",
       "      <th>gf_frac_tight</th>\n",
       "      <th>gf_bot_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSFNET</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1-&gt;8-&gt;10-&gt;11-&gt;12-&gt;13</td>\n",
       "      <td>5</td>\n",
       "      <td>22.537527</td>\n",
       "      <td>1.362599</td>\n",
       "      <td>0.787454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSFNET</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>14-&gt;11-&gt;10-&gt;8-&gt;1</td>\n",
       "      <td>4</td>\n",
       "      <td>22.537527</td>\n",
       "      <td>1.538080</td>\n",
       "      <td>0.795083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSFNET</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5-&gt;7-&gt;8</td>\n",
       "      <td>2</td>\n",
       "      <td>23.277962</td>\n",
       "      <td>4.136134</td>\n",
       "      <td>0.636687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topology  day  src  dst                  path  hops  gf_osnr_min  \\\n",
       "0   NSFNET    0    1   13  1->8->10->11->12->13     5    22.537527   \n",
       "1   NSFNET    0   14    1      14->11->10->8->1     4    22.537527   \n",
       "2   NSFNET    0    5    8               5->7->8     2    23.277962   \n",
       "\n",
       "   gf_osnr_var  gf_util_mean  gf_shift_max  gf_scale_min  gf_frac_shifted  \\\n",
       "0     1.362599      0.787454           0.0           1.0              0.0   \n",
       "1     1.538080      0.795083           0.0           1.0              0.0   \n",
       "2     4.136134      0.636687           0.0           1.0              0.0   \n",
       "\n",
       "   gf_frac_tight  gf_bot_pos  \n",
       "0            0.0    0.500000  \n",
       "1            0.0    0.333333  \n",
       "2            0.0    0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build per-path graph-aware features by joining link telemetry for the same day/topology.\n",
    "# Output: ./outputs/paths_graph_enriched.csv  (+ a quick preview)\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# --- load fallbacks in case the kernel was restarted ---\n",
    "DATA_DIR = Path(\".\")\n",
    "if 'paths' not in globals():\n",
    "    paths = pd.read_csv(DATA_DIR/\"eon_paths_timeseries.csv\")\n",
    "if 'links' not in globals():\n",
    "    links = pd.read_csv(DATA_DIR/\"eon_links_timeseries.csv\")\n",
    "\n",
    "OUT_DIR = Path(\"./outputs\"); (OUT_DIR/\"figs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Fast lookup for link telemetry\n",
    "lk = links.set_index([\"topology\",\"day\",\"edge_id\"])\n",
    "\n",
    "def edge_ids_from_path(path_str):\n",
    "    ns = [int(x) for x in str(path_str).split(\"->\")]\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(ns[:-1], ns[1:])]\n",
    "\n",
    "def compute_graph_feats(row):\n",
    "    topo = row[\"topology\"]; day = row[\"day\"]; p = row[\"path\"]\n",
    "    eids = edge_ids_from_path(p)\n",
    "    recs = []\n",
    "    for eid in eids:\n",
    "        try:\n",
    "            rec = lk.loc[(topo, day, eid)]\n",
    "        except KeyError:\n",
    "            return pd.Series({\"gf_ok\":0})  # mark join failure; will drop later\n",
    "        recs.append(dict(\n",
    "            eid=eid,\n",
    "            osnr=float(rec[\"osnr_db\"]),\n",
    "            snr=float(rec[\"snr_db\"]),\n",
    "            util=float(rec[\"bandwidth_utilization\"]),\n",
    "            shift=float(rec[\"center_freq_offset_ghz\"]),\n",
    "            scale=float(rec[\"filter_bw_scale\"]),\n",
    "            length=float(rec[\"length_km\"]),\n",
    "            latency=float(rec[\"latency_ms\"])\n",
    "        ))\n",
    "    # aggregate\n",
    "    hops = len(recs)\n",
    "    osnrs = np.array([r[\"osnr\"] for r in recs])\n",
    "    utils = np.array([r[\"util\"] for r in recs])\n",
    "    shifts = np.array([r[\"shift\"] for r in recs])\n",
    "    scales = np.array([r[\"scale\"] for r in recs])\n",
    "    lens = np.array([r[\"length\"] for r in recs])\n",
    "    lats = np.array([r[\"latency\"] for r in recs])\n",
    "\n",
    "    # bottleneck (min OSNR) index & normalized position\n",
    "    bot_idx = int(np.argmin(osnrs))\n",
    "    bot_pos = bot_idx / max(1, hops-1)\n",
    "\n",
    "    # fractions of “shifted” and “tightened” edges\n",
    "    frac_shifted = float((shifts > 0).mean())\n",
    "    frac_tight   = float((scales < 1.0).mean())\n",
    "\n",
    "    return pd.Series({\n",
    "        \"gf_ok\": 1,\n",
    "        \"gf_len_sum_km\": float(lens.sum()),\n",
    "        \"gf_lat_sum_ms\": float(lats.sum()),\n",
    "        \"gf_osnr_min\": float(osnrs.min()),\n",
    "        \"gf_osnr_mean\": float(osnrs.mean()),\n",
    "        \"gf_osnr_var\": float(osnrs.var()) if hops>1 else 0.0,\n",
    "        \"gf_snr_min\": float(np.min([r[\"snr\"] for r in recs])),\n",
    "        \"gf_util_mean\": float(utils.mean()),\n",
    "        \"gf_util_max\": float(utils.max()),\n",
    "        \"gf_shift_max\": float(shifts.max()),\n",
    "        \"gf_scale_min\": float(scales.min()),\n",
    "        \"gf_frac_shifted\": frac_shifted,\n",
    "        \"gf_frac_tight\": frac_tight,\n",
    "        \"gf_bot_pos\": float(bot_pos)  # where along the path the bottleneck sits (0…1)\n",
    "    })\n",
    "\n",
    "# Compute features (this may take ~seconds on 10k rows)\n",
    "enriched = paths.join(paths.apply(compute_graph_feats, axis=1))\n",
    "\n",
    "# Drop rows where join failed (gf_ok!=1)\n",
    "before = len(enriched)\n",
    "enriched = enriched[enriched[\"gf_ok\"]==1].drop(columns=[\"gf_ok\"])\n",
    "after = len(enriched)\n",
    "print(f\"Enriched rows: {after}/{before} (dropped {before-after} due to missing link joins)\")\n",
    "\n",
    "# Save & preview\n",
    "out_csv = OUT_DIR/\"paths_graph_enriched.csv\"\n",
    "enriched.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n",
    "display(enriched.head(3)[[\n",
    "    \"topology\",\"day\",\"src\",\"dst\",\"path\",\"hops\",\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_shift_max\",\"gf_scale_min\",\n",
    "    \"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d7d052-cc93-4951-8a47-23fb3c0330e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QoT (HARD) — Zero-shot GEANT2\n",
      "Base     : {'acc': 0.9809, 'f1': 0.9754, 'auc': 0.9984}\n",
      "+GraphFea: {'acc': 0.9598, 'f1': 0.9473, 'auc': 0.9938}\n",
      "Δ (enr-base): {'acc': -0.0211, 'f1': -0.028, 'auc': -0.0046}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devon\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failure Detection (HARD) — Zero-shot GEANT2\n",
      "Base     : {'acc': 0.8415, 'f1': 0.4935}\n",
      "+GraphFea: {'acc': 0.9931, 'f1': 0.9862}\n",
      "Δ (enr-base): {'acc': 0.1517, 'f1': 0.4927}\n"
     ]
    }
   ],
   "source": [
    "# Compare zero-shot performance with/without graph-aware features (HARD setup) on GEANT2.\n",
    "# Requires: you already ran Step 14 and have ./outputs/paths_graph_enriched.csv\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# --- Load enriched file (contains original + gf_* columns)\n",
    "ENRICHED_CSV = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "assert ENRICHED_CSV.exists(), \"Run Step 14 first to create paths_graph_enriched.csv\"\n",
    "df = pd.read_csv(ENRICHED_CSV)\n",
    "\n",
    "# --- Splits\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()    # NSFNET\n",
    "test  = df[df[\"split\"]==\"test_target\"].copy()     # GEANT2\n",
    "\n",
    "# --- Feature sets\n",
    "# QoT (HARD): drop req/margin (decision-time realistic), keep other numerics\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "# Enriched = Base + graph-aware summaries\n",
    "GF_COLS = [\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "    \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]\n",
    "ENR_QOT = BASE_QOT + GF_COLS\n",
    "\n",
    "# Failure Detection (HARD): drop direct fingerprints in base (offset/scale)\n",
    "BASE_FAIL = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'   # note: no offset/scale here\n",
    "]\n",
    "ENR_FAIL = BASE_FAIL + GF_COLS\n",
    "\n",
    "CAT = ['modulation']\n",
    "\n",
    "def run_task(task=\"qot\", cols=None):\n",
    "    Xtr = train[cols + CAT].copy()\n",
    "    Xte = test[cols + CAT].copy()\n",
    "    # encode modulation\n",
    "    Xtr['modulation'] = Xtr['modulation'].astype('category').cat.codes\n",
    "    Xte['modulation'] = Xte['modulation'].astype('category').cat.codes\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = scaler.fit_transform(Xtr)\n",
    "    Xte_s = scaler.transform(Xte)\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                        alpha=1e-4, learning_rate_init=1e-3,\n",
    "                        max_iter=200, random_state=42)\n",
    "    if task==\"qot\":\n",
    "        ytr = train['qot_ok'].astype(int).values\n",
    "        yte = test['qot_ok'].astype(int).values\n",
    "        clf.fit(Xtr_s, ytr)\n",
    "        p   = clf.predict_proba(Xte_s)[:,1]\n",
    "        y   = (p>=0.5).astype(int)\n",
    "        out = dict(acc=accuracy_score(yte,y),\n",
    "                   f1 =f1_score(yte,y,average='macro'),\n",
    "                   auc=roc_auc_score(yte,p))\n",
    "    elif task==\"fail\":\n",
    "        ytr = train['failure_present'].astype(int).values\n",
    "        yte = test['failure_present'].astype(int).values\n",
    "        clf.fit(Xtr_s, ytr)\n",
    "        y   = clf.predict(Xte_s)\n",
    "        out = dict(acc=accuracy_score(yte,y),\n",
    "                   f1 =f1_score(yte,y,average='macro'))\n",
    "    else:\n",
    "        raise ValueError(\"task must be 'qot' or 'fail'\")\n",
    "    return out\n",
    "\n",
    "def pretty(delta, keys):\n",
    "    return {k: round(delta[k],4) for k in keys if k in delta}\n",
    "\n",
    "# --- QoT: base vs enriched\n",
    "m_qot_base = run_task(\"qot\", BASE_QOT)\n",
    "m_qot_enr  = run_task(\"qot\", ENR_QOT)\n",
    "dq = {k: m_qot_enr[k]-m_qot_base[k] for k in m_qot_enr}\n",
    "print(\"\\nQoT (HARD) — Zero-shot GEANT2\")\n",
    "print(\"Base     :\", {k: round(v,4) for k,v in m_qot_base.items()})\n",
    "print(\"+GraphFea:\", {k: round(v,4) for k,v in m_qot_enr.items()})\n",
    "print(\"Δ (enr-base):\", pretty(dq, [\"acc\",\"f1\",\"auc\"]))\n",
    "\n",
    "# --- Failure Detection: base vs enriched\n",
    "m_fd_base = run_task(\"fail\", BASE_FAIL)\n",
    "m_fd_enr  = run_task(\"fail\", ENR_FAIL)\n",
    "dfd = {k: m_fd_enr[k]-m_fd_base[k] for k in m_fd_enr}\n",
    "print(\"\\nFailure Detection (HARD) — Zero-shot GEANT2\")\n",
    "print(\"Base     :\", {k: round(v,4) for k,v in m_fd_base.items()})\n",
    "print(\"+GraphFea:\", {k: round(v,4) for k,v in m_fd_enr.items()})\n",
    "print(\"Δ (enr-base):\", pretty(dfd, [\"acc\",\"f1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6920965b-9f5f-4909-b8a1-5bea783e362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QoT-guided Rerouting (GEANT2, zero-shot, HARD)\n",
      "BASE     : {'model': 'BASE', 'considered': 166, 'salvaged': 38, 'salvage_rate_pct': 22.892, 'avg_extra_km': 212.433, 'avg_extra_ms': 1.062}\n",
      "+GraphFea: {'model': '+GraphFea', 'considered': 158, 'salvaged': 43, 'salvage_rate_pct': 27.215, 'avg_extra_km': 217.185, 'avg_extra_ms': 1.086}\n",
      "Δ (enr - base): {'salvage_rate_pct': 4.32, 'avg_extra_km': 4.751, 'avg_extra_ms': 0.024}\n"
     ]
    }
   ],
   "source": [
    "# Step 16 (fixed): QoT-guided rerouting — BASE vs +GraphFea (zero-shot, HARD)\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- deps\n",
    "try:\n",
    "    import networkx as nx\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install networkx: pip install networkx\")\n",
    "\n",
    "# ---- load data\n",
    "ENRICHED_CSV = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "assert ENRICHED_CSV.exists(), \"Run Step 14 first to create outputs/paths_graph_enriched.csv\"\n",
    "df = pd.read_csv(ENRICHED_CSV)\n",
    "\n",
    "try:\n",
    "    links\n",
    "except NameError:\n",
    "    links = pd.read_csv(\"eon_links_timeseries.csv\")\n",
    "\n",
    "# ---- splits\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()\n",
    "test  = df[(df[\"split\"]==\"test_target\") & (df[\"topology\"]==\"GEANT2\")].copy()\n",
    "\n",
    "# ---- features (HARD setup)\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "GF_COLS = [\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "    \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]\n",
    "ENR_QOT = BASE_QOT + GF_COLS\n",
    "CAT = ['modulation']\n",
    "\n",
    "# consistent modulation encoding based on train categories\n",
    "train_mod_cats = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def encode_mod(series):\n",
    "    return pd.Categorical(series, categories=train_mod_cats).codes\n",
    "\n",
    "def to_ordered_df(df_in, col_order):\n",
    "    \"\"\"Return df with columns exactly in col_order; raises if a col is missing.\"\"\"\n",
    "    missing = [c for c in col_order if c not in df_in.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns for model: {missing}\")\n",
    "    df_out = df_in[col_order].copy()\n",
    "    return df_out\n",
    "\n",
    "# ---- train helper returning scaler + exact col order used\n",
    "def fit_qot(Xtr_raw, ytr, col_order):\n",
    "    sc = StandardScaler()\n",
    "    Xtr = to_ordered_df(Xtr_raw, col_order)\n",
    "    Xs  = sc.fit_transform(Xtr)\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                        alpha=1e-4, learning_rate_init=1e-3,\n",
    "                        max_iter=200, random_state=42)\n",
    "    clf.fit(Xs, ytr)\n",
    "    return clf, sc, col_order\n",
    "\n",
    "# ---- train BASE model\n",
    "col_order_base = BASE_QOT + CAT\n",
    "Xtr_base = train[col_order_base].copy()\n",
    "Xtr_base['modulation'] = encode_mod(Xtr_base['modulation'])\n",
    "ytr = train['qot_ok'].astype(int).values\n",
    "clf_base, sc_base, col_order_base = fit_qot(Xtr_base, ytr, col_order_base)\n",
    "\n",
    "# ---- train +GraphFea model\n",
    "col_order_enr = ENR_QOT + CAT\n",
    "Xtr_enr = train[col_order_enr].copy()\n",
    "Xtr_enr['modulation'] = encode_mod(Xtr_enr['modulation'])\n",
    "clf_enr, sc_enr, col_order_enr = fit_qot(Xtr_enr, ytr, col_order_enr)\n",
    "\n",
    "# ---- graph build + utilities\n",
    "def build_graph_for_day(day):\n",
    "    sub = links[(links['topology']=='GEANT2') & (links['day']==day)]\n",
    "    G = nx.Graph()\n",
    "    for _, r in sub.iterrows():\n",
    "        u, v = int(r['u']), int(r['v'])\n",
    "        G.add_edge(u, v,\n",
    "                   edge_id=f\"{min(u,v)}-{max(u,v)}\",\n",
    "                   length_km=float(r['length_km']),\n",
    "                   latency_ms=float(r['latency_ms']))\n",
    "    return G, sub.set_index('edge_id')\n",
    "\n",
    "def edge_ids_from_nodes(nodes):\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(nodes[:-1], nodes[1:])]\n",
    "\n",
    "def path_aggs(eids, idx):\n",
    "    rows = idx.loc[eids]\n",
    "    hops = len(eids)\n",
    "    base = {\n",
    "        \"hops\": hops,\n",
    "        \"distance_km\": float(rows['length_km'].sum()),\n",
    "        \"latency_ms\":  float(rows['latency_ms'].sum()),\n",
    "        \"avg_utilization\": float(rows['bandwidth_utilization'].mean()),\n",
    "        \"min_osnr_db\": float(rows['osnr_db'].min()),\n",
    "        \"min_snr_db\":  float(rows['snr_db'].min()),\n",
    "        \"max_center_offset_ghz\": float(rows['center_freq_offset_ghz'].max()),\n",
    "        \"min_filter_bw_scale\":   float(rows['filter_bw_scale'].min()),\n",
    "    }\n",
    "    osnrs = rows['osnr_db'].values\n",
    "    utils = rows['bandwidth_utilization'].values\n",
    "    shifts= rows['center_freq_offset_ghz'].values\n",
    "    scales= rows['filter_bw_scale'].values\n",
    "    gf = {\n",
    "        \"gf_osnr_min\": float(osnrs.min()),\n",
    "        \"gf_osnr_var\": float(np.var(osnrs)) if hops>1 else 0.0,\n",
    "        \"gf_util_mean\": float(utils.mean()),\n",
    "        \"gf_util_max\": float(utils.max()),\n",
    "        \"gf_shift_max\": float(shifts.max()),\n",
    "        \"gf_scale_min\": float(scales.min()),\n",
    "        \"gf_frac_shifted\": float((shifts>0).mean()),\n",
    "        \"gf_frac_tight\": float((scales<1.0).mean()),\n",
    "        \"gf_bot_pos\": float(np.argmin(osnrs)/max(1,hops-1))\n",
    "    }\n",
    "    return base, gf\n",
    "\n",
    "def features_for_candidate(eids, day, tx_row, idx, use_graph):\n",
    "    base, gf = path_aggs(eids, idx)\n",
    "    row = {\n",
    "        **base,\n",
    "        \"symbol_rate_gbaud\": float(tx_row['symbol_rate_gbaud']),\n",
    "        \"bitrate_gbps\": float(tx_row['bitrate_gbps']),\n",
    "        \"modulation\": tx_row['modulation']\n",
    "    }\n",
    "    if use_graph:\n",
    "        row.update(gf)\n",
    "    return row\n",
    "\n",
    "# ---- core evaluation\n",
    "def reroute_eval(model_tag, feat_cols, clf, scaler, col_order, K=3, sample_n=600, seed=7):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    sample = test.sample(n=min(sample_n, len(test)), random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # original predictions (who needs reroute?)\n",
    "    Xtest = sample[feat_cols + CAT].copy()\n",
    "    Xtest['modulation'] = encode_mod(Xtest['modulation'])\n",
    "    Xtest_ord = to_ordered_df(Xtest, col_order)\n",
    "    p = clf.predict_proba(scaler.transform(Xtest_ord))[:,1]\n",
    "    yhat = (p>=0.5).astype(int)\n",
    "\n",
    "    considered = salvaged = 0\n",
    "    extra_dist, extra_lat = [], []\n",
    "\n",
    "    for i, row in sample.iterrows():\n",
    "        if int(yhat[i]) == 1:\n",
    "            continue  # already predicted feasible\n",
    "\n",
    "        day = int(row['day'])\n",
    "        try:\n",
    "            G, idx = build_graph_for_day(day)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        nodes = [int(x) for x in row['path'].split('->')]\n",
    "        try:\n",
    "            alt_iter = nx.shortest_simple_paths(G, nodes[0], nodes[-1], weight=lambda u,v,d: d['length_km'])\n",
    "        except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "            continue\n",
    "\n",
    "        orig_eids = edge_ids_from_nodes(nodes)\n",
    "        considered += 1\n",
    "        tried = 0\n",
    "        for cand_nodes in alt_iter:\n",
    "            if cand_nodes == nodes:\n",
    "                continue\n",
    "            tried += 1\n",
    "            if tried > K:\n",
    "                break\n",
    "            cand_eids = edge_ids_from_nodes(cand_nodes)\n",
    "            # candidate features\n",
    "            try:\n",
    "                row_feat = features_for_candidate(cand_eids, day, row, idx, use_graph=(model_tag==\"+GraphFea\"))\n",
    "            except KeyError:\n",
    "                continue  # missing edge stats\n",
    "            Xc = pd.DataFrame([row_feat])\n",
    "            Xc['modulation'] = encode_mod(Xc['modulation'])\n",
    "            # reorder columns EXACTLY as in fit\n",
    "            Xc_ord = to_ordered_df(Xc, col_order)\n",
    "            pred_good = int(clf.predict(scaler.transform(Xc_ord))[0]) == 1\n",
    "            if pred_good:\n",
    "                # overhead stats\n",
    "                try:\n",
    "                    orig_dist = float(idx.loc[orig_eids]['length_km'].sum())\n",
    "                    cand_dist = float(idx.loc[cand_eids]['length_km'].sum())\n",
    "                    orig_lat  = float(idx.loc[orig_eids]['latency_ms'].sum())\n",
    "                    cand_lat  = float(idx.loc[cand_eids]['latency_ms'].sum())\n",
    "                except KeyError:\n",
    "                    orig_dist = cand_dist = orig_lat = cand_lat = np.nan\n",
    "                extra_dist.append(cand_dist - orig_dist if (not np.isnan(cand_dist) and not np.isnan(orig_dist)) else np.nan)\n",
    "                extra_lat.append(cand_lat - orig_lat if (not np.isnan(cand_lat) and not np.isnan(orig_lat)) else np.nan)\n",
    "                salvaged += 1\n",
    "                break\n",
    "\n",
    "    # summarize\n",
    "    rate = 100.0*salvaged/max(1,considered)\n",
    "    # drop NaNs for means\n",
    "    ed = [x for x in extra_dist if not (isinstance(x,float) and np.isnan(x))]\n",
    "    el = [x for x in extra_lat  if not (isinstance(x,float) and np.isnan(x))]\n",
    "    od_km = float(np.mean(ed)) if ed else float(\"nan\")\n",
    "    ol_ms = float(np.mean(el)) if el else float(\"nan\")\n",
    "    return dict(model=model_tag, considered=considered, salvaged=salvaged,\n",
    "                salvage_rate_pct=rate, avg_extra_km=od_km, avg_extra_ms=ol_ms)\n",
    "\n",
    "# ---- run both models\n",
    "res_base = reroute_eval(\"BASE\", BASE_QOT, clf_base, sc_base, col_order_base, K=3, sample_n=600)\n",
    "res_enr  = reroute_eval(\"+GraphFea\", ENR_QOT, clf_enr, sc_enr, col_order_enr, K=3, sample_n=600)\n",
    "\n",
    "# ---- print paper-ready summary\n",
    "def r4(d): return {k:(round(v,3) if isinstance(v,(int,float)) else v) for k,v in d.items()}\n",
    "print(\"QoT-guided Rerouting (GEANT2, zero-shot, HARD)\")\n",
    "print(\"BASE     :\", r4(res_base))\n",
    "print(\"+GraphFea:\", r4(res_enr))\n",
    "print(\"Δ (enr - base):\", {\n",
    "    \"salvage_rate_pct\": round(res_enr[\"salvage_rate_pct\"]-res_base[\"salvage_rate_pct\"], 2),\n",
    "    \"avg_extra_km\":     (round(res_enr[\"avg_extra_km\"]-res_base[\"avg_extra_km\"], 3)\n",
    "                          if all(isinstance(x,(int,float)) and not np.isnan(x) \n",
    "                                 for x in [res_enr[\"avg_extra_km\"], res_base[\"avg_extra_km\"]]) else \"n/a\"),\n",
    "    \"avg_extra_ms\":     (round(res_enr[\"avg_extra_ms\"]-res_base[\"avg_extra_ms\"], 3)\n",
    "                          if all(isinstance(x,(int,float)) and not np.isnan(x) \n",
    "                                 for x in [res_enr[\"avg_extra_ms\"], res_base[\"avg_extra_ms\"]]) else \"n/a\"),\n",
    "})\n",
    "# save JSON for tables\n",
    "import json; OUTM = Path(\"./outputs/metrics\"); OUTM.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUTM/\"reroute_base.json\",\"w\") as f: json.dump(res_base, f, indent=2)\n",
    "with open(OUTM/\"reroute_graph.json\",\"w\") as f: json.dump(res_enr,  f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72daba06-62ca-45b6-a7ca-f824551280cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in c:\\users\\devon\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3df7caf-bbda-4748-9bdc-95c1e12659f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QOT  k=  0  ->  acc=0.9598  f1=0.9473  auc=0.9938\n",
      "QOT  k= 10  ->  acc=0.9662  f1=0.9559  auc=0.9959\n",
      "QOT  k= 40  ->  acc=0.9681  f1=0.9577  auc=0.9954\n",
      "QOT  k=100  ->  acc=0.9759  f1=0.9684  auc=0.9976\n",
      "QOT  k=200  ->  acc=0.9819  f1=0.9758  auc=0.9986\n",
      "FAIL  k=  0  ->  acc=0.9931  f1=0.9862\n",
      "FAIL  k= 10  ->  acc=0.9939  f1=0.9876\n",
      "FAIL  k= 40  ->  acc=0.9951  f1=0.9902\n",
      "FAIL  k=100  ->  acc=0.9949  f1=0.9895\n",
      "FAIL  k=200  ->  acc=0.9956  f1=0.9905\n",
      "Saved:\n",
      " - outputs\\metrics\\kshot_qot_enriched.csv\n",
      " - outputs\\metrics\\kshot_fail_enriched.csv\n",
      " - outputs\\figs\\kshot_qot_enriched.png\n",
      " - outputs\\figs\\kshot_fail_enriched.png\n"
     ]
    }
   ],
   "source": [
    "# Step 17 (fixed): k-shot curves (QoT & Failure) with graph-enriched features\n",
    "# This cell computes gf_* for few-shot rows (if missing), then runs k in {0,10,40,100,200}.\n",
    "# Outputs:\n",
    "#   ./outputs/metrics/kshot_qot_enriched.csv\n",
    "#   ./outputs/metrics/kshot_fail_enriched.csv\n",
    "#   ./outputs/figs/kshot_qot_enriched.png\n",
    "#   ./outputs/figs/kshot_fail_enriched.png\n",
    "\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# ----------------- Load data -----------------\n",
    "ENRICHED = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "FEWSHOT  = Path(\"./eon_target_fewshot.csv\")\n",
    "LINKS    = Path(\"./eon_links_timeseries.csv\")\n",
    "assert ENRICHED.exists(), \"Run Step 14 first to generate paths_graph_enriched.csv\"\n",
    "assert FEWSHOT.exists(),  \"Few-shot CSV not found: eon_target_fewshot.csv\"\n",
    "assert LINKS.exists(),    \"Missing eon_links_timeseries.csv in working folder\"\n",
    "\n",
    "df  = pd.read_csv(ENRICHED)       # already has gf_* columns\n",
    "few = pd.read_csv(FEWSHOT)\n",
    "links = pd.read_csv(LINKS)        # used to compute gf_* for few-shot\n",
    "\n",
    "# Splits from enriched paths\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()   # NSFNET\n",
    "test  = df[df[\"split\"]==\"test_target\"].copy()    # GEANT2 (has gf_*)\n",
    "\n",
    "# ----------------- Graph feature builder (for few-shot rows) -----------------\n",
    "lk_idx = links.set_index([\"topology\",\"day\",\"edge_id\"])\n",
    "\n",
    "def edge_ids_from_path(path_str):\n",
    "    ns = [int(x) for x in str(path_str).split(\"->\")]\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(ns[:-1], ns[1:])]\n",
    "\n",
    "GF_COLS = [\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "    \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]\n",
    "\n",
    "def compute_graph_feats_for_df(df_in):\n",
    "    \"\"\"Return df_in joined with gf_* columns; drops rows where link join fails.\"\"\"\n",
    "    rows = []\n",
    "    miss = 0\n",
    "    for _, r in df_in.iterrows():\n",
    "        topo, day, path = r[\"topology\"], r[\"day\"], r[\"path\"]\n",
    "        eids = edge_ids_from_path(path)\n",
    "        try:\n",
    "            rows_link = lk_idx.loc[(topo, day, eids)]\n",
    "        except KeyError:\n",
    "            miss += 1\n",
    "            continue\n",
    "        hops = len(eids)\n",
    "        osnrs = rows_link[\"osnr_db\"].values\n",
    "        utils = rows_link[\"bandwidth_utilization\"].values\n",
    "        shifts= rows_link[\"center_freq_offset_ghz\"].values\n",
    "        scales= rows_link[\"filter_bw_scale\"].values\n",
    "        gf = dict(\n",
    "            gf_osnr_min = float(osnrs.min()),\n",
    "            gf_osnr_var = float(np.var(osnrs)) if hops>1 else 0.0,\n",
    "            gf_util_mean= float(utils.mean()),\n",
    "            gf_util_max = float(utils.max()),\n",
    "            gf_shift_max= float(shifts.max()),\n",
    "            gf_scale_min= float(scales.min()),\n",
    "            gf_frac_shifted = float((shifts>0).mean()),\n",
    "            gf_frac_tight   = float((scales<1.0).mean()),\n",
    "            gf_bot_pos = float(np.argmin(osnrs)/max(1,hops-1))\n",
    "        )\n",
    "        base_row = r.to_dict(); base_row.update(gf); rows.append(base_row)\n",
    "    if miss:\n",
    "        print(f\"[info] compute_graph_feats_for_df: dropped {miss} rows due to missing link joins.\")\n",
    "    return pd.DataFrame(rows) if rows else df_in.assign(**{c: np.nan for c in GF_COLS})\n",
    "\n",
    "def ensure_gf_columns(df_subset):\n",
    "    \"\"\"If gf_* missing in df_subset, compute & return a copy with gf_* present.\"\"\"\n",
    "    if all(c in df_subset.columns for c in GF_COLS):\n",
    "        return df_subset.copy()\n",
    "    res = compute_graph_feats_for_df(df_subset)\n",
    "    # If some gf_* still missing (shouldn't), fill with NaN then drop\n",
    "    for c in GF_COLS:\n",
    "        if c not in res.columns:\n",
    "            res[c] = np.nan\n",
    "    # drop rows with NaNs in gf_* to keep training clean\n",
    "    res = res.dropna(subset=GF_COLS)\n",
    "    return res\n",
    "\n",
    "# ----------------- Features (HARD setup) -----------------\n",
    "# QoT (HARD): drop req/margin; add gf_*\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "ENR_QOT = BASE_QOT + GF_COLS\n",
    "\n",
    "# Failure (HARD): base without direct fingerprints; add gf_*\n",
    "BASE_FAIL = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "ENR_FAIL = BASE_FAIL + GF_COLS\n",
    "\n",
    "CAT = ['modulation']\n",
    "\n",
    "# Consistent modulation encoding from train\n",
    "train_mod_cats = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def enc_mod(s):\n",
    "    return pd.Categorical(s, categories=train_mod_cats).codes\n",
    "\n",
    "def ordered(df_in, cols):\n",
    "    missing = [c for c in cols if c not in df_in.columns]\n",
    "    if missing: \n",
    "        raise KeyError(f\"Missing columns: {missing}\")\n",
    "    return df_in[cols].copy()\n",
    "\n",
    "# ----------------- Few-shot held-out split -----------------\n",
    "KEY = ['topology','day','src','dst','path','hops','distance_km']\n",
    "def make_heldout(few_subset):\n",
    "    # remove few_subset keys from test to create held-out\n",
    "    test_all = test.copy()\n",
    "    test_all['__key__'] = test_all[KEY].astype(str).agg('|'.join, axis=1)\n",
    "    keys = set(few_subset[KEY].astype(str).agg('|'.join, axis=1))\n",
    "    return test_all[~test_all['__key__'].isin(keys)].copy()\n",
    "\n",
    "# ----------------- Generic runner -----------------\n",
    "def run_kshot(task=\"qot\", k_list=(0,10,40,100,200), seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    results = []\n",
    "    if task==\"qot\":\n",
    "        FEATS = ENR_QOT\n",
    "        ycol  = 'qot_ok'\n",
    "        metrics = [\"acc\",\"f1\",\"auc\"]\n",
    "    elif task==\"fail\":\n",
    "        FEATS = ENR_FAIL\n",
    "        ycol  = 'failure_present'\n",
    "        metrics = [\"acc\",\"f1\"]\n",
    "    else:\n",
    "        raise ValueError(\"task must be 'qot' or 'fail'\")\n",
    "\n",
    "    # Train on NSFNET (already enriched df)\n",
    "    Xtr = train[FEATS + CAT].copy()\n",
    "    Xtr['modulation'] = enc_mod(Xtr['modulation'])\n",
    "    ytr = train[ycol].astype(int).values\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s  = scaler.fit_transform(ordered(Xtr, FEATS + CAT))\n",
    "    base_clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                             alpha=1e-4, learning_rate_init=1e-3,\n",
    "                             max_iter=200, random_state=seed)\n",
    "    base_clf.fit(Xtr_s, ytr)\n",
    "\n",
    "    # shuffle few-shot pool (balanced file)\n",
    "    few_pool = few.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    for k in k_list:\n",
    "        few_k = few_pool.iloc[:min(k, len(few_pool))].copy()\n",
    "        if k > 0:\n",
    "            # ensure gf_* present for few-shot subset\n",
    "            few_k = ensure_gf_columns(few_k)\n",
    "\n",
    "        held = make_heldout(few_k)  # held from enriched df (already has gf_*)\n",
    "\n",
    "        # Held-out matrices\n",
    "        Xho = held[FEATS + CAT].copy()\n",
    "        Xho['modulation'] = enc_mod(Xho['modulation'])\n",
    "        yho = held[ycol].astype(int).values\n",
    "        Xho_s = scaler.transform(ordered(Xho, FEATS + CAT))\n",
    "\n",
    "        if k == 0:\n",
    "            clf = base_clf\n",
    "        else:\n",
    "            # Build few-shot matrix\n",
    "            Xfs = few_k[FEATS + CAT].copy()\n",
    "            Xfs['modulation'] = enc_mod(Xfs['modulation'])\n",
    "            yfs = few_k[ycol].astype(int).values\n",
    "            Xfs_s = scaler.transform(ordered(Xfs, FEATS + CAT))\n",
    "            # Warm-start refit on source + few-shot (stable alternative to partial_fit)\n",
    "            X_comb = np.vstack([Xtr_s, Xfs_s])\n",
    "            y_comb = np.concatenate([ytr,  yfs])\n",
    "            clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                                alpha=1e-4, learning_rate_init=1e-3,\n",
    "                                max_iter=200, random_state=seed)\n",
    "            clf.fit(X_comb, y_comb)\n",
    "\n",
    "        if task==\"qot\":\n",
    "            probs = clf.predict_proba(Xho_s)[:,1]\n",
    "            yhat  = (probs>=0.5).astype(int)\n",
    "            row = dict(task=task, k=k,\n",
    "                       acc=accuracy_score(yho, yhat),\n",
    "                       f1 =f1_score(yho, yhat, average='macro'),\n",
    "                       auc=roc_auc_score(yho, probs))\n",
    "        else:\n",
    "            yhat = clf.predict(Xho_s)\n",
    "            row = dict(task=task, k=k,\n",
    "                       acc=accuracy_score(yho, yhat),\n",
    "                       f1 =f1_score(yho, yhat, average='macro'))\n",
    "        results.append(row)\n",
    "        print(f\"{task.upper()}  k={k:3d}  ->  \" + \"  \".join([f\"{m}={row[m]:.4f}\" for m in metrics]))\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ----------------- Run curves & save -----------------\n",
    "OUTM = Path(\"./outputs/metrics\"); OUTF = Path(\"./outputs/figs\")\n",
    "OUTM.mkdir(parents=True, exist_ok=True); OUTF.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_qot  = run_kshot(\"qot\",  k_list=[0,10,40,100,200], seed=42)\n",
    "df_fail = run_kshot(\"fail\", k_list=[0,10,40,100,200], seed=42)\n",
    "\n",
    "df_qot.to_csv(OUTM/\"kshot_qot_enriched.csv\", index=False)\n",
    "df_fail.to_csv(OUTM/\"kshot_fail_enriched.csv\", index=False)\n",
    "\n",
    "# ----------------- Plots -----------------\n",
    "plt.figure()\n",
    "plt.plot(df_qot['k'], df_qot['f1'], marker='o', label='QoT F1')\n",
    "plt.plot(df_qot['k'], df_qot['auc'], marker='s', label='QoT AUC')\n",
    "plt.xlabel(\"k (few-shot labels)\"); plt.ylabel(\"score\"); plt.title(\"QoT — k-shot on GEANT2 (graph-enriched, HARD)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.savefig(OUTF/\"kshot_qot_enriched.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_fail['k'], df_fail['f1'], marker='o', label='Failure F1')\n",
    "plt.xlabel(\"k (few-shot labels)\"); plt.ylabel(\"F1\"); plt.title(\"Failure Detection — k-shot on GEANT2 (graph-enriched, HARD)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.savefig(OUTF/\"kshot_fail_enriched.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", OUTM/\"kshot_qot_enriched.csv\")\n",
    "print(\" -\", OUTM/\"kshot_fail_enriched.csv\")\n",
    "print(\" -\", OUTF/\"kshot_qot_enriched.png\")\n",
    "print(\" -\", OUTF/\"kshot_fail_enriched.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c244219e-8bf6-4878-913a-1ec10b8283f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devon\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\devon\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs\\metrics\\ablations_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>scenario</th>\n",
       "      <th>k</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qot</td>\n",
       "      <td>base</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980926</td>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.998353</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qot</td>\n",
       "      <td>+graph</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959815</td>\n",
       "      <td>0.947331</td>\n",
       "      <td>0.993752</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qot</td>\n",
       "      <td>+graph_noFP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.961667</td>\n",
       "      <td>0.947639</td>\n",
       "      <td>0.995775</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qot</td>\n",
       "      <td>+graph_noOSNR</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959444</td>\n",
       "      <td>0.944633</td>\n",
       "      <td>0.993406</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qot</td>\n",
       "      <td>+graph</td>\n",
       "      <td>100</td>\n",
       "      <td>0.974533</td>\n",
       "      <td>0.965945</td>\n",
       "      <td>0.995881</td>\n",
       "      <td>5301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fail</td>\n",
       "      <td>base</td>\n",
       "      <td>0</td>\n",
       "      <td>0.841481</td>\n",
       "      <td>0.493459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fail</td>\n",
       "      <td>+graph</td>\n",
       "      <td>0</td>\n",
       "      <td>0.993148</td>\n",
       "      <td>0.986161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fail</td>\n",
       "      <td>+graph_noFP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818148</td>\n",
       "      <td>0.488958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fail</td>\n",
       "      <td>+graph_noOSNR</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996111</td>\n",
       "      <td>0.992187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fail</td>\n",
       "      <td>+graph</td>\n",
       "      <td>100</td>\n",
       "      <td>0.995284</td>\n",
       "      <td>0.990244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fail</td>\n",
       "      <td>+graph_shift</td>\n",
       "      <td>0</td>\n",
       "      <td>0.979123</td>\n",
       "      <td>0.494726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fail</td>\n",
       "      <td>+graph_tighten</td>\n",
       "      <td>0</td>\n",
       "      <td>0.945338</td>\n",
       "      <td>0.485950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fail</td>\n",
       "      <td>+graph_shift</td>\n",
       "      <td>100</td>\n",
       "      <td>0.995575</td>\n",
       "      <td>0.498891</td>\n",
       "      <td>NaN</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fail</td>\n",
       "      <td>+graph_tighten</td>\n",
       "      <td>100</td>\n",
       "      <td>0.979239</td>\n",
       "      <td>0.494755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task        scenario    k       acc        f1       auc     n\n",
       "0    qot            base    0  0.980926  0.975380  0.998353  5400\n",
       "1    qot          +graph    0  0.959815  0.947331  0.993752  5400\n",
       "2    qot     +graph_noFP    0  0.961667  0.947639  0.995775  5400\n",
       "3    qot   +graph_noOSNR    0  0.959444  0.944633  0.993406  5400\n",
       "4    qot          +graph  100  0.974533  0.965945  0.995881  5301\n",
       "5   fail            base    0  0.841481  0.493459       NaN  5400\n",
       "6   fail          +graph    0  0.993148  0.986161       NaN  5400\n",
       "7   fail     +graph_noFP    0  0.818148  0.488958       NaN  5400\n",
       "8   fail   +graph_noOSNR    0  0.996111  0.992187       NaN  5400\n",
       "9   fail          +graph  100  0.995284  0.990244       NaN  5301\n",
       "10  fail    +graph_shift    0  0.979123  0.494726       NaN   479\n",
       "11  fail  +graph_tighten    0  0.945338  0.485950       NaN   311\n",
       "12  fail    +graph_shift  100  0.995575  0.498891       NaN   452\n",
       "13  fail  +graph_tighten  100  0.979239  0.494755       NaN   289"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots:\n",
      " - outputs\\figs\\abl_qot.png\n",
      " - outputs\\figs\\abl_fail.png\n",
      " - outputs\\figs\\abl_fail_shift_tight.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devon\\AppData\\Local\\Temp\\ipykernel_29000\\3360143247.py:225: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  f1_0 = [float(sub0[sub0['scenario']==f'+graph_{lab}']['f1']) if not sub0[sub0['scenario']==f'+graph_{lab}'].empty else np.nan for lab in labels]\n",
      "C:\\Users\\devon\\AppData\\Local\\Temp\\ipykernel_29000\\3360143247.py:226: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  f1_1 = [float(sub1[sub1['scenario']==f'+graph_{lab}']['f1']) if not sub1[sub1['scenario']==f'+graph_{lab}'].empty else np.nan for lab in labels]\n"
     ]
    }
   ],
   "source": [
    "# Step 18: Ablations — Base vs +Graph; telemetry masks; shift/tighten breakdown; optional k=100\n",
    "# Outputs:\n",
    "#   ./outputs/metrics/ablations_summary.csv\n",
    "#   ./outputs/figs/abl_qot.png\n",
    "#   ./outputs/figs/abl_fail.png\n",
    "#   ./outputs/figs/abl_fail_shift_tight.png\n",
    "\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# ---------- Load ----------\n",
    "ENRICHED = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "FEWSHOT  = Path(\"./eon_target_fewshot.csv\")\n",
    "LINKS    = Path(\"./eon_links_timeseries.csv\")\n",
    "assert ENRICHED.exists(), \"Run Step 14 first to generate paths_graph_enriched.csv\"\n",
    "assert FEWSHOT.exists(),  \"Missing eon_target_fewshot.csv\"\n",
    "assert LINKS.exists(),    \"Missing eon_links_timeseries.csv\"\n",
    "\n",
    "df   = pd.read_csv(ENRICHED)     # has gf_* columns\n",
    "few  = pd.read_csv(FEWSHOT)\n",
    "links= pd.read_csv(LINKS)        # only needed if we must recompute gf_* for few-shot\n",
    "\n",
    "OUTM = Path(\"./outputs/metrics\"); OUTF = Path(\"./outputs/figs\")\n",
    "OUTM.mkdir(parents=True, exist_ok=True); OUTF.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Splits ----------\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()   # NSFNET\n",
    "test  = df[df[\"split\"]==\"test_target\"].copy()    # GEANT2 (target)\n",
    "\n",
    "# ---------- Graph features list ----------\n",
    "GF_COLS = [\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "    \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]\n",
    "\n",
    "# ---------- Feature sets (HARD) ----------\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "ENR_QOT = BASE_QOT + GF_COLS\n",
    "\n",
    "BASE_FAIL = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "ENR_FAIL = BASE_FAIL + GF_COLS\n",
    "\n",
    "# Telemetry masks:\n",
    "# - No fingerprints: drop offset/scale summaries/signals\n",
    "NO_FP_DROP = ['max_center_offset_ghz','min_filter_bw_scale','gf_shift_max','gf_scale_min','gf_frac_shifted','gf_frac_tight']\n",
    "# - No OSNR: drop OSNR & its gf summaries\n",
    "NO_OSNR_DROP = ['min_osnr_db','gf_osnr_min','gf_osnr_var']\n",
    "\n",
    "CAT = ['modulation']\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "train_mod_cats = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def enc_mod(s): return pd.Categorical(s, categories=train_mod_cats).codes\n",
    "def ordered(df_in, cols):\n",
    "    miss = [c for c in cols if c not in df_in.columns]\n",
    "    if miss: raise KeyError(f\"Missing columns: {miss}\")\n",
    "    return df_in[cols].copy()\n",
    "\n",
    "KEY = ['topology','day','src','dst','path','hops','distance_km']\n",
    "def heldout_after_few(few_subset):\n",
    "    test_all = test.copy()\n",
    "    test_all['__key__'] = test_all[KEY].astype(str).agg('|'.join, axis=1)\n",
    "    keys = set(few_subset[KEY].astype(str).agg('|'.join, axis=1))\n",
    "    return test_all[~test_all['__key__'].isin(keys)].copy()\n",
    "\n",
    "def ensure_gf_for_few(df_subset):\n",
    "    \"\"\"few-shot CSV may not have gf_*; join from enriched df by composite key.\"\"\"\n",
    "    if all(c in df_subset.columns for c in GF_COLS):\n",
    "        return df_subset.copy()\n",
    "    join_cols = KEY + ['modulation','qot_ok','failure_present','failure_type','symbol_rate_gbaud','bitrate_gbps',\n",
    "                       'avg_utilization','min_snr_db','max_center_offset_ghz','min_filter_bw_scale']\n",
    "    # build key in both, then merge gf_* from enriched df\n",
    "    tmp_f = df_subset.copy()\n",
    "    tmp_f['__key__'] = tmp_f[KEY].astype(str).agg('|'.join, axis=1)\n",
    "    tmp_d = df.copy()\n",
    "    tmp_d['__key__'] = tmp_d[KEY].astype(str).agg('|'.join, axis=1)\n",
    "    gf_cols_present = [c for c in GF_COLS if c in tmp_d.columns]\n",
    "    merged = tmp_f.merge(tmp_d[['__key__']+gf_cols_present], on='__key__', how='left')\n",
    "    # drop rows where gf_* could not be found\n",
    "    merged = merged.dropna(subset=gf_cols_present)\n",
    "    # keep original columns + gf_*\n",
    "    for c in gf_cols_present:\n",
    "        if c not in merged.columns: merged[c] = np.nan\n",
    "    return merged\n",
    "\n",
    "def train_and_eval(task, feat_list, k=0, subset=None, seed=42):\n",
    "    \"\"\"Train on NSFNET; optional k-shot on GEANT2; evaluate on GEANT2 held-out.\n",
    "       subset: None / 'shift' / 'tighten' (only for failure task).\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    ycol = 'qot_ok' if task=='qot' else 'failure_present'\n",
    "    # prepare training\n",
    "    Xtr = train[feat_list + CAT].copy()\n",
    "    Xtr['modulation'] = enc_mod(Xtr['modulation'])\n",
    "    ytr = train[ycol].astype(int).values\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = scaler.fit_transform(ordered(Xtr, feat_list + CAT))\n",
    "    base_clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                             alpha=1e-4, learning_rate_init=1e-3,\n",
    "                             max_iter=200, random_state=seed)\n",
    "    base_clf.fit(Xtr_s, ytr)\n",
    "    # few-shot subset (balanced file); ensure gf_* present if using ENR features\n",
    "    few_pool = few.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    few_k = few_pool.iloc[:min(k, len(few_pool))].copy()\n",
    "    if k>0 and any(c.startswith('gf_') for c in feat_list):\n",
    "        few_k = ensure_gf_for_few(few_k)\n",
    "    held = heldout_after_few(few_k)\n",
    "    # optional subset filter (only for failure task)\n",
    "    if subset in ('shift','tighten'):\n",
    "        held = held[(held['failure_present']==1) & (held['failure_type']==subset)].copy()\n",
    "        if len(held)==0:\n",
    "            return dict(acc=np.nan, f1=np.nan, auc=(np.nan if task=='qot' else None), n=0)\n",
    "    # build held matrices\n",
    "    Xho = held[feat_list + CAT].copy()\n",
    "    Xho['modulation'] = enc_mod(Xho['modulation'])\n",
    "    yho = held[ycol].astype(int).values\n",
    "    Xho_s = scaler.transform(ordered(Xho, feat_list + CAT))\n",
    "    # choose classifier: base or refit with few-shot\n",
    "    clf = base_clf\n",
    "    if k>0:\n",
    "        Xfs = few_k[feat_list + CAT].copy()\n",
    "        Xfs['modulation'] = enc_mod(Xfs['modulation'])\n",
    "        yfs = few_k[ycol].astype(int).values\n",
    "        Xfs_s = scaler.transform(ordered(Xfs, feat_list + CAT))\n",
    "        Xc = np.vstack([Xtr_s, Xfs_s])\n",
    "        yc = np.concatenate([ytr, yfs])\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                            alpha=1e-4, learning_rate_init=1e-3,\n",
    "                            max_iter=200, random_state=seed)\n",
    "        clf.fit(Xc, yc)\n",
    "    # eval\n",
    "    if task=='qot':\n",
    "        prob = clf.predict_proba(Xho_s)[:,1]\n",
    "        yhat = (prob>=0.5).astype(int)\n",
    "        return dict(acc=accuracy_score(yho,yhat), f1=f1_score(yho,yhat,average='macro'),\n",
    "                    auc=roc_auc_score(yho,prob), n=len(held))\n",
    "    else:\n",
    "        yhat = clf.predict(Xho_s)\n",
    "        return dict(acc=accuracy_score(yho,yhat), f1=f1_score(yho,yhat,average='macro'),\n",
    "                    n=len(held))\n",
    "\n",
    "# ---------- Build scenarios ----------\n",
    "scenarios = []\n",
    "\n",
    "# QoT: Base / +Graph / +Graph-NoFP / +Graph-NoOSNR (k=0); +Graph (k=100)\n",
    "qot_sets = {\n",
    "    \"base\": ENR_QOT.copy(),   # start from enriched list to make drop logic easy, then remove gf_* for base\n",
    "}\n",
    "# For QoT base we want BASE_QOT only:\n",
    "qot_sets[\"base\"] = BASE_QOT\n",
    "qot_sets[\"+graph\"] = ENR_QOT\n",
    "qot_sets[\"+graph_noFP\"] = [c for c in ENR_QOT if c not in NO_FP_DROP]\n",
    "qot_sets[\"+graph_noOSNR\"] = [c for c in ENR_QOT if c not in NO_OSNR_DROP]\n",
    "\n",
    "for name, feats in qot_sets.items():\n",
    "    m = train_and_eval(\"qot\", feats, k=0, subset=None)\n",
    "    scenarios.append(dict(task=\"qot\", scenario=name, k=0, **m))\n",
    "# few-shot k=100 on +graph\n",
    "m = train_and_eval(\"qot\", ENR_QOT, k=100, subset=None)\n",
    "scenarios.append(dict(task=\"qot\", scenario=\"+graph\", k=100, **m))\n",
    "\n",
    "# Failure: Base / +Graph / +Graph-NoFP / +Graph-NoOSNR (k=0); +Graph (k=100)\n",
    "fail_sets = {\n",
    "    \"base\": BASE_FAIL,\n",
    "    \"+graph\": ENR_FAIL,\n",
    "    \"+graph_noFP\": [c for c in ENR_FAIL if c not in NO_FP_DROP],\n",
    "    \"+graph_noOSNR\": [c for c in ENR_FAIL if c not in NO_OSNR_DROP]\n",
    "}\n",
    "for name, feats in fail_sets.items():\n",
    "    m = train_and_eval(\"fail\", feats, k=0, subset=None)\n",
    "    scenarios.append(dict(task=\"fail\", scenario=name, k=0, auc=None, **m))\n",
    "# few-shot k=100 on +graph\n",
    "m = train_and_eval(\"fail\", ENR_FAIL, k=100, subset=None)\n",
    "scenarios.append(dict(task=\"fail\", scenario=\"+graph\", k=100, auc=None, **m))\n",
    "\n",
    "# Failure per-class breakdown (shift vs tighten) for +graph (k=0 and k=100)\n",
    "for kshot in (0,100):\n",
    "    for cls in (\"shift\",\"tighten\"):\n",
    "        m = train_and_eval(\"fail\", ENR_FAIL, k=kshot, subset=cls)\n",
    "        scenarios.append(dict(task=\"fail\", scenario=f\"+graph_{cls}\", k=kshot, auc=None, **m))\n",
    "\n",
    "# ---------- Save summary ----------\n",
    "df_sum = pd.DataFrame(scenarios)\n",
    "df_sum.to_csv(OUTM/\"ablations_summary.csv\", index=False)\n",
    "print(\"Saved:\", OUTM/\"ablations_summary.csv\")\n",
    "display(df_sum)\n",
    "\n",
    "# ---------- Quick plots (paper-friendly) ----------\n",
    "# (1) QoT F1/AUC bars\n",
    "plt.figure()\n",
    "sub = df_sum[(df_sum.task==\"qot\") & (df_sum.k==0)]\n",
    "x = np.arange(len(sub))\n",
    "plt.bar(x-0.15, sub['f1'], width=0.3, label='F1')\n",
    "plt.bar(x+0.15, sub['auc'], width=0.3, label='AUC')\n",
    "plt.xticks(x, sub['scenario'], rotation=15); plt.ylabel(\"score\"); plt.title(\"QoT (GEANT2, zero-shot, HARD)\")\n",
    "plt.legend(); plt.grid(axis='y', alpha=0.3)\n",
    "plt.savefig(OUTF/\"abl_qot.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# (2) Failure F1 bars\n",
    "plt.figure()\n",
    "sub = df_sum[(df_sum.task==\"fail\") & (df_sum.k==0) & (~df_sum['scenario'].str.contains(\"_\"))]\n",
    "x = np.arange(len(sub))\n",
    "plt.bar(x, sub['f1'], width=0.5, label='F1')\n",
    "plt.xticks(x, sub['scenario'], rotation=15); plt.ylabel(\"F1\"); plt.title(\"Failure Detection (GEANT2, zero-shot, HARD)\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.savefig(OUTF/\"abl_fail.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# (3) Shift vs Tighten (Failure) — +graph @ k=0 vs k=100\n",
    "plt.figure()\n",
    "sub0 = df_sum[(df_sum.task==\"fail\") & (df_sum.k==0) & (df_sum['scenario'].isin(['+graph_shift','+graph_tighten']))]\n",
    "sub1 = df_sum[(df_sum.task==\"fail\") & (df_sum.k==100) & (df_sum['scenario'].isin(['+graph_shift','+graph_tighten']))]\n",
    "labels = ['shift','tighten']\n",
    "x = np.arange(len(labels))\n",
    "f1_0 = [float(sub0[sub0['scenario']==f'+graph_{lab}']['f1']) if not sub0[sub0['scenario']==f'+graph_{lab}'].empty else np.nan for lab in labels]\n",
    "f1_1 = [float(sub1[sub1['scenario']==f'+graph_{lab}']['f1']) if not sub1[sub1['scenario']==f'+graph_{lab}'].empty else np.nan for lab in labels]\n",
    "plt.bar(x-0.15, f1_0, width=0.3, label='k=0')\n",
    "plt.bar(x+0.15, f1_1, width=0.3, label='k=100')\n",
    "plt.xticks(x, labels); plt.ylabel(\"F1\"); plt.title(\"Failure Detection — shift vs tighten (+graph)\")\n",
    "plt.legend(); plt.grid(axis='y', alpha=0.3)\n",
    "plt.ylim(0,1.0)\n",
    "plt.savefig(OUTF/\"abl_fail_shift_tight.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "print(\"Saved plots:\")\n",
    "print(\" -\", OUTF/\"abl_qot.png\")\n",
    "print(\" -\", OUTF/\"abl_fail.png\")\n",
    "print(\" -\", OUTF/\"abl_fail_shift_tight.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9156c12-a1bc-4674-86bb-cf85261e9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pack created at: C:\\devonboard\\research\\daily taskk\\EACE2025\\by gpt\\paper_pack\n",
      "Copied metrics: ['reroute_base.json', 'reroute_graph.json', 'reroute_summary.csv', 'kshot_qot_enriched.csv', 'kshot_fail_enriched.csv', 'ablations_summary.csv']\n",
      "Copied figures: ['qot_roc_zero_vs_tta.png', 'qot_reliability_zero_vs_tta.png', 'qot_risk_coverage_zero_vs_tta.png', 'kshot_qot_enriched.png', 'kshot_fail_enriched.png', 'abl_qot.png', 'abl_fail.png', 'abl_fail_shift_tight.png']\n",
      "\n",
      "All expected files found.\n"
     ]
    }
   ],
   "source": [
    "# Step 19 — Build a paper_pack/ with metrics + figures + summary README\n",
    "import os, json, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "SRC_MET = ROOT/\"outputs/metrics\"\n",
    "SRC_FIG = ROOT/\"outputs/figs\"\n",
    "\n",
    "PACK = ROOT/\"paper_pack\"\n",
    "PMET  = PACK/\"metrics\"\n",
    "PFIG  = PACK/\"figs\"\n",
    "PACK.mkdir(exist_ok=True); PMET.mkdir(parents=True, exist_ok=True); PFIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- files to collect (copy only if present) ----\n",
    "metrics_files = [\n",
    "    \"reroute_base.json\",\n",
    "    \"reroute_graph.json\",\n",
    "    \"reroute_summary.csv\",\n",
    "    \"kshot_qot_enriched.csv\",\n",
    "    \"kshot_fail_enriched.csv\",\n",
    "    \"ablations_summary.csv\",\n",
    "]\n",
    "figs_files = [\n",
    "    \"qot_roc_zero_vs_tta.png\",\n",
    "    \"qot_reliability_zero_vs_tta.png\",\n",
    "    \"qot_risk_coverage_zero_vs_tta.png\",\n",
    "    \"kshot_qot_enriched.png\",\n",
    "    \"kshot_fail_enriched.png\",\n",
    "    \"abl_qot.png\",\n",
    "    \"abl_fail.png\",\n",
    "    \"abl_fail_shift_tight.png\",\n",
    "]\n",
    "\n",
    "copied = {\"metrics\": [], \"figs\": [], \"missing\": []}\n",
    "\n",
    "def try_copy(src_dir, name, dst_dir):\n",
    "    p = src_dir/name\n",
    "    if p.exists():\n",
    "        shutil.copy2(p, dst_dir/name)\n",
    "        return True\n",
    "    else:\n",
    "        copied[\"missing\"].append(str(p))\n",
    "        return False\n",
    "\n",
    "for f in metrics_files:\n",
    "    if try_copy(SRC_MET, f, PMET):\n",
    "        copied[\"metrics\"].append(f)\n",
    "\n",
    "for f in figs_files:\n",
    "    if try_copy(SRC_FIG, f, PFIG):\n",
    "        copied[\"figs\"].append(f)\n",
    "\n",
    "# ---- summarize key results into a manifest ----\n",
    "manifest = {\"files\": copied, \"highlights\": {}}\n",
    "\n",
    "# Rerouting summary\n",
    "rer_base = PMET/\"reroute_base.json\"\n",
    "rer_graph= PMET/\"reroute_graph.json\"\n",
    "if rer_base.exists() and rer_graph.exists():\n",
    "    jb = json.loads(rer_base.read_text())\n",
    "    jg = json.loads(rer_graph.read_text())\n",
    "    manifest[\"highlights\"][\"rerouting\"] = {\n",
    "        \"considered_base\": jb.get(\"considered\"),\n",
    "        \"considered_graph\": jg.get(\"considered\"),\n",
    "        \"salvage_rate_base_pct\": round(jb.get(\"salvage_rate_pct\", float(\"nan\")), 3),\n",
    "        \"salvage_rate_graph_pct\": round(jg.get(\"salvage_rate_pct\", float(\"nan\")), 3),\n",
    "        \"delta_salvage_pp\": round(jg.get(\"salvage_rate_pct\",0) - jb.get(\"salvage_rate_pct\",0), 3),\n",
    "        \"avg_extra_km_base\": round(jb.get(\"avg_extra_km\", float(\"nan\")), 3) if isinstance(jb.get(\"avg_extra_km\"), (int,float)) else None,\n",
    "        \"avg_extra_km_graph\": round(jg.get(\"avg_extra_km\", float(\"nan\")), 3) if isinstance(jg.get(\"avg_extra_km\"), (int,float)) else None,\n",
    "        \"avg_extra_ms_base\": round(jb.get(\"avg_extra_ms\", float(\"nan\")), 3) if isinstance(jb.get(\"avg_extra_ms\"), (int,float)) else None,\n",
    "        \"avg_extra_ms_graph\": round(jg.get(\"avg_extra_ms\", float(\"nan\")), 3) if isinstance(jg.get(\"avg_extra_ms\"), (int,float)) else None,\n",
    "    }\n",
    "\n",
    "# k-shot curves summary\n",
    "kq = PMET/\"kshot_qot_enriched.csv\"\n",
    "kf = PMET/\"kshot_fail_enriched.csv\"\n",
    "def kshot_summary(csv_path, cols):\n",
    "    if not csv_path.exists(): return None\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.sort_values(\"k\")\n",
    "    return df[[\"k\"]+cols].to_dict(orient=\"records\")\n",
    "\n",
    "manifest[\"highlights\"][\"kshot_qot\"]  = kshot_summary(kq, [\"f1\",\"auc\"]) if kq.exists() else None\n",
    "manifest[\"highlights\"][\"kshot_fail\"] = kshot_summary(kf, [\"f1\"])       if kf.exists() else None\n",
    "\n",
    "# Ablations quick picks\n",
    "abl = PMET/\"ablations_summary.csv\"\n",
    "if abl.exists():\n",
    "    df = pd.read_csv(abl)\n",
    "    def pick(task, scenario, k=0, cols=(\"acc\",\"f1\",\"auc\",\"n\")):\n",
    "        row = df[(df.task==task) & (df.scenario==scenario) & (df.k==k)]\n",
    "        if row.empty: return None\n",
    "        r = row.iloc[0].to_dict()\n",
    "        return {c: (float(r[c]) if c in r and pd.notna(r[c]) else None) for c in cols}\n",
    "    manifest[\"highlights\"][\"ablations\"] = {\n",
    "        \"qot_base_k0\":   pick(\"qot\",\"base\",0),\n",
    "        \"qot_graph_k0\":  pick(\"qot\",\"+graph\",0),\n",
    "        \"qot_graph_k100\":pick(\"qot\",\"+graph\",100),\n",
    "        \"fail_base_k0\":  pick(\"fail\",\"base\",0, cols=(\"acc\",\"f1\",\"n\")),\n",
    "        \"fail_graph_k0\": pick(\"fail\",\"+graph\",0, cols=(\"acc\",\"f1\",\"n\")),\n",
    "        \"fail_graph_noFP_k0\": pick(\"fail\",\"+graph_noFP\",0, cols=(\"acc\",\"f1\",\"n\")),\n",
    "        \"fail_graph_noOSNR_k0\": pick(\"fail\",\"+graph_noOSNR\",0, cols=(\"acc\",\"f1\",\"n\")),\n",
    "        \"fail_graph_k100\": pick(\"fail\",\"+graph\",100, cols=(\"acc\",\"f1\",\"n\")),\n",
    "    }\n",
    "\n",
    "# Save manifest\n",
    "with open(PACK/\"manifest.json\",\"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "# ---- Write a compact README.md with headline numbers ----\n",
    "lines = []\n",
    "lines.append(\"# Paper Pack\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"This folder aggregates metrics and figures generated in Steps 13–18.\")\n",
    "lines.append(\"\")\n",
    "if \"rerouting\" in manifest[\"highlights\"]:\n",
    "    rr = manifest[\"highlights\"][\"rerouting\"]\n",
    "    lines += [\n",
    "        \"## QoT-guided Rerouting (GEANT2, zero-shot, HARD)\",\n",
    "        f\"- BASE salvage: **{rr['salvage_rate_base_pct']}%** ; +GraphFea: **{rr['salvage_rate_graph_pct']}%** ; Δ = **{rr['delta_salvage_pp']} pp**\",\n",
    "        f\"- Overhead (avg): BASE **{rr['avg_extra_km_base']} km / {rr['avg_extra_ms_base']} ms**, +GraphFea **{rr['avg_extra_km_graph']} km / {rr['avg_extra_ms_graph']} ms**\",\n",
    "        \"\"\n",
    "    ]\n",
    "if manifest[\"highlights\"].get(\"kshot_qot\"):\n",
    "    lines.append(\"## k-shot (QoT, graph-enriched, HARD)\")\n",
    "    lines.append(\"| k | F1 | AUC |\"); lines.append(\"|---:|---:|---:|\")\n",
    "    for r in manifest[\"highlights\"][\"kshot_qot\"]:\n",
    "        lines.append(f\"| {r['k']} | {r['f1']:.4f} | {r['auc']:.4f} |\")\n",
    "    lines.append(\"\")\n",
    "if manifest[\"highlights\"].get(\"kshot_fail\"):\n",
    "    lines.append(\"## k-shot (Failure detection, graph-enriched, HARD)\")\n",
    "    lines.append(\"| k | F1 |\"); lines.append(\"|---:|---:|\")\n",
    "    for r in manifest[\"highlights\"][\"kshot_fail\"]:\n",
    "        lines.append(f\"| {r['k']} | {r['f1']:.4f} |\")\n",
    "    lines.append(\"\")\n",
    "if manifest[\"highlights\"].get(\"ablations\"):\n",
    "    ab = manifest[\"highlights\"][\"ablations\"]\n",
    "    lines += [\n",
    "        \"## Ablations (Zero-shot unless noted)\",\n",
    "        f\"- QoT: Base k=0 F1 **{ab['qot_base_k0']['f1']:.4f}**, +Graph k=0 F1 **{ab['qot_graph_k0']['f1']:.4f}**, +Graph k=100 F1 **{ab['qot_graph_k100']['f1']:.4f}**.\",\n",
    "        f\"- Failure: Base k=0 F1 **{ab['fail_base_k0']['f1']:.4f}**, +Graph k=0 F1 **{ab['fail_graph_k0']['f1']:.4f}**, +Graph(noFP) k=0 F1 **{ab['fail_graph_noFP_k0']['f1']:.4f}**, +Graph(noOSNR) k=0 F1 **{ab['fail_graph_noOSNR_k0']['f1']:.4f}**; +Graph k=100 F1 **{ab['fail_graph_k100']['f1']:.4f}**.\",\n",
    "        \"\"\n",
    "    ]\n",
    "lines += [\n",
    "    \"## Figures\",\n",
    "    \"- ROC: `figs/qot_roc_zero_vs_tta.png`, Reliability: `figs/qot_reliability_zero_vs_tta.png`, Risk–Coverage: `figs/qot_risk_coverage_zero_vs_tta.png`\",\n",
    "    \"- k-shot: `figs/kshot_qot_enriched.png`, `figs/kshot_fail_enriched.png`\",\n",
    "    \"- Ablations: `figs/abl_qot.png`, `figs/abl_fail.png`, `figs/abl_fail_shift_tight.png`\",\n",
    "    \"\",\n",
    "    \"## Files included\",\n",
    "    f\"- Metrics: {', '.join(copied['metrics'])}\",\n",
    "    f\"- Figures: {', '.join(copied['figs'])}\",\n",
    "    \"\",\n",
    "    \"_Auto-generated by Step 19._\"\n",
    "]\n",
    "(PACK/\"README.md\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Pack created at:\", PACK.resolve())\n",
    "print(\"Copied metrics:\", copied['metrics'])\n",
    "print(\"Copied figures:\", copied['figs'])\n",
    "if copied[\"missing\"]:\n",
    "    print(\"\\nMissing (not found, skipped):\")\n",
    "    for m in copied[\"missing\"]:\n",
    "        print(\" -\", m)\n",
    "else:\n",
    "    print(\"\\nAll expected files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06a8caf-7b49-4a2f-9727-a1b7edfa3ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved case-study:\n",
      " - CSV: outputs\\metrics\\case_study_days_57_58_sd_1_22.csv\n",
      " - PNG: outputs\\figs\\case_study_days_57_58_sd_1_22.png\n",
      "Picked path: 1->22  path=1->8->5->4->22  days 57-58\n"
     ]
    }
   ],
   "source": [
    "# Step 20: Case-study time series (2-day window) with Zero-shot vs TTA-lite QoT predictions\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ----------------- Load enriched data -----------------\n",
    "ENRICHED = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "assert ENRICHED.exists(), \"Run Step 14 first to create paths_graph_enriched.csv\"\n",
    "df = pd.read_csv(ENRICHED)\n",
    "\n",
    "# Focus on target domain\n",
    "test = df[(df[\"split\"]==\"test_target\") & (df[\"topology\"]==\"GEANT2\")].copy()\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()  # NSFNET for training\n",
    "\n",
    "# ----------------- Feature schema (HARD + graph-aware) -----------------\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "GF_COLS = [\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "    \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]\n",
    "FEATS = BASE_QOT + GF_COLS\n",
    "CAT   = ['modulation']\n",
    "COL_ORDER = FEATS + CAT\n",
    "\n",
    "# Consistent modulation encoding (fit on train)\n",
    "train_mod_cats = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def enc_mod(s): return pd.Categorical(s, categories=train_mod_cats).codes\n",
    "def ordered(df_in): return df_in[COL_ORDER].copy()\n",
    "\n",
    "# ----------------- Train base QoT model on NSFNET -----------------\n",
    "Xtr = train[COL_ORDER].copy()\n",
    "Xtr['modulation'] = enc_mod(Xtr['modulation'])\n",
    "ytr = train['qot_ok'].astype(int).values\n",
    "\n",
    "sc_src = StandardScaler()\n",
    "Xtr_s  = sc_src.fit_transform(ordered(Xtr))\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                    alpha=1e-4, learning_rate_init=1e-3,\n",
    "                    max_iter=200, random_state=42)\n",
    "clf.fit(Xtr_s, ytr)\n",
    "\n",
    "# ----------------- Pick a \"loud\" path (max telemetry variance & failures) -----------------\n",
    "grp = (test\n",
    "       .assign(var_shift=test['gf_shift_max'].fillna(0.0).groupby([test['src'],test['dst'],test['path']]).transform('var'),\n",
    "               var_scale=test['gf_scale_min'].fillna(1.0).groupby([test['src'],test['dst'],test['path']]).transform('var'),\n",
    "               fail_cnt=test['failure_present'].groupby([test['src'],test['dst'],test['path']]).transform('sum'))\n",
    "       .copy())\n",
    "\n",
    "# score = more failures + more variability\n",
    "grp['score'] = grp['fail_cnt'].fillna(0) + grp['var_shift'].fillna(0) + grp['var_scale'].fillna(0)\n",
    "key_cols = ['src','dst','path']\n",
    "best_key = (grp.sort_values('score', ascending=False)[key_cols].iloc[0].to_dict())\n",
    "\n",
    "case = test[(test['src']==best_key['src']) & (test['dst']==best_key['dst']) & (test['path']==best_key['path'])].copy()\n",
    "case_days = sorted(case['day'].unique())\n",
    "# pick a 2-day window around the first failure day if possible\n",
    "fail_days = sorted(case[case['failure_present']==1]['day'].unique())\n",
    "if fail_days:\n",
    "    d0 = fail_days[0]\n",
    "else:\n",
    "    # no failure: just take earliest two consecutive days\n",
    "    d0 = case_days[0]\n",
    "d1 = d0 + 1\n",
    "if d1 not in case_days:  # fallbacks\n",
    "    d1 = d0\n",
    "    d0 = max(min(case_days), d1-1)\n",
    "\n",
    "window = case[case['day'].isin([d0, d1])].sort_values('day').copy()\n",
    "title_id = f\"{int(best_key['src'])}->{int(best_key['dst'])}  path={best_key['path']}  days {d0}-{d1}\"\n",
    "\n",
    "# ----------------- Zero-shot vs TTA-lite predictions across the window -----------------\n",
    "def bn_adapt_batch(X_raw, sc, gamma=0.3):\n",
    "    mu_src, sd_src = sc.mean_, sc.scale_\n",
    "    mu_b  = X_raw.mean(axis=0)\n",
    "    sd_b  = X_raw.std(axis=0, ddof=0) + 1e-6\n",
    "    mu = (1-gamma)*mu_src + gamma*mu_b\n",
    "    sd = (1-gamma)*sd_src + gamma*sd_b\n",
    "    return (X_raw - mu)/sd\n",
    "\n",
    "def tta_probs_for_day(day_rows, day_batch_all, aug_n=5, jitter=0.03, gamma=0.3):\n",
    "    # day_rows: DataFrame (subset of the case path for this day)\n",
    "    # day_batch_all: ALL GEANT2 rows for the same day (for batch stats)\n",
    "    Xb_raw = day_batch_all[COL_ORDER].copy()\n",
    "    Xb_raw['modulation'] = enc_mod(Xb_raw['modulation'])\n",
    "    Xb_raw = ordered(Xb_raw).values\n",
    "    Xb_s   = bn_adapt_batch(Xb_raw, sc_src, gamma=gamma)\n",
    "\n",
    "    # We need indices of our case rows inside batch to extract their probs\n",
    "    Xday = day_rows[COL_ORDER].copy()\n",
    "    Xday['modulation'] = enc_mod(Xday['modulation'])\n",
    "    Xday = ordered(Xday).values\n",
    "\n",
    "    # Map day_rows into the same normalized space using batch stats:\n",
    "    # (recompute mean/std from Xb_raw to avoid mismatch)\n",
    "    mu_src, sd_src = sc_src.mean_, sc_src.scale_\n",
    "    mu_b  = Xb_raw.mean(axis=0); sd_b = Xb_raw.std(axis=0, ddof=0)+1e-6\n",
    "    mu = (1-gamma)*mu_src + gamma*mu_b\n",
    "    sd = (1-gamma)*sd_src + gamma*sd_b\n",
    "    Xday_s = (Xday - mu)/sd\n",
    "\n",
    "    # Monte-Carlo jitter + averaging\n",
    "    rng = np.random.RandomState(7)\n",
    "    probs = []\n",
    "    for _ in range(aug_n):\n",
    "        Xj = Xday_s.copy()\n",
    "        noise = rng.normal(0.0, jitter, size=Xj[:,:-1].shape)  # don't jitter modulation (last col)\n",
    "        Xj[:,:-1] = Xj[:,:-1]*(1+noise)\n",
    "        probs.append(clf.predict_proba(Xj)[:,1])\n",
    "    return np.mean(probs, axis=0)\n",
    "\n",
    "# Build per-day frames\n",
    "rows = []\n",
    "for day in [d0, d1]:\n",
    "    day_all = test[test['day']==day].copy()\n",
    "    day_case = window[window['day']==day].copy()\n",
    "    if len(day_case)==0: \n",
    "        continue\n",
    "    # zero-shot\n",
    "    Xz = day_case[COL_ORDER].copy()\n",
    "    Xz['modulation'] = enc_mod(Xz['modulation'])\n",
    "    Xz_s = sc_src.transform(ordered(Xz))\n",
    "    p_zero = clf.predict_proba(Xz_s)[:,1]\n",
    "    # TTA-lite (batch = all GEANT2 rows of that day)\n",
    "    p_tta  = tta_probs_for_day(day_case, day_all, aug_n=5, jitter=0.03, gamma=0.3)\n",
    "    for i,(ix,r) in enumerate(day_case.iterrows()):\n",
    "        rows.append({\n",
    "            \"day\": int(day),\n",
    "            \"gf_shift_max\": float(r[\"gf_shift_max\"]),\n",
    "            \"gf_scale_min\": float(r[\"gf_scale_min\"]),\n",
    "            \"gf_osnr_min\":  float(r[\"gf_osnr_min\"]),\n",
    "            \"failure_present\": int(r[\"failure_present\"]),\n",
    "            \"failure_type\": str(r[\"failure_type\"]),\n",
    "            \"p_zero\": float(p_zero[i]),\n",
    "            \"p_tta\":  float(p_tta[i]),\n",
    "        })\n",
    "ts = pd.DataFrame(rows).sort_values(\"day\")\n",
    "\n",
    "# ----------------- Save CSV + Plot -----------------\n",
    "OUTF = Path(\"./outputs/figs\"); OUTM = Path(\"./outputs/metrics\")\n",
    "OUTF.mkdir(parents=True, exist_ok=True); OUTM.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = OUTM/f\"case_study_days_{d0}_{d1}_sd_{best_key['src']}_{best_key['dst']}.csv\"\n",
    "ts.to_csv(csv_path, index=False)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "# (1) Shift / Scale\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(ts['day'], ts['gf_shift_max'], marker='o', label='gf_shift_max')\n",
    "plt.plot(ts['day'], ts['gf_scale_min'], marker='s', label='gf_scale_min')\n",
    "for d,fp in zip(ts['day'], ts['failure_present']):\n",
    "    if fp==1: plt.axvspan(d-0.5, d+0.5, alpha=0.1)\n",
    "plt.ylabel(\"shift / scale\")\n",
    "plt.title(f\"Case Study: {title_id}\")\n",
    "plt.legend()\n",
    "\n",
    "# (2) Bottleneck OSNR\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(ts['day'], ts['gf_osnr_min'], marker='o', label='gf_osnr_min')\n",
    "for d,fp in zip(ts['day'], ts['failure_present']):\n",
    "    if fp==1: plt.axvspan(d-0.5, d+0.5, alpha=0.1)\n",
    "plt.ylabel(\"OSNR (min)\")\n",
    "plt.legend()\n",
    "\n",
    "# (3) QoT probability: Zero-shot vs TTA-lite\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(ts['day'], ts['p_zero'], marker='o', label='Zero-shot P(QoT OK)')\n",
    "plt.plot(ts['day'], ts['p_tta'], marker='s', label='TTA-lite P(QoT OK)')\n",
    "plt.axhline(0.5, linestyle='--', linewidth=1)\n",
    "for d,fp in zip(ts['day'], ts['failure_present']):\n",
    "    if fp==1: plt.axvspan(d-0.5, d+0.5, alpha=0.1)\n",
    "plt.xlabel(\"day\"); plt.ylabel(\"probability\")\n",
    "plt.legend()\n",
    "\n",
    "png_path = OUTF/f\"case_study_days_{d0}_{d1}_sd_{best_key['src']}_{best_key['dst']}.png\"\n",
    "plt.tight_layout(); plt.savefig(png_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "print(\"Saved case-study:\")\n",
    "print(\" - CSV:\", csv_path)\n",
    "print(\" - PNG:\", png_path)\n",
    "print(\"Picked path:\", title_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0127f291-c0b3-4f26-9ac8-67b8ccaa0f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\devonboard\\research\\daily taskk\\EACE2025\\by gpt\\paper_pack\\results_summary.md\n",
      "\n",
      "Preview (first 40 lines):\n",
      "\n",
      "# Results Summary (2025-08-22 15:37)\n",
      "\n",
      "**Novelty (recap):** label-free test-time adaptation (TTA) that boosts zero-shot QoT; graph-aware localization using link fingerprints; and few-shot label efficiency on GEANT2.\n",
      "\n",
      "## QoT-guided Re-routing (GEANT2, zero-shot, HARD)\n",
      "- **Salvage rate:** BASE 22.89% → +GraphFea 27.22% (Δ **4.32 pp**)\n",
      "- **Overhead (avg):** BASE 212.433 km / 1.062 ms;  +GraphFea 217.185 km / 1.086 ms\n",
      "\n",
      "## Few-shot (QoT, graph-enriched, HARD)\n",
      "\n",
      "| k | F1 | AUC |\n",
      "|---:|---:|---:|\n",
      "| 0 | 0.9473 | 0.9938 |\n",
      "| 10 | 0.9559 | 0.9959 |\n",
      "| 40 | 0.9577 | 0.9954 |\n",
      "| 100 | 0.9684 | 0.9976 |\n",
      "| 200 | 0.9758 | 0.9986 |\n",
      "\n",
      "## Few-shot (Failure detection, graph-enriched, HARD)\n",
      "\n",
      "| k | F1 |\n",
      "|---:|---:|\n",
      "| 0 | 0.9862 |\n",
      "| 10 | 0.9876 |\n",
      "| 40 | 0.9902 |\n",
      "| 100 | 0.9895 |\n",
      "| 200 | 0.9905 |\n",
      "\n",
      "## Ablations (Zero-shot unless noted)\n",
      "- **QoT:** Base F1 0.975, +Graph F1 0.947, +Graph (k=100) F1 0.966.\n",
      "- **Failure:** Base F1 0.493, +Graph F1 0.986, +Graph(noFP) F1 0.489, +Graph(noOSNR) F1 0.992; +Graph (k=100) F1 0.990.\n",
      "\n",
      "## Label-free Test-Time Adaptation (QoT)\n",
      "- ROC: `outputs\\figs\\qot_roc_zero_vs_tta.png`\n",
      "- Reliability: `outputs\\figs\\qot_reliability_zero_vs_tta.png`\n",
      "- Risk–Coverage: `outputs\\figs\\qot_risk_coverage_zero_vs_tta.png`\n",
      "\n",
      "## Case Study (2-day dynamics)\n",
      "- Figure: `outputs\\figs\\case_study_days_57_58_sd_1_22.png`\n",
      "- Avg Δ P(QoT OK), TTA − Zero-shot: **-0.000**\n"
     ]
    }
   ],
   "source": [
    "# Step 21: Auto-generate a paper-ready \"Results Summary\" markdown from your saved metrics/figures.\n",
    "from pathlib import Path\n",
    "import json, pandas as pd, numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "PACK = ROOT/\"paper_pack\"\n",
    "PMET  = PACK/\"metrics\"\n",
    "PFIG  = PACK/\"figs\"\n",
    "OUTM  = ROOT/\"outputs\"/\"metrics\"\n",
    "OUTF  = ROOT/\"outputs\"/\"figs\"\n",
    "\n",
    "# ensure paper_pack exists (if Step 19 not run, create minimal structure)\n",
    "PACK.mkdir(exist_ok=True); PMET.mkdir(exist_ok=True, parents=True); PFIG.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# -------- helpers --------\n",
    "def try_read_json(p: Path):\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return json.loads(p.read_text())\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def try_read_csv(p: Path):\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return pd.read_csv(p)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def fmt(x, nd=3, pct=False):\n",
    "    if x is None or (isinstance(x,float) and (np.isnan(x) or np.isinf(x))):\n",
    "        return \"—\"\n",
    "    if pct:\n",
    "        return f\"{x:.{nd}f}%\"\n",
    "    return f\"{x:.{nd}f}\"\n",
    "\n",
    "def pick_last(glob_pat, in_dir):\n",
    "    cands = sorted(in_dir.glob(glob_pat), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return cands[0] if cands else None\n",
    "\n",
    "lines = []\n",
    "lines.append(f\"# Results Summary ({datetime.now().strftime('%Y-%m-%d %H:%M')})\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**Novelty (recap):** label-free test-time adaptation (TTA) that boosts zero-shot QoT; \"\n",
    "             \"graph-aware localization using link fingerprints; and few-shot label efficiency on GEANT2.\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# -------- Rerouting (from Step 16) --------\n",
    "rer_base = PMET/\"reroute_base.json\"; rer_graph = PMET/\"reroute_graph.json\"\n",
    "if not rer_base.exists(): rer_base = OUTM/\"reroute_base.json\"\n",
    "if not rer_graph.exists(): rer_graph = OUTM/\"reroute_graph.json\"\n",
    "jb, jg = try_read_json(rer_base), try_read_json(rer_graph)\n",
    "\n",
    "if jb and jg:\n",
    "    lines += [\n",
    "        \"## QoT-guided Re-routing (GEANT2, zero-shot, HARD)\",\n",
    "        f\"- **Salvage rate:** BASE {fmt(jb.get('salvage_rate_pct'),2,pct=True)} → +GraphFea {fmt(jg.get('salvage_rate_pct'),2,pct=True)} \"\n",
    "        f\"(Δ **{fmt(jg.get('salvage_rate_pct',0)-jb.get('salvage_rate_pct',0),2)} pp**)\",\n",
    "        f\"- **Overhead (avg):** BASE {fmt(jb.get('avg_extra_km'))} km / {fmt(jb.get('avg_extra_ms'))} ms;  \"\n",
    "        f\"+GraphFea {fmt(jg.get('avg_extra_km'))} km / {fmt(jg.get('avg_extra_ms'))} ms\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "# -------- k-shot curves (from Step 17) --------\n",
    "kq = PMET/\"kshot_qot_enriched.csv\"; kf = PMET/\"kshot_fail_enriched.csv\"\n",
    "if not kq.exists(): kq = OUTM/\"kshot_qot_enriched.csv\"\n",
    "if not kf.exists(): kf = OUTM/\"kshot_fail_enriched.csv\"\n",
    "df_kq, df_kf = try_read_csv(kq), try_read_csv(kf)\n",
    "\n",
    "if df_kq is not None:\n",
    "    df_kq = df_kq.sort_values(\"k\")\n",
    "    lines += [\"## Few-shot (QoT, graph-enriched, HARD)\", \"\", \"| k | F1 | AUC |\", \"|---:|---:|---:|\"]\n",
    "    for _, r in df_kq.iterrows():\n",
    "        lines.append(f\"| {int(r['k'])} | {fmt(r['f1'],4)} | {fmt(r['auc'],4)} |\")\n",
    "    lines.append(\"\")\n",
    "if df_kf is not None:\n",
    "    df_kf = df_kf.sort_values(\"k\")\n",
    "    lines += [\"## Few-shot (Failure detection, graph-enriched, HARD)\", \"\", \"| k | F1 |\", \"|---:|---:|\"]\n",
    "    for _, r in df_kf.iterrows():\n",
    "        lines.append(f\"| {int(r['k'])} | {fmt(r['f1'],4)} |\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "# -------- Ablations (from Step 18) --------\n",
    "abl = PMET/\"ablations_summary.csv\"\n",
    "if not abl.exists(): abl = OUTM/\"ablations_summary.csv\"\n",
    "df_abl = try_read_csv(abl)\n",
    "if df_abl is not None:\n",
    "    def pick(task, scenario, k=0, cols=(\"acc\",\"f1\",\"auc\",\"n\")):\n",
    "        row = df_abl[(df_abl.task==task) & (df_abl.scenario==scenario) & (df_abl.k==k)]\n",
    "        if row.empty: return None\n",
    "        r = row.iloc[0].to_dict()\n",
    "        return {c: (float(r[c]) if c in r and pd.notna(r[c]) else None) for c in cols}\n",
    "    qb = pick(\"qot\",\"base\",0); qg0 = pick(\"qot\",\"+graph\",0); qg100 = pick(\"qot\",\"+graph\",100)\n",
    "    fb = pick(\"fail\",\"base\",0, cols=(\"acc\",\"f1\",\"n\")); fg0 = pick(\"fail\",\"+graph\",0, cols=(\"acc\",\"f1\",\"n\"))\n",
    "    fnfp = pick(\"fail\",\"+graph_noFP\",0, cols=(\"acc\",\"f1\",\"n\"))\n",
    "    fnos = pick(\"fail\",\"+graph_noOSNR\",0, cols=(\"acc\",\"f1\",\"n\"))\n",
    "    fg100 = pick(\"fail\",\"+graph\",100, cols=(\"acc\",\"f1\",\"n\"))\n",
    "    lines += [\n",
    "        \"## Ablations (Zero-shot unless noted)\",\n",
    "        f\"- **QoT:** Base F1 {fmt(qb['f1'])}, +Graph F1 {fmt(qg0['f1'])}, +Graph (k=100) F1 {fmt(qg100['f1'])}.\",\n",
    "        f\"- **Failure:** Base F1 {fmt(fb['f1'])}, +Graph F1 {fmt(fg0['f1'])}, \"\n",
    "        f\"+Graph(noFP) F1 {fmt(fnfp['f1'])}, +Graph(noOSNR) F1 {fmt(fnos['f1'])}; \"\n",
    "        f\"+Graph (k=100) F1 {fmt(fg100['f1'])}.\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "# -------- Zero-shot vs TTA (from Step 13) --------\n",
    "roc_png = pick_last(\"qot_roc_zero_vs_tta.png\", OUTF) or pick_last(\"qot_roc_zero_vs_tta.png\", PFIG)\n",
    "rel_png = pick_last(\"qot_reliability_zero_vs_tta.png\", OUTF) or pick_last(\"qot_reliability_zero_vs_tta.png\", PFIG)\n",
    "rc_png  = pick_last(\"qot_risk_coverage_zero_vs_tta.png\", OUTF) or pick_last(\"qot_risk_coverage_zero_vs_tta.png\", PFIG)\n",
    "if roc_png or rel_png or rc_png:\n",
    "    lines += [\"## Label-free Test-Time Adaptation (QoT)\",]\n",
    "    if roc_png: lines.append(f\"- ROC: `{roc_png}`\")\n",
    "    if rel_png: lines.append(f\"- Reliability: `{rel_png}`\")\n",
    "    if rc_png:  lines.append(f\"- Risk–Coverage: `{rc_png}`\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "# -------- Case study (from Step 20) --------\n",
    "case_csv = pick_last(\"case_study_days_*.csv\", OUTM)\n",
    "case_png = pick_last(\"case_study_days_*.png\", OUTF)\n",
    "if case_png:\n",
    "    lines += [\"## Case Study (2-day dynamics)\", f\"- Figure: `{case_png}`\"]\n",
    "    if case_csv:\n",
    "        try:\n",
    "            dcs = pd.read_csv(case_csv)\n",
    "            delta = float(dcs[\"p_tta\"].mean() - dcs[\"p_zero\"].mean())\n",
    "            lines.append(f\"- Avg Δ P(QoT OK), TTA − Zero-shot: **{fmt(delta,3)}**\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    lines.append(\"\")\n",
    "\n",
    "# -------- Write files --------\n",
    "out_md = PACK/\"results_summary.md\"\n",
    "out_md.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\", out_md.resolve())\n",
    "print(\"\\nPreview (first 40 lines):\\n\")\n",
    "print(\"\\n\".join(lines[:40]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2f234e-66bf-43cb-9418-cbe50ba3efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QoT-guided Re-routing (GEANT2, zero-shot, HARD) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Considered</th>\n",
       "      <th>Salvaged</th>\n",
       "      <th>Salvage_pct</th>\n",
       "      <th>Overhead_km</th>\n",
       "      <th>Overhead_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BASE</td>\n",
       "      <td>166</td>\n",
       "      <td>38</td>\n",
       "      <td>22.89</td>\n",
       "      <td>212.433</td>\n",
       "      <td>1.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+GraphFea</td>\n",
       "      <td>158</td>\n",
       "      <td>43</td>\n",
       "      <td>27.22</td>\n",
       "      <td>217.185</td>\n",
       "      <td>1.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Δ (+GraphFea - BASE)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4.32</td>\n",
       "      <td>4.751</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model Considered Salvaged  Salvage_pct  Overhead_km  Overhead_ms\n",
       "0                  BASE        166       38        22.89      212.433        1.062\n",
       "1             +GraphFea        158       43        27.22      217.185        1.086\n",
       "2  Δ (+GraphFea - BASE)                             4.32        4.751        0.024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Few-shot (QoT, graph-enriched, HARD) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9473</td>\n",
       "      <td>0.9938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.9559</td>\n",
       "      <td>0.9959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>0.9954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.9684</td>\n",
       "      <td>0.9976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.9758</td>\n",
       "      <td>0.9986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k      f1     auc\n",
       "0    0  0.9473  0.9938\n",
       "1   10  0.9559  0.9959\n",
       "2   40  0.9577  0.9954\n",
       "3  100  0.9684  0.9976\n",
       "4  200  0.9758  0.9986"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Few-shot (Failure detection, graph-enriched, HARD) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.9876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>0.9902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.9895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.9905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k      f1\n",
       "0    0  0.9862\n",
       "1   10  0.9876\n",
       "2   40  0.9902\n",
       "3  100  0.9895\n",
       "4  200  0.9905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QoT Ablations (GEANT2, HARD) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>k</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.9754</td>\n",
       "      <td>0.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+graph</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9598</td>\n",
       "      <td>0.9473</td>\n",
       "      <td>0.9938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+graph_noFP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9617</td>\n",
       "      <td>0.9476</td>\n",
       "      <td>0.9958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+graph_noOSNR</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9594</td>\n",
       "      <td>0.9446</td>\n",
       "      <td>0.9934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+graph</td>\n",
       "      <td>100</td>\n",
       "      <td>0.9745</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>0.9959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        scenario    k     acc      f1     auc\n",
       "0           base    0  0.9809  0.9754  0.9984\n",
       "1         +graph    0  0.9598  0.9473  0.9938\n",
       "2    +graph_noFP    0  0.9617  0.9476  0.9958\n",
       "3  +graph_noOSNR    0  0.9594  0.9446  0.9934\n",
       "4         +graph  100  0.9745  0.9659  0.9959"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Failure Ablations (GEANT2, HARD) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>k</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8415</td>\n",
       "      <td>0.4935</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+graph</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+graph_noFP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8181</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+graph_noOSNR</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>0.9922</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+graph</td>\n",
       "      <td>100</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>5301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>+graph_shift</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>+graph_tighten</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>0.4860</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         scenario    k     acc      f1     n\n",
       "0            base    0  0.8415  0.4935  5400\n",
       "1          +graph    0  0.9931  0.9862  5400\n",
       "2     +graph_noFP    0  0.8181  0.4890  5400\n",
       "3   +graph_noOSNR    0  0.9961  0.9922  5400\n",
       "4          +graph  100  0.9953  0.9902  5301\n",
       "5    +graph_shift    0  0.9791  0.4947   479\n",
       "6  +graph_tighten    0  0.9453  0.4860   311"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Failure per-class (shift/tighten) — macro-F1 shown (see note) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>k</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+graph_shift</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+graph_tighten</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>0.4860</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+graph_shift</td>\n",
       "      <td>100</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>0.4989</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+graph_tighten</td>\n",
       "      <td>100</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.4948</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         scenario    k     acc      f1    n\n",
       "0    +graph_shift    0  0.9791  0.4947  479\n",
       "1  +graph_tighten    0  0.9453  0.4860  311\n",
       "2    +graph_shift  100  0.9956  0.4989  452\n",
       "3  +graph_tighten  100  0.9792  0.4948  289"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: macro-F1 looks ~0.5 because these subsets contain only positives.\n",
      "If you want positive-class F1, run the small helper cell I shared earlier.\n"
     ]
    }
   ],
   "source": [
    "# Step 22 (alt): Print results inline (no LaTeX) — rerouting, k-shot, ablations.\n",
    "# It reads metrics from ./outputs/metrics or paper_pack/metrics and prints tidy tables.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "OUTM = ROOT/\"outputs\"/\"metrics\"\n",
    "PPM  = ROOT/\"paper_pack\"/\"metrics\"\n",
    "OUTM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def pick(*paths: Path):\n",
    "    for p in paths:\n",
    "        if p.exists(): return p\n",
    "    return None\n",
    "\n",
    "def fmt_num(x, nd=3):\n",
    "    if x is None: return np.nan\n",
    "    try:\n",
    "        xf = float(x)\n",
    "        if np.isnan(xf) or np.isinf(xf): return np.nan\n",
    "        return round(xf, nd)\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "# ---------- 1) QoT-guided re-routing (BASE vs +GraphFea) ----------\n",
    "rer_base_p  = pick(PPM/\"reroute_base.json\", OUTM/\"reroute_base.json\")\n",
    "rer_graph_p = pick(PPM/\"reroute_graph.json\", OUTM/\"reroute_graph.json\")\n",
    "\n",
    "if rer_base_p and rer_graph_p:\n",
    "    jb = json.loads(rer_base_p.read_text()); jg = json.loads(rer_graph_p.read_text())\n",
    "    rer_df = pd.DataFrame([\n",
    "        dict(Model=\"BASE\",\n",
    "             Considered=jb.get(\"considered\"), Salvaged=jb.get(\"salvaged\"),\n",
    "             Salvage_pct=fmt_num(jb.get(\"salvage_rate_pct\"),2),\n",
    "             Overhead_km=fmt_num(jb.get(\"avg_extra_km\")), Overhead_ms=fmt_num(jb.get(\"avg_extra_ms\"))),\n",
    "        dict(Model=\"+GraphFea\",\n",
    "             Considered=jg.get(\"considered\"), Salvaged=jg.get(\"salvaged\"),\n",
    "             Salvage_pct=fmt_num(jg.get(\"salvage_rate_pct\"),2),\n",
    "             Overhead_km=fmt_num(jg.get(\"avg_extra_km\")), Overhead_ms=fmt_num(jg.get(\"avg_extra_ms\"))),\n",
    "        dict(Model=\"Δ (+GraphFea - BASE)\",\n",
    "             Considered=\"\", Salvaged=\"\",\n",
    "             Salvage_pct=fmt_num(jg.get(\"salvage_rate_pct\",0)-jb.get(\"salvage_rate_pct\",0),2),\n",
    "             Overhead_km=(fmt_num(jg.get(\"avg_extra_km\")-jb.get(\"avg_extra_km\")) if all(isinstance(x,(int,float)) for x in [jg.get(\"avg_extra_km\"),jb.get(\"avg_extra_km\")]) else \"\"),\n",
    "             Overhead_ms=(fmt_num(jg.get(\"avg_extra_ms\")-jb.get(\"avg_extra_ms\")) if all(isinstance(x,(int,float)) for x in [jg.get(\"avg_extra_ms\"),jb.get(\"avg_extra_ms\")]) else \"\")),\n",
    "    ])\n",
    "    print(\"\\n=== QoT-guided Re-routing (GEANT2, zero-shot, HARD) ===\")\n",
    "    display(rer_df)\n",
    "else:\n",
    "    print(\"Rerouting metrics not found. Run Step 16 + save step first.\")\n",
    "\n",
    "# ---------- 2) k-shot curves (QoT & Failure) ----------\n",
    "kq_p = pick(PPM/\"kshot_qot_enriched.csv\", OUTM/\"kshot_qot_enriched.csv\")\n",
    "kf_p = pick(PPM/\"kshot_fail_enriched.csv\", OUTM/\"kshot_fail_enriched.csv\")\n",
    "\n",
    "if kq_p and kq_p.exists():\n",
    "    kq = pd.read_csv(kq_p).sort_values(\"k\")\n",
    "    kq_out = kq[[\"k\",\"f1\",\"auc\"]].copy()\n",
    "    kq_out[\"f1\"]  = kq_out[\"f1\"].map(lambda x: round(x,4))\n",
    "    kq_out[\"auc\"] = kq_out[\"auc\"].map(lambda x: round(x,4))\n",
    "    print(\"\\n=== Few-shot (QoT, graph-enriched, HARD) ===\")\n",
    "    display(kq_out)\n",
    "else:\n",
    "    print(\"\\nQoT k-shot file not found.\")\n",
    "\n",
    "if kf_p and kf_p.exists():\n",
    "    kf = pd.read_csv(kf_p).sort_values(\"k\")\n",
    "    kf_out = kf[[\"k\",\"f1\"]].copy()\n",
    "    kf_out[\"f1\"] = kf_out[\"f1\"].map(lambda x: round(x,4))\n",
    "    print(\"\\n=== Few-shot (Failure detection, graph-enriched, HARD) ===\")\n",
    "    display(kf_out)\n",
    "else:\n",
    "    print(\"\\nFailure k-shot file not found.\")\n",
    "\n",
    "# ---------- 3) Ablations summary (QoT & Failure) ----------\n",
    "abl_p = pick(PPM/\"ablations_summary.csv\", OUTM/\"ablations_summary.csv\")\n",
    "if abl_p and abl_p.exists():\n",
    "    abl = pd.read_csv(abl_p)\n",
    "\n",
    "    # QoT: k=0 rows + k=100 for +graph\n",
    "    q0 = abl[(abl.task==\"qot\") & ((abl.k==0) | ((abl.k==100) & (abl.scenario==\"+graph\")))].copy()\n",
    "    qtab = (q0[[\"scenario\",\"k\",\"acc\",\"f1\",\"auc\"]]\n",
    "            .assign(acc=lambda d: d[\"acc\"].round(4),\n",
    "                    f1=lambda d: d[\"f1\"].round(4),\n",
    "                    auc=lambda d: d[\"auc\"].round(4)))\n",
    "    print(\"\\n=== QoT Ablations (GEANT2, HARD) ===\")\n",
    "    display(qtab.reset_index(drop=True))\n",
    "\n",
    "    # Failure: k=0 rows + k=100 for +graph\n",
    "    f0 = abl[(abl.task==\"fail\") & ((abl.k==0) | ((abl.k==100) & (abl.scenario==\"+graph\")))].copy()\n",
    "    ftab = (f0[[\"scenario\",\"k\",\"acc\",\"f1\",\"n\"]]\n",
    "            .assign(acc=lambda d: d[\"acc\"].round(4),\n",
    "                    f1=lambda d: d[\"f1\"].round(4)))\n",
    "    print(\"\\n=== Failure Ablations (GEANT2, HARD) ===\")\n",
    "    display(ftab.reset_index(drop=True))\n",
    "\n",
    "    # (Optional) If you also want per-class rows that exist in the CSV:\n",
    "    per_cls = abl[(abl.task==\"fail\") & (abl[\"scenario\"].isin([\"+graph_shift\",\"+graph_tighten\"]))].copy()\n",
    "    if not per_cls.empty:\n",
    "        pctab = (per_cls[[\"scenario\",\"k\",\"acc\",\"f1\",\"n\"]]\n",
    "                 .assign(acc=lambda d: d[\"acc\"].round(4),\n",
    "                         f1=lambda d: d[\"f1\"].round(4)))\n",
    "        print(\"\\n=== Failure per-class (shift/tighten) — macro-F1 shown (see note) ===\")\n",
    "        display(pctab.reset_index(drop=True))\n",
    "        print(\"Note: macro-F1 looks ~0.5 because these subsets contain only positives.\\n\"\n",
    "              \"If you want positive-class F1, run the small helper cell I shared earlier.\")\n",
    "else:\n",
    "    print(\"\\nAblations file not found. Run Step 18 first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fd69b7a-9b12-407c-8f2d-5e6ee9bdd4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QoT (GEANT2, HARD) — AUC: Zero-shot vs TTA-lite ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "      <th>95% CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero-shot</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>[0.9924, 0.9948]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTA-lite</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>[0.9928, 0.9952]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Δ (TTA - Zero)</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>[-0.0015, 0.0022]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model     AUC             95% CI\n",
       "0       Zero-shot  0.9938   [0.9924, 0.9948]\n",
       "1        TTA-lite  0.9940   [0.9928, 0.9952]\n",
       "2  Δ (TTA - Zero)  0.0003  [-0.0015, 0.0022]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Failure Detection (GEANT2, HARD, +GraphFea) — Macro-F1 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1 (macro)</th>\n",
       "      <th>95% CI</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+GraphFea</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>[0.9820, 0.9903]</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  F1 (macro)            95% CI     N\n",
       "0  +GraphFea      0.9862  [0.9820, 0.9903]  5400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QoT-guided Re-routing (GEANT2, HARD) — Salvage rate (%) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Salvage %</th>\n",
       "      <th>95% CI</th>\n",
       "      <th>Considered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BASE</td>\n",
       "      <td>22.892</td>\n",
       "      <td>[16.867, 29.518]</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+GraphFea</td>\n",
       "      <td>27.215</td>\n",
       "      <td>[20.886, 34.177]</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Δ (+Graph - BASE)</td>\n",
       "      <td>4.324</td>\n",
       "      <td>[-4.866, 14.207]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bootstrap one-sided p(Δ&lt;=0)</td>\n",
       "      <td>0.180</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  Salvage %            95% CI Considered\n",
       "0                         BASE     22.892  [16.867, 29.518]        166\n",
       "1                    +GraphFea     27.215  [20.886, 34.177]        158\n",
       "2            Δ (+Graph - BASE)      4.324  [-4.866, 14.207]           \n",
       "3  Bootstrap one-sided p(Δ<=0)      0.180                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 23 — Bootstrap 95% CIs for key metrics: QoT AUC (Zero vs TTA), Failure F1 (+Graph), Rerouting salvage (BASE vs +Graph)\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------ Load data ------------------\n",
    "ENRICHED = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "LINKS    = Path(\"./eon_links_timeseries.csv\")\n",
    "assert ENRICHED.exists(), \"Run Step 14 first (paths_graph_enriched.csv).\"\n",
    "assert LINKS.exists(), \"Missing eon_links_timeseries.csv in working folder.\"\n",
    "\n",
    "df    = pd.read_csv(ENRICHED)\n",
    "links = pd.read_csv(LINKS)\n",
    "\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()                   # NSFNET\n",
    "test  = df[(df[\"split\"]==\"test_target\") & (df[\"topology\"]==\"GEANT2\")].copy()  # GEANT2\n",
    "\n",
    "# ------------------ Feature schemas (HARD) ------------------\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "GF_COLS = [\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "    \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]\n",
    "ENR_QOT  = BASE_QOT + GF_COLS\n",
    "BASE_FAIL= [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "ENR_FAIL = BASE_FAIL + GF_COLS\n",
    "CAT = ['modulation']\n",
    "\n",
    "# Consistent modulation encoding\n",
    "train_mod_cats = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def enc_mod(s): return pd.Categorical(s, categories=train_mod_cats).codes\n",
    "def ordered(df_in, cols): return df_in[cols].copy()\n",
    "\n",
    "# ------------------ Train base models (NSFNET) ------------------\n",
    "# QoT model (+GraphFea)\n",
    "COL_QOT = ENR_QOT + CAT\n",
    "Xtr_q = train[COL_QOT].copy(); Xtr_q['modulation'] = enc_mod(Xtr_q['modulation'])\n",
    "ytr_q = train['qot_ok'].astype(int).values\n",
    "sc_q  = StandardScaler(); Xtr_qs = sc_q.fit_transform(ordered(Xtr_q, COL_QOT))\n",
    "clf_q = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                      alpha=1e-4, learning_rate_init=1e-3,\n",
    "                      max_iter=200, random_state=42).fit(Xtr_qs, ytr_q)\n",
    "\n",
    "# Failure model (+GraphFea)\n",
    "COL_F = ENR_FAIL + CAT\n",
    "Xtr_f = train[COL_F].copy(); Xtr_f['modulation'] = enc_mod(Xtr_f['modulation'])\n",
    "ytr_f = train['failure_present'].astype(int).values\n",
    "sc_f  = StandardScaler(); Xtr_fs = sc_f.fit_transform(ordered(Xtr_f, COL_F))\n",
    "clf_f = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                      alpha=1e-4, learning_rate_init=1e-3,\n",
    "                      max_iter=200, random_state=42).fit(Xtr_fs, ytr_f)\n",
    "\n",
    "# ------------------ QoT: Zero-shot vs TTA-lite probs on GEANT2 ------------------\n",
    "Xte_q = test[COL_QOT].copy(); Xte_q['modulation'] = enc_mod(Xte_q['modulation'])\n",
    "yte_q = test['qot_ok'].astype(int).values\n",
    "Xte_qs = sc_q.transform(ordered(Xte_q, COL_QOT))\n",
    "p_zero = clf_q.predict_proba(Xte_qs)[:,1]\n",
    "\n",
    "def bn_adapt_batch(X_raw, sc, gamma=0.3):\n",
    "    mu_src, sd_src = sc.mean_, sc.scale_\n",
    "    mu_b  = X_raw.mean(axis=0); sd_b = X_raw.std(axis=0, ddof=0) + 1e-6\n",
    "    mu = (1-gamma)*mu_src + gamma*mu_b\n",
    "    sd = (1-gamma)*sd_src + gamma*sd_b\n",
    "    return (X_raw - mu)/sd\n",
    "\n",
    "def tta_probs(X_raw, sc, clf, aug_n=5, jitter=0.03, gamma=0.3):\n",
    "    Xb = bn_adapt_batch(X_raw, sc, gamma=gamma)\n",
    "    rng = np.random.RandomState(7)\n",
    "    probs = []\n",
    "    for _ in range(aug_n):\n",
    "        Xj = Xb.copy()\n",
    "        noise = rng.normal(0.0, jitter, size=Xj[:,:-1].shape) # don't jitter modulation col\n",
    "        Xj[:,:-1] = Xj[:,:-1]*(1+noise)\n",
    "        probs.append(clf.predict_proba(Xj)[:,1])\n",
    "    return np.mean(probs, axis=0)\n",
    "\n",
    "p_tta = tta_probs(ordered(Xte_q, COL_QOT).values, sc_q, clf_q, aug_n=5, jitter=0.03, gamma=0.3)\n",
    "\n",
    "# ------------------ Failure detection: predictions on GEANT2 (+GraphFea) ------------------\n",
    "Xte_f = test[COL_F].copy(); Xte_f['modulation'] = enc_mod(Xte_f['modulation'])\n",
    "yte_f = test['failure_present'].astype(int).values\n",
    "Xte_fs= sc_f.transform(ordered(Xte_f, COL_F))\n",
    "yhat_f = clf_f.predict(Xte_fs)\n",
    "\n",
    "# ------------------ Rerouting: re-evaluate with per-demand outcomes for bootstrap ------------------\n",
    "import networkx as nx\n",
    "\n",
    "def build_graph_for_day(day):\n",
    "    sub = links[(links['topology']=='GEANT2') & (links['day']==day)]\n",
    "    G = nx.Graph()\n",
    "    for _, r in sub.iterrows():\n",
    "        u, v = int(r['u']), int(r['v'])\n",
    "        G.add_edge(u, v, length_km=float(r['length_km']))\n",
    "    return G, sub.set_index('edge_id')\n",
    "\n",
    "def edge_ids_from_nodes(nodes):\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(nodes[:-1], nodes[1:])]\n",
    "\n",
    "def path_aggs(eids, idx):\n",
    "    rows = idx.loc[eids]\n",
    "    hops = len(eids)\n",
    "    base = {\n",
    "        \"hops\": hops,\n",
    "        \"distance_km\": float(rows['length_km'].sum()),\n",
    "        \"latency_ms\":  float(rows['latency_ms'].sum()),\n",
    "        \"avg_utilization\": float(rows['bandwidth_utilization'].mean()),\n",
    "        \"min_osnr_db\": float(rows['osnr_db'].min()),\n",
    "        \"min_snr_db\":  float(rows['snr_db'].min()),\n",
    "        \"max_center_offset_ghz\": float(rows['center_freq_offset_ghz'].max()),\n",
    "        \"min_filter_bw_scale\":   float(rows['filter_bw_scale'].min()),\n",
    "    }\n",
    "    osnrs = rows['osnr_db'].values\n",
    "    utils = rows['bandwidth_utilization'].values\n",
    "    shifts= rows['center_freq_offset_ghz'].values\n",
    "    scales= rows['filter_bw_scale'].values\n",
    "    gf = {\n",
    "        \"gf_osnr_min\": float(osnrs.min()),\n",
    "        \"gf_osnr_var\": float(np.var(osnrs)) if hops>1 else 0.0,\n",
    "        \"gf_util_mean\": float(utils.mean()),\n",
    "        \"gf_util_max\": float(utils.max()),\n",
    "        \"gf_shift_max\": float(shifts.max()),\n",
    "        \"gf_scale_min\": float(scales.min()),\n",
    "        \"gf_frac_shifted\": float((shifts>0).mean()),\n",
    "        \"gf_frac_tight\": float((scales<1.0).mean()),\n",
    "        \"gf_bot_pos\": float(np.argmin(osnrs)/max(1,hops-1))\n",
    "    }\n",
    "    return base, gf\n",
    "\n",
    "def cand_features(eids, tx_row, idx, use_graph):\n",
    "    base, gf = path_aggs(eids, idx)\n",
    "    row = {\n",
    "        **base,\n",
    "        \"symbol_rate_gbaud\": float(tx_row['symbol_rate_gbaud']),\n",
    "        \"bitrate_gbps\": float(tx_row['bitrate_gbps']),\n",
    "        \"modulation\": tx_row['modulation']\n",
    "    }\n",
    "    if use_graph:\n",
    "        row.update(gf)\n",
    "    return row\n",
    "\n",
    "def reroute_eval_collect(feat_cols, clf, scaler, use_graph, K=3, sample_n=600, seed=7):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    sample = test.sample(n=min(sample_n, len(test)), random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # who needs reroute?\n",
    "    Xtest = sample[feat_cols + CAT].copy(); Xtest['modulation'] = enc_mod(Xtest['modulation'])\n",
    "    Xtest_ord = ordered(Xtest, feat_cols + CAT)\n",
    "    need = (clf.predict_proba(scaler.transform(Xtest_ord))[:,1] < 0.5).astype(int)\n",
    "\n",
    "    considered_idx = np.where(need==1)[0].tolist()\n",
    "    outcomes = []   # 1 if salvaged, 0 if not\n",
    "    for i in considered_idx:\n",
    "        row = sample.iloc[i]\n",
    "        day = int(row['day'])\n",
    "        try:\n",
    "            G, idx = build_graph_for_day(day)\n",
    "        except Exception:\n",
    "            continue\n",
    "        nodes = [int(x) for x in row['path'].split('->')]\n",
    "        try:\n",
    "            alt_iter = nx.shortest_simple_paths(G, nodes[0], nodes[-1], weight='length_km')\n",
    "        except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "            continue\n",
    "        tried = 0; salv=False\n",
    "        for cand_nodes in alt_iter:\n",
    "            if cand_nodes == nodes: continue\n",
    "            tried += 1\n",
    "            if tried > K: break\n",
    "            eids = edge_ids_from_nodes(cand_nodes)\n",
    "            try:\n",
    "                row_feat = cand_features(eids, row, idx, use_graph=use_graph)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            Xc = pd.DataFrame([row_feat]); Xc['modulation'] = enc_mod(Xc['modulation'])\n",
    "            Xc_ord = ordered(Xc, feat_cols + CAT)\n",
    "            if int(clf.predict(scaler.transform(Xc_ord))[0]) == 1:\n",
    "                salv=True; break\n",
    "        outcomes.append(1 if salv else 0)\n",
    "    return outcomes  # list of 0/1 for considered demands\n",
    "\n",
    "# Train two QoT models for rerouting\n",
    "COL_BASE = BASE_QOT + CAT\n",
    "Xtr_b = train[COL_BASE].copy(); Xtr_b['modulation'] = enc_mod(Xtr_b['modulation'])\n",
    "ytr_b = train['qot_ok'].astype(int).values\n",
    "sc_b  = StandardScaler(); Xtr_bs = sc_b.fit_transform(ordered(Xtr_b, COL_BASE))\n",
    "clf_b = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                      alpha=1e-4, learning_rate_init=1e-3,\n",
    "                      max_iter=200, random_state=42).fit(Xtr_bs, ytr_b)\n",
    "\n",
    "COL_ENR = ENR_QOT + CAT\n",
    "Xtr_g = train[COL_ENR].copy(); Xtr_g['modulation'] = enc_mod(Xtr_g['modulation'])\n",
    "ytr_g = ytr_b\n",
    "sc_g  = StandardScaler(); Xtr_gs = sc_g.fit_transform(ordered(Xtr_g, COL_ENR))\n",
    "clf_g = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                      alpha=1e-4, learning_rate_init=1e-3,\n",
    "                      max_iter=200, random_state=42).fit(Xtr_gs, ytr_g)\n",
    "\n",
    "# Collect per-demand salvage outcomes (BASE / +GraphFea)\n",
    "out_base = reroute_eval_collect(BASE_QOT, clf_b, sc_b, use_graph=False, K=3, sample_n=600, seed=7)\n",
    "out_graph= reroute_eval_collect(ENR_QOT,  clf_g, sc_g, use_graph=True,  K=3, sample_n=600, seed=7)\n",
    "\n",
    "def salvage_rate(lst): \n",
    "    return (np.mean(lst)*100.0 if len(lst)>0 else np.nan), len(lst)\n",
    "\n",
    "# ------------------ Bootstrap helpers ------------------\n",
    "def ci_percentile(samples, alpha=0.05):\n",
    "    lo = np.percentile(samples, 100*alpha/2)\n",
    "    hi = np.percentile(samples, 100*(1-alpha/2))\n",
    "    return float(lo), float(hi)\n",
    "\n",
    "def bootstrap_metric_binary(y, yhat, metric_fn, B=1000, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    N = len(y); vals=[]\n",
    "    for _ in range(B):\n",
    "        idx = rng.randint(0, N, size=N)\n",
    "        vals.append(metric_fn(y[idx], yhat[idx]))\n",
    "    return np.array(vals)\n",
    "\n",
    "def bootstrap_auc(y, p, B=1000, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    N = len(y); vals=[]\n",
    "    for _ in range(B):\n",
    "        idx = rng.randint(0, N, size=N)\n",
    "        try:\n",
    "            vals.append(roc_auc_score(y[idx], p[idx]))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return np.array(vals)\n",
    "\n",
    "def bootstrap_salvage(outcomes, B=2000, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = len(outcomes); outs = np.array(outcomes, dtype=float)\n",
    "    vals=[]\n",
    "    for _ in range(B):\n",
    "        idx = rng.randint(0, n, size=n)\n",
    "        vals.append(outs[idx].mean()*100.0)\n",
    "    return np.array(vals)\n",
    "\n",
    "# ------------------ Compute CIs ------------------\n",
    "# QoT AUC\n",
    "auc0 = roc_auc_score(yte_q, p_zero)\n",
    "aucA = roc_auc_score(yte_q, p_tta)\n",
    "b0   = bootstrap_auc(yte_q, p_zero, B=1000, seed=1)\n",
    "bA   = bootstrap_auc(yte_q, p_tta,  B=1000, seed=2)\n",
    "dA   = bA[:min(len(bA),len(b0))] - b0[:min(len(bA),len(b0))]\n",
    "ci0  = ci_percentile(b0); ciA = ci_percentile(bA); cid = ci_percentile(dA)\n",
    "\n",
    "# Failure F1 (+GraphFea)\n",
    "f1  = f1_score(yte_f, yhat_f, average='macro')\n",
    "bf1 = bootstrap_metric_binary(yte_f, yhat_f, lambda yt, yh: f1_score(yt, yh, average='macro'), B=1000, seed=3)\n",
    "cif = ci_percentile(bf1)\n",
    "\n",
    "# Rerouting salvage\n",
    "rate_b, n_b = salvage_rate(out_base)\n",
    "rate_g, n_g = salvage_rate(out_graph)\n",
    "bb = bootstrap_salvage(out_base, B=2000, seed=4)\n",
    "bg = bootstrap_salvage(out_graph, B=2000, seed=5)\n",
    "bd = bg[:min(len(bg),len(bb))] - bb[:min(len(bg),len(bb))]\n",
    "cib = ci_percentile(bb); cig = ci_percentile(bg); cid_s = ci_percentile(bd)\n",
    "p_boot = float(np.mean(bd <= 0.0))  # one-sided: +GraphFea <= BASE\n",
    "\n",
    "# ------------------ Print tidy tables ------------------\n",
    "def tbl(rows, cols):\n",
    "    dfp = pd.DataFrame(rows, columns=cols)\n",
    "    display(dfp)\n",
    "\n",
    "print(\"\\n=== QoT (GEANT2, HARD) — AUC: Zero-shot vs TTA-lite ===\")\n",
    "tbl([\n",
    "    [\"Zero-shot\", round(auc0,4), f\"[{ci0[0]:.4f}, {ci0[1]:.4f}]\"],\n",
    "    [\"TTA-lite\",  round(aucA,4), f\"[{ciA[0]:.4f}, {ciA[1]:.4f}]\"],\n",
    "    [\"Δ (TTA - Zero)\", round(aucA-auc0,4), f\"[{cid[0]:.4f}, {cid[1]:.4f}]\"],\n",
    "], [\"Model\",\"AUC\",\"95% CI\"])\n",
    "\n",
    "print(\"\\n=== Failure Detection (GEANT2, HARD, +GraphFea) — Macro-F1 ===\")\n",
    "tbl([\n",
    "    [\"+GraphFea\", round(f1,4), f\"[{cif[0]:.4f}, {cif[1]:.4f}]\", len(yte_f)]\n",
    "], [\"Model\",\"F1 (macro)\",\"95% CI\",\"N\"])\n",
    "\n",
    "print(\"\\n=== QoT-guided Re-routing (GEANT2, HARD) — Salvage rate (%) ===\")\n",
    "tbl([\n",
    "    [\"BASE\",      round(rate_b,3), f\"[{cib[0]:.3f}, {cib[1]:.3f}]\", n_b],\n",
    "    [\"+GraphFea\", round(rate_g,3), f\"[{cig[0]:.3f}, {cig[1]:.3f}]\", n_g],\n",
    "    [\"Δ (+Graph - BASE)\", round(rate_g-rate_b,3), f\"[{cid_s[0]:.3f}, {cid_s[1]:.3f}]\", \"\"],\n",
    "    [\"Bootstrap one-sided p(Δ<=0)\", p_boot, \"\", \"\"],\n",
    "], [\"Model\",\"Salvage %\",\"95% CI\",\"Considered\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482103c1-bb46-4963-ab3e-18ef16165077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Seed sweep — QoT AUC (Zero vs TTA) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>auc_zero</th>\n",
       "      <th>auc_tta</th>\n",
       "      <th>delta_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>-0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9946</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  auc_zero  auc_tta  delta_auc\n",
       "0     1    0.9946   0.9951     0.0005\n",
       "1     2    0.9954   0.9957     0.0003\n",
       "2     3    0.9933   0.9939     0.0006\n",
       "3     4    0.9939   0.9939     0.0001\n",
       "4     5    0.9944   0.9939    -0.0004\n",
       "5     6    0.9944   0.9946     0.0002\n",
       "6     7    0.9940   0.9942     0.0001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Seed sweep — Failure Detection F1 (+GraphFea) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.9865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.9827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.9873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  f1_macro\n",
       "0     1    0.9861\n",
       "1     2    0.9895\n",
       "2     3    0.9880\n",
       "3     4    0.9850\n",
       "4     5    0.9865\n",
       "5     6    0.9827\n",
       "6     7    0.9873"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Seed sweep — Rerouting Salvage % (BASE vs +GraphFea) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>salv_base</th>\n",
       "      <th>salv_graph</th>\n",
       "      <th>delta_salv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22.013</td>\n",
       "      <td>26.036</td>\n",
       "      <td>4.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>25.175</td>\n",
       "      <td>26.000</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19.463</td>\n",
       "      <td>20.859</td>\n",
       "      <td>1.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>25.000</td>\n",
       "      <td>24.161</td>\n",
       "      <td>-0.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>22.297</td>\n",
       "      <td>19.286</td>\n",
       "      <td>-3.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>19.333</td>\n",
       "      <td>20.915</td>\n",
       "      <td>1.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>19.231</td>\n",
       "      <td>17.834</td>\n",
       "      <td>-1.396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  salv_base  salv_graph  delta_salv\n",
       "0     1     22.013      26.036       4.023\n",
       "1     2     25.175      26.000       0.825\n",
       "2     3     19.463      20.859       1.396\n",
       "3     4     25.000      24.161      -0.839\n",
       "4     5     22.297      19.286      -3.012\n",
       "5     6     19.333      20.915       1.582\n",
       "6     7     19.231      17.834      -1.396"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Seed sweep — Median ± IQR (paper-ready) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Median</th>\n",
       "      <th>IQR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QoT AUC (Zero)</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QoT AUC (TTA)</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QoT ΔAUC (TTA−Zero)</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Failure F1 (macro, +GraphFea)</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reroute Salvage % (BASE)</td>\n",
       "      <td>22.0130</td>\n",
       "      <td>4.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reroute Salvage % (+GraphFea)</td>\n",
       "      <td>20.9150</td>\n",
       "      <td>5.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Δ Salvage pp (+Graph−BASE)</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>2.6060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Metric   Median     IQR\n",
       "0                 QoT AUC (Zero)   0.9944  0.0005\n",
       "1                  QoT AUC (TTA)   0.9942  0.0009\n",
       "2            QoT ΔAUC (TTA−Zero)   0.0002  0.0003\n",
       "3  Failure F1 (macro, +GraphFea)   0.9865  0.0021\n",
       "4       Reroute Salvage % (BASE)  22.0130  4.2500\n",
       "5  Reroute Salvage % (+GraphFea)  20.9150  5.0080\n",
       "6     Δ Salvage pp (+Graph−BASE)   0.8250  2.6060"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved:\n",
      " - outputs\\metrics\\seed_sweep_qot.csv\n",
      " - outputs\\metrics\\seed_sweep_fail.csv\n",
      " - outputs\\metrics\\seed_sweep_reroute.csv\n",
      " - outputs\\metrics\\seed_sweep_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 24 — Reproducibility: seed sweep (median ± IQR) for QoT AUC, Failure F1, Rerouting salvage\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- Optional deps for rerouting ----------\n",
    "try:\n",
    "    import networkx as nx\n",
    "except Exception:\n",
    "    nx = None\n",
    "\n",
    "# ---------- Paths & data ----------\n",
    "ENRICHED = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "LINKS    = Path(\"./eon_links_timeseries.csv\")\n",
    "assert ENRICHED.exists(), \"Run Step 14 first (paths_graph_enriched.csv must exist).\"\n",
    "df = pd.read_csv(ENRICHED)\n",
    "\n",
    "links = None\n",
    "if LINKS.exists():\n",
    "    links = pd.read_csv(LINKS)\n",
    "\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()                     # NSFNET\n",
    "test  = df[(df[\"split\"]==\"test_target\") & (df[\"topology\"]==\"GEANT2\")].copy()  # GEANT2\n",
    "\n",
    "# ---------- Features (HARD) ----------\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "GF_COLS = [\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "    \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]\n",
    "ENR_QOT  = BASE_QOT + GF_COLS\n",
    "\n",
    "BASE_FAIL= [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "ENR_FAIL = BASE_FAIL + GF_COLS\n",
    "\n",
    "CAT = ['modulation']\n",
    "MOD_CATS = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def enc_mod(s): return pd.Categorical(s, categories=MOD_CATS).codes\n",
    "def ordered(df_in, cols): \n",
    "    missing = [c for c in cols if c not in df_in.columns]\n",
    "    if missing: \n",
    "        raise KeyError(f\"Missing columns: {missing}\")\n",
    "    return df_in[cols].copy()\n",
    "\n",
    "# ---------- TTA-lite helpers ----------\n",
    "def bn_adapt_batch(X_raw, sc, gamma=0.3):\n",
    "    mu_src, sd_src = sc.mean_, sc.scale_\n",
    "    mu_b  = X_raw.mean(axis=0); sd_b = X_raw.std(axis=0, ddof=0) + 1e-6\n",
    "    mu = (1-gamma)*mu_src + gamma*mu_b\n",
    "    sd = (1-gamma)*sd_src + gamma*sd_b\n",
    "    return (X_raw - mu)/sd\n",
    "\n",
    "def tta_probs(X_raw, sc, clf, aug_n=5, jitter=0.03, gamma=0.3, seed=7):\n",
    "    Xb = bn_adapt_batch(X_raw, sc, gamma=gamma)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    probs = []\n",
    "    for _ in range(aug_n):\n",
    "        Xj = Xb.copy()\n",
    "        noise = rng.normal(0.0, jitter, size=Xj[:,:-1].shape)  # don't jitter modulation col\n",
    "        Xj[:,:-1] = Xj[:,:-1] * (1 + noise)\n",
    "        probs.append(clf.predict_proba(Xj)[:,1])\n",
    "    return np.mean(probs, axis=0)\n",
    "\n",
    "# ---------- Rerouting helpers ----------\n",
    "def build_graph_for_day(day, links_df):\n",
    "    sub = links_df[(links_df['topology']=='GEANT2') & (links_df['day']==day)]\n",
    "    G = nx.Graph()\n",
    "    for _, r in sub.iterrows():\n",
    "        u, v = int(r['u']), int(r['v'])\n",
    "        G.add_edge(u, v, length_km=float(r['length_km']))\n",
    "    return G, sub.set_index('edge_id')\n",
    "\n",
    "def edge_ids_from_nodes(nodes):\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(nodes[:-1], nodes[1:])]\n",
    "\n",
    "def path_aggs(eids, idx):\n",
    "    rows = idx.loc[eids]\n",
    "    hops = len(eids)\n",
    "    base = {\n",
    "        \"hops\": hops,\n",
    "        \"distance_km\": float(rows['length_km'].sum()),\n",
    "        \"latency_ms\":  float(rows['latency_ms'].sum()),\n",
    "        \"avg_utilization\": float(rows['bandwidth_utilization'].mean()),\n",
    "        \"min_osnr_db\": float(rows['osnr_db'].min()),\n",
    "        \"min_snr_db\":  float(rows['snr_db'].min()),\n",
    "        \"max_center_offset_ghz\": float(rows['center_freq_offset_ghz'].max()),\n",
    "        \"min_filter_bw_scale\":   float(rows['filter_bw_scale'].min()),\n",
    "    }\n",
    "    osnrs = rows['osnr_db'].values\n",
    "    utils = rows['bandwidth_utilization'].values\n",
    "    shifts= rows['center_freq_offset_ghz'].values\n",
    "    scales= rows['filter_bw_scale'].values\n",
    "    gf = {\n",
    "        \"gf_osnr_min\": float(osnrs.min()),\n",
    "        \"gf_osnr_var\": float(np.var(osnrs)) if hops>1 else 0.0,\n",
    "        \"gf_util_mean\": float(utils.mean()),\n",
    "        \"gf_util_max\": float(utils.max()),\n",
    "        \"gf_shift_max\": float(shifts.max()),\n",
    "        \"gf_scale_min\": float(scales.min()),\n",
    "        \"gf_frac_shifted\": float((shifts>0).mean()),\n",
    "        \"gf_frac_tight\": float((scales<1.0).mean()),\n",
    "        \"gf_bot_pos\": float(np.argmin(osnrs)/max(1,hops-1))\n",
    "    }\n",
    "    return base, gf\n",
    "\n",
    "def cand_features(eids, tx_row, idx, use_graph):\n",
    "    base, gf = path_aggs(eids, idx)\n",
    "    row = {\n",
    "        **base,\n",
    "        \"symbol_rate_gbaud\": float(tx_row['symbol_rate_gbaud']),\n",
    "        \"bitrate_gbps\": float(tx_row['bitrate_gbps']),\n",
    "        \"modulation\": tx_row['modulation']\n",
    "    }\n",
    "    if use_graph:\n",
    "        row.update(gf)\n",
    "    return row\n",
    "\n",
    "def reroute_eval_seed(feat_cols, clf, scaler, use_graph, K=3, sample_n=600, seed=7):\n",
    "    \"\"\"Return salvage rate (%) for given model under a fixed seed.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    sample = test.sample(n=min(sample_n, len(test)), random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # who needs reroute?\n",
    "    Xtest = sample[feat_cols + CAT].copy(); Xtest['modulation'] = enc_mod(Xtest['modulation'])\n",
    "    Xtest_ord = ordered(Xtest, feat_cols + CAT)\n",
    "    need = (clf.predict_proba(scaler.transform(Xtest_ord))[:,1] < 0.5).astype(int)\n",
    "\n",
    "    considered = salvaged = 0\n",
    "    for i, row in sample.iterrows():\n",
    "        if int(need[i]) == 0:\n",
    "            continue\n",
    "        day = int(row['day'])\n",
    "        try:\n",
    "            G, idx = build_graph_for_day(day, links)\n",
    "        except Exception:\n",
    "            continue\n",
    "        nodes = [int(x) for x in row['path'].split('->')]\n",
    "        try:\n",
    "            alt_iter = nx.shortest_simple_paths(G, nodes[0], nodes[-1], weight='length_km')\n",
    "        except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "            continue\n",
    "        tried = 0; considered += 1\n",
    "        for cand_nodes in alt_iter:\n",
    "            if cand_nodes == nodes: \n",
    "                continue\n",
    "            tried += 1\n",
    "            if tried > K: \n",
    "                break\n",
    "            eids = edge_ids_from_nodes(cand_nodes)\n",
    "            try:\n",
    "                row_feat = cand_features(eids, row, idx, use_graph=use_graph)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            Xc = pd.DataFrame([row_feat]); Xc['modulation'] = enc_mod(Xc['modulation'])\n",
    "            Xc_ord = ordered(Xc, feat_cols + CAT)\n",
    "            if int(clf.predict(scaler.transform(Xc_ord))[0]) == 1:\n",
    "                salvaged += 1\n",
    "                break\n",
    "    return (salvaged/considered*100.0) if considered>0 else np.nan\n",
    "\n",
    "# ---------- Seed sweep ----------\n",
    "SEEDS = [1,2,3,4,5,6,7]\n",
    "rows_qot, rows_fail, rows_rer = [], [], []\n",
    "\n",
    "for s in SEEDS:\n",
    "    # --- QoT model (+GraphFea) trained on NSFNET\n",
    "    COL_QOT = ENR_QOT + CAT\n",
    "    Xtr = train[COL_QOT].copy(); Xtr['modulation'] = enc_mod(Xtr['modulation'])\n",
    "    ytr = train['qot_ok'].astype(int).values\n",
    "    sc_q  = StandardScaler(); Xtr_s = sc_q.fit_transform(ordered(Xtr, COL_QOT))\n",
    "    clf_q = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                          alpha=1e-4, learning_rate_init=1e-3,\n",
    "                          max_iter=200, random_state=s).fit(Xtr_s, ytr)\n",
    "    # GEANT2 probs: zero vs TTA\n",
    "    Xte = test[COL_QOT].copy(); Xte['modulation'] = enc_mod(Xte['modulation'])\n",
    "    yte = test['qot_ok'].astype(int).values\n",
    "    Xte_s = sc_q.transform(ordered(Xte, COL_QOT))\n",
    "    p_zero = clf_q.predict_proba(Xte_s)[:,1]\n",
    "    p_tta  = tta_probs(ordered(Xte, COL_QOT).values, sc_q, clf_q, aug_n=5, jitter=0.03, gamma=0.3, seed=s)\n",
    "    rows_qot.append(dict(seed=s, auc_zero=roc_auc_score(yte, p_zero), auc_tta=roc_auc_score(yte, p_tta),\n",
    "                         delta_auc=(roc_auc_score(yte, p_tta)-roc_auc_score(yte, p_zero))))\n",
    "\n",
    "    # --- Failure model (+GraphFea) trained on NSFNET\n",
    "    COL_F = ENR_FAIL + CAT\n",
    "    Xtrf = train[COL_F].copy(); Xtrf['modulation'] = enc_mod(Xtrf['modulation'])\n",
    "    ytrf = train['failure_present'].astype(int).values\n",
    "    sc_f = StandardScaler(); Xtrf_s = sc_f.fit_transform(ordered(Xtrf, COL_F))\n",
    "    clf_f= MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                         alpha=1e-4, learning_rate_init=1e-3,\n",
    "                         max_iter=200, random_state=s).fit(Xtrf_s, ytrf)\n",
    "    Xtef = test[COL_F].copy(); Xtef['modulation'] = enc_mod(Xtef['modulation'])\n",
    "    ytef = test['failure_present'].astype(int).values\n",
    "    Xtef_s = sc_f.transform(ordered(Xtef, COL_F))\n",
    "    yhat_f = clf_f.predict(Xtef_s)\n",
    "    rows_fail.append(dict(seed=s, f1_macro=f1_score(ytef, yhat_f, average='macro')))\n",
    "\n",
    "    # --- Rerouting salvage (BASE vs +GraphFea) — only if deps present\n",
    "    if (nx is not None) and (links is not None):\n",
    "        # BASE model for rerouting decision\n",
    "        COL_B = BASE_QOT + CAT\n",
    "        Xtrb = train[COL_B].copy(); Xtrb['modulation'] = enc_mod(Xtrb['modulation'])\n",
    "        ytrb = train['qot_ok'].astype(int).values\n",
    "        sc_b = StandardScaler(); Xtrb_s = sc_b.fit_transform(ordered(Xtrb, COL_B))\n",
    "        clf_b= MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                             alpha=1e-4, learning_rate_init=1e-3,\n",
    "                             max_iter=200, random_state=s).fit(Xtrb_s, ytrb)\n",
    "        # +GraphFea model for rerouting decision\n",
    "        sc_g = sc_q; clf_g = clf_q  # reuse QoT +GraphFea\n",
    "        salv_b = reroute_eval_seed(BASE_QOT, clf_b, sc_b, use_graph=False, K=3, sample_n=600, seed=s)\n",
    "        salv_g = reroute_eval_seed(ENR_QOT,  clf_g, sc_g, use_graph=True,  K=3, sample_n=600, seed=s)\n",
    "        rows_rer.append(dict(seed=s, salv_base=salv_b, salv_graph=salv_g, delta_salv=salv_g - salv_b))\n",
    "    else:\n",
    "        rows_rer.append(dict(seed=s, salv_base=np.nan, salv_graph=np.nan, delta_salv=np.nan))\n",
    "\n",
    "# ---------- Build DataFrames ----------\n",
    "df_qot  = pd.DataFrame(rows_qot)\n",
    "df_fail = pd.DataFrame(rows_fail)\n",
    "df_rer  = pd.DataFrame(rows_rer)\n",
    "\n",
    "OUTM = Path(\"./outputs/metrics\"); OUTM.mkdir(parents=True, exist_ok=True)\n",
    "df_qot.to_csv(OUTM/\"seed_sweep_qot.csv\", index=False)\n",
    "df_fail.to_csv(OUTM/\"seed_sweep_fail.csv\", index=False)\n",
    "df_rer.to_csv(OUTM/\"seed_sweep_reroute.csv\", index=False)\n",
    "\n",
    "# ---------- Summaries: median ± IQR ----------\n",
    "def med_iqr(s):\n",
    "    s = pd.Series(s).dropna()\n",
    "    if s.empty: return np.nan, np.nan\n",
    "    med = s.median()\n",
    "    iqr = s.quantile(0.75) - s.quantile(0.25)\n",
    "    return med, iqr\n",
    "\n",
    "summ_rows = []\n",
    "m, i = med_iqr(df_qot['auc_zero']); summ_rows.append([\"QoT AUC (Zero)\", round(m,4), round(i,4)])\n",
    "m, i = med_iqr(df_qot['auc_tta']);  summ_rows.append([\"QoT AUC (TTA)\",  round(m,4), round(i,4)])\n",
    "m, i = med_iqr(df_qot['delta_auc']);summ_rows.append([\"QoT ΔAUC (TTA−Zero)\", round(m,4), round(i,4)])\n",
    "m, i = med_iqr(df_fail['f1_macro']);summ_rows.append([\"Failure F1 (macro, +GraphFea)\", round(m,4), round(i,4)])\n",
    "m, i = med_iqr(df_rer['salv_base']);summ_rows.append([\"Reroute Salvage % (BASE)\", round(m,3), round(i,3)])\n",
    "m, i = med_iqr(df_rer['salv_graph']);summ_rows.append([\"Reroute Salvage % (+GraphFea)\", round(m,3), round(i,3)])\n",
    "m, i = med_iqr(df_rer['delta_salv']);summ_rows.append([\"Δ Salvage pp (+Graph−BASE)\", round(m,3), round(i,3)])\n",
    "\n",
    "summary = pd.DataFrame(summ_rows, columns=[\"Metric\",\"Median\",\"IQR\"])\n",
    "summary.to_csv(OUTM/\"seed_sweep_summary.csv\", index=False)\n",
    "\n",
    "# ---------- Print clean tables ----------\n",
    "print(\"\\n=== Seed sweep — QoT AUC (Zero vs TTA) ===\")\n",
    "display(df_qot.round(4))\n",
    "\n",
    "print(\"\\n=== Seed sweep — Failure Detection F1 (+GraphFea) ===\")\n",
    "display(df_fail.round(4))\n",
    "\n",
    "print(\"\\n=== Seed sweep — Rerouting Salvage % (BASE vs +GraphFea) ===\")\n",
    "display(df_rer.round(3))\n",
    "\n",
    "print(\"\\n=== Seed sweep — Median ± IQR (paper-ready) ===\")\n",
    "display(summary)\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", OUTM/\"seed_sweep_qot.csv\")\n",
    "print(\" -\", OUTM/\"seed_sweep_fail.csv\")\n",
    "print(\" -\", OUTM/\"seed_sweep_reroute.csv\")\n",
    "print(\" -\", OUTM/\"seed_sweep_summary.csv\")\n",
    "if (nx is None) or (links is None):\n",
    "    print(\"\\n[Note] Rerouting was skipped (missing networkx or links csv). Install `networkx` and ensure `eon_links_timeseries.csv` exists to enable it.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe461bf-3500-4bed-ad12-a4e17f57e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted temperature T = 3.010 (from 600 few-shot samples)\n",
      "\n",
      "=== QoT Calibration (held-out GEANT2) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "      <th>ACC</th>\n",
       "      <th>NLL</th>\n",
       "      <th>Brier</th>\n",
       "      <th>ECE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pre-calibration</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.2214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Temp-scaled</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.2045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model     AUC    ACC     NLL   Brier     ECE\n",
       "0  Pre-calibration  0.9944  0.964  0.1625  0.0305  0.2214\n",
       "1      Temp-scaled  0.9940  0.964  0.0901  0.0254  0.2045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - outputs\\metrics\\qot_calibration_metrics.csv\n",
      " - outputs\\figs\\qot_reliability_temp_scaling.png\n",
      " - outputs\\figs\\qot_risk_coverage_temp_scaling.png\n"
     ]
    }
   ],
   "source": [
    "# Step 25 (fixed) — Temperature scaling (calibration) + risk–coverage for QoT\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss, brier_score_loss, roc_auc_score, accuracy_score\n",
    "\n",
    "# ---------- Load ----------\n",
    "ENRICHED = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "FEWSHOT  = Path(\"./eon_target_fewshot.csv\")\n",
    "LINKS    = Path(\"./eon_links_timeseries.csv\")\n",
    "assert ENRICHED.exists(), \"Run Step 14 first.\"\n",
    "assert FEWSHOT.exists(),  \"Missing eon_target_fewshot.csv\"\n",
    "assert LINKS.exists(),    \"Missing eon_links_timeseries.csv\"\n",
    "\n",
    "df   = pd.read_csv(ENRICHED)   # already has gf_* columns\n",
    "few  = pd.read_csv(FEWSHOT)\n",
    "links= pd.read_csv(LINKS)\n",
    "\n",
    "# ---------- Splits ----------\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()                                     # NSFNET\n",
    "test  = df[(df[\"split\"]==\"test_target\") & (df[\"topology\"]==\"GEANT2\")].copy()       # GEANT2\n",
    "\n",
    "# ---------- Graph features for few-shot (if missing) ----------\n",
    "GF_COLS = [\"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "           \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"]\n",
    "\n",
    "lk_idx = links.set_index([\"topology\",\"day\",\"edge_id\"])\n",
    "def edge_ids_from_path(path_str):\n",
    "    ns = [int(x) for x in str(path_str).split(\"->\")]\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(ns[:-1], ns[1:])]\n",
    "\n",
    "def ensure_gf_columns(df_in):\n",
    "    if all(c in df_in.columns for c in GF_COLS):\n",
    "        return df_in.copy()\n",
    "    rows, miss = [], 0\n",
    "    for _, r in df_in.iterrows():\n",
    "        topo, day, path = r[\"topology\"], r[\"day\"], r[\"path\"]\n",
    "        eids = edge_ids_from_path(path)\n",
    "        try:\n",
    "            rows_link = lk_idx.loc[(topo, day, eids)]\n",
    "        except KeyError:\n",
    "            miss += 1; continue\n",
    "        hops   = len(eids)\n",
    "        osnrs  = rows_link[\"osnr_db\"].values\n",
    "        utils  = rows_link[\"bandwidth_utilization\"].values\n",
    "        shifts = rows_link[\"center_freq_offset_ghz\"].values\n",
    "        scales = rows_link[\"filter_bw_scale\"].values\n",
    "        gf = dict(\n",
    "            gf_osnr_min=float(osnrs.min()),\n",
    "            gf_osnr_var=float(np.var(osnrs)) if hops>1 else 0.0,\n",
    "            gf_util_mean=float(utils.mean()),\n",
    "            gf_util_max=float(utils.max()),\n",
    "            gf_shift_max=float(shifts.max()),\n",
    "            gf_scale_min=float(scales.min()),\n",
    "            gf_frac_shifted=float((shifts>0).mean()),\n",
    "            gf_frac_tight=float((scales<1.0).mean()),\n",
    "            gf_bot_pos=float(np.argmin(osnrs)/max(1,hops-1)),\n",
    "        )\n",
    "        row = r.to_dict(); row.update(gf); rows.append(row)\n",
    "    out = pd.DataFrame(rows)\n",
    "    if miss: print(f\"[info] few-shot gf_: dropped {miss} rows where link join failed.\")\n",
    "    return out\n",
    "\n",
    "few = ensure_gf_columns(few)\n",
    "\n",
    "# ---------- Held-out = GEANT2 \\ few-shot keys ----------\n",
    "KEY = ['topology','day','src','dst','path','hops','distance_km']\n",
    "test['_key'] = test[KEY].astype(str).agg('|'.join, axis=1)\n",
    "few['_key']  = few[KEY].astype(str).agg('|'.join, axis=1)\n",
    "held = test[~test['_key'].isin(set(few['_key']))].copy()\n",
    "\n",
    "# ---------- Feature schema (QoT, HARD + graph-aware) ----------\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "ENR_QOT = BASE_QOT + GF_COLS\n",
    "CAT     = ['modulation']\n",
    "COLS    = ENR_QOT + CAT\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "train_mod_cats = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def enc_mod(s): return pd.Categorical(s, categories=train_mod_cats).codes\n",
    "def ordered(df_in, cols): \n",
    "    missing = [c for c in cols if c not in df_in.columns]\n",
    "    if missing: raise KeyError(f\"Missing: {missing}\")\n",
    "    return df_in[cols].copy()\n",
    "def sigmoid(z): return 1.0/(1.0 + np.exp(-z))\n",
    "def logit(p):\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "# ---------- Train base QoT model on NSFNET ----------\n",
    "Xtr = train[COLS].copy(); Xtr['modulation'] = enc_mod(Xtr['modulation'])\n",
    "ytr = train['qot_ok'].astype(int).values\n",
    "sc  = StandardScaler(); Xtr_s = sc.fit_transform(ordered(Xtr, COLS))\n",
    "clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                    alpha=1e-4, learning_rate_init=1e-3,\n",
    "                    max_iter=200, random_state=42).fit(Xtr_s, ytr)\n",
    "\n",
    "# ---------- Build calibration & test matrices ----------\n",
    "# calibration from few-shot GEANT2\n",
    "Xcal = few[COLS].copy(); Xcal['modulation'] = enc_mod(Xcal['modulation'])\n",
    "ycal = few['qot_ok'].astype(int).values\n",
    "Xcal_s = sc.transform(ordered(Xcal, COLS))\n",
    "p_cal  = clf.predict_proba(Xcal_s)[:,1]\n",
    "z_cal  = logit(p_cal)  # logits from probabilities\n",
    "\n",
    "# held-out test\n",
    "Xte = held[COLS].copy(); Xte['modulation'] = enc_mod(Xte['modulation'])\n",
    "yte = held['qot_ok'].astype(int).values\n",
    "Xte_s = sc.transform(ordered(Xte, COLS))\n",
    "p_pre = clf.predict_proba(Xte_s)[:,1]\n",
    "z_te  = logit(p_pre)\n",
    "\n",
    "# ---------- Fit temperature T on calibration (minimize NLL) ----------\n",
    "def nll_for_T(T):\n",
    "    T = max(T, 1e-3)\n",
    "    p = sigmoid(z_cal / T)\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    return log_loss(ycal, p)\n",
    "\n",
    "grid = np.linspace(0.3, 3.0, 136)\n",
    "vals = np.array([nll_for_T(t) for t in grid])\n",
    "T0   = float(grid[np.argmin(vals)])\n",
    "local = np.linspace(max(0.1, T0-0.5), T0+0.5, 101)\n",
    "vals2 = np.array([nll_for_T(t) for t in local])\n",
    "T_hat = float(local[np.argmin(vals2)])\n",
    "print(f\"Fitted temperature T = {T_hat:.3f} (from {len(few)} few-shot samples)\")\n",
    "\n",
    "# ---------- Evaluate: pre/post calibration ----------\n",
    "p0 = p_pre                      # pre-calibration probabilities\n",
    "p1 = sigmoid(z_te / T_hat)      # temp-scaled probabilities\n",
    "\n",
    "def ece(p, y, n_bins=15):\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    e = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        idx = (p>=lo) & (p<hi) if i<n_bins-1 else (p>=lo) & (p<=hi)\n",
    "        if np.any(idx):\n",
    "            conf = p[idx].mean()\n",
    "            acc  = (y[idx]==(p[idx]>=0.5)).mean()\n",
    "            e   += (idx.mean()) * abs(acc - conf)\n",
    "    return float(e)\n",
    "\n",
    "def metrics(p, y):\n",
    "    return dict(\n",
    "        AUC = roc_auc_score(y, p),\n",
    "        ACC = accuracy_score(y, (p>=0.5).astype(int)),\n",
    "        NLL = log_loss(y, np.clip(p,1e-6,1-1e-6)),\n",
    "        Brier = brier_score_loss(y, p),\n",
    "        ECE = ece(p, y, n_bins=15)\n",
    "    )\n",
    "\n",
    "m0 = metrics(p0, yte)\n",
    "m1 = metrics(p1, yte)\n",
    "\n",
    "# ---------- Risk–coverage curves (selective accuracy) ----------\n",
    "def risk_coverage(p, y, steps=25):\n",
    "    conf = np.maximum(p, 1-p)\n",
    "    order = np.argsort(conf)[::-1]\n",
    "    covs, accs = [], []\n",
    "    for k in np.linspace(0.05, 1.0, steps):\n",
    "        n = max(1, int(k*len(y)))\n",
    "        idx = order[:n]\n",
    "        acc = accuracy_score(y[idx], (p[idx]>=0.5).astype(int))\n",
    "        covs.append(k); accs.append(acc)\n",
    "    return np.array(covs), np.array(accs)\n",
    "\n",
    "cov0, acc0 = risk_coverage(p0, yte)\n",
    "cov1, acc1 = risk_coverage(p1, yte)\n",
    "\n",
    "# ---------- Save + Print ----------\n",
    "OUTM = Path(\"./outputs/metrics\"); OUTF = Path(\"./outputs/figs\")\n",
    "OUTM.mkdir(parents=True, exist_ok=True); OUTF.mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame([{\"model\":\"pre\", **m0}, {\"model\":\"temp_scaled\", **m1}]).to_csv(OUTM/\"qot_calibration_metrics.csv\", index=False)\n",
    "\n",
    "# Reliability diagram\n",
    "def reliability_points(p, y, n_bins=15):\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    xs, ys = [], []\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        idx = (p>=lo) & (p<hi) if i<n_bins-1 else (p>=lo) & (p<=hi)\n",
    "        if np.any(idx):\n",
    "            xs.append(p[idx].mean())\n",
    "            ys.append((y[idx]==(p[idx]>=0.5)).mean())\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "x0,y0 = reliability_points(p0, yte)\n",
    "x1,y1 = reliability_points(p1, yte)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot([0,1],[0,1],'--',linewidth=1)\n",
    "plt.scatter(x0,y0,label=f'Pre-cal (ECE={m0[\"ECE\"]:.3f})', s=30)\n",
    "plt.scatter(x1,y1,label=f'Temp-scaled (ECE={m1[\"ECE\"]:.3f})', s=30, marker='s')\n",
    "plt.xlabel(\"Mean predicted confidence\"); plt.ylabel(\"Empirical accuracy\")\n",
    "plt.title(\"Reliability (QoT on GEANT2, held-out)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.savefig(OUTF/\"qot_reliability_temp_scaling.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# Risk–coverage\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(cov0, acc0, marker='o', label='Pre-cal')\n",
    "plt.plot(cov1, acc1, marker='s', label='Temp-scaled')\n",
    "plt.xlabel(\"Coverage (kept fraction)\"); plt.ylabel(\"Selective accuracy\")\n",
    "plt.title(\"Risk–Coverage (QoT on GEANT2, held-out)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.savefig(OUTF/\"qot_risk_coverage_temp_scaling.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# Inline paper-ready table\n",
    "def tidy(m): \n",
    "    return {k: (round(v,4) if isinstance(v,(int,float)) else v) for k,v in m.items()}\n",
    "print(\"\\n=== QoT Calibration (held-out GEANT2) ===\")\n",
    "display(pd.DataFrame([\n",
    "    {\"Model\":\"Pre-calibration\", **tidy(m0)},\n",
    "    {\"Model\":\"Temp-scaled\",     **tidy(m1)},\n",
    "])[[\"Model\",\"AUC\",\"ACC\",\"NLL\",\"Brier\",\"ECE\"]])\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", OUTM/\"qot_calibration_metrics.csv\")\n",
    "print(\" -\", OUTF/\"qot_reliability_temp_scaling.png\")\n",
    "print(\" -\", OUTF/\"qot_risk_coverage_temp_scaling.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf751cb-3791-47a1-9f51-71983228678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted T for TTA-lite = 2.980 (from 600 few-shot logits)\n",
      "\n",
      "=== QoT Calibration for TTA-lite (held-out GEANT2) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "      <th>ACC</th>\n",
       "      <th>NLL</th>\n",
       "      <th>Brier</th>\n",
       "      <th>ECE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTA pre-cal</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTA temp-scaled</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model     AUC    ACC     NLL   Brier     ECE\n",
       "0      TTA pre-cal  0.9945  0.959  0.1987  0.0362  0.2234\n",
       "1  TTA temp-scaled  0.9941  0.959  0.0996  0.0289  0.1996"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - outputs\\metrics\\qot_calibration_TTA_metrics.csv\n",
      " - outputs\\figs\\qot_reliability_tta_temp_scaling.png\n",
      " - outputs\\figs\\qot_risk_coverage_tta_temp_scaling.png\n"
     ]
    }
   ],
   "source": [
    "# Step 26 — Calibrate TTA-lite probabilities (few-shot temperature scaling) and evaluate on held-out GEANT2\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss, brier_score_loss, roc_auc_score, accuracy_score\n",
    "\n",
    "# ---------- Load ----------\n",
    "ENRICHED = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "FEWSHOT  = Path(\"./eon_target_fewshot.csv\")\n",
    "LINKS    = Path(\"./eon_links_timeseries.csv\")\n",
    "assert ENRICHED.exists(), \"Run Step 14 first.\"\n",
    "assert FEWSHOT.exists(),  \"Missing eon_target_fewshot.csv\"\n",
    "assert LINKS.exists(),    \"Missing eon_links_timeseries.csv\"\n",
    "\n",
    "df   = pd.read_csv(ENRICHED)   # already has gf_* columns\n",
    "few  = pd.read_csv(FEWSHOT)\n",
    "links= pd.read_csv(LINKS)\n",
    "\n",
    "# ---------- Splits ----------\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()                                     # NSFNET\n",
    "test  = df[(df[\"split\"]==\"test_target\") & (df[\"topology\"]==\"GEANT2\")].copy()       # GEANT2\n",
    "\n",
    "# ---------- Ensure gf_* present for few-shot (if user exported a minimal few-shot file) ----------\n",
    "GF_COLS = [\"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "           \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"]\n",
    "\n",
    "lk_idx = links.set_index([\"topology\",\"day\",\"edge_id\"])\n",
    "def edge_ids_from_path(path_str):\n",
    "    ns = [int(x) for x in str(path_str).split(\"->\")]\n",
    "    return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(ns[:-1], ns[1:])]\n",
    "\n",
    "def ensure_gf_columns(df_in):\n",
    "    if all(c in df_in.columns for c in GF_COLS):\n",
    "        return df_in.copy()\n",
    "    rows, miss = [], 0\n",
    "    for _, r in df_in.iterrows():\n",
    "        topo, day, path = r[\"topology\"], r[\"day\"], r[\"path\"]\n",
    "        eids = edge_ids_from_path(path)\n",
    "        try:\n",
    "            rows_link = lk_idx.loc[(topo, day, eids)]\n",
    "        except KeyError:\n",
    "            miss += 1; continue\n",
    "        hops   = len(eids)\n",
    "        osnrs  = rows_link[\"osnr_db\"].values\n",
    "        utils  = rows_link[\"bandwidth_utilization\"].values\n",
    "        shifts = rows_link[\"center_freq_offset_ghz\"].values\n",
    "        scales = rows_link[\"filter_bw_scale\"].values\n",
    "        gf = dict(\n",
    "            gf_osnr_min=float(osnrs.min()),\n",
    "            gf_osnr_var=float(np.var(osnrs)) if hops>1 else 0.0,\n",
    "            gf_util_mean=float(utils.mean()),\n",
    "            gf_util_max=float(utils.max()),\n",
    "            gf_shift_max=float(shifts.max()),\n",
    "            gf_scale_min=float(scales.min()),\n",
    "            gf_frac_shifted=float((shifts>0).mean()),\n",
    "            gf_frac_tight=float((scales<1.0).mean()),\n",
    "            gf_bot_pos=float(np.argmin(osnrs)/max(1,hops-1)),\n",
    "        )\n",
    "        row = r.to_dict(); row.update(gf); rows.append(row)\n",
    "    out = pd.DataFrame(rows)\n",
    "    if miss: print(f\"[info] few-shot gf_: dropped {miss} rows where link join failed.\")\n",
    "    return out\n",
    "\n",
    "few = ensure_gf_columns(few)\n",
    "\n",
    "# ---------- Held-out = GEANT2 \\ few-shot keys ----------\n",
    "KEY = ['topology','day','src','dst','path','hops','distance_km']\n",
    "test['_key'] = test[KEY].astype(str).agg('|'.join, axis=1)\n",
    "few['_key']  = few[KEY].astype(str).agg('|'.join, axis=1)\n",
    "held = test[~test['_key'].isin(set(few['_key']))].copy()\n",
    "\n",
    "# ---------- Feature schema (QoT, HARD + graph-aware) ----------\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "ENR_QOT = BASE_QOT + GF_COLS\n",
    "CAT     = ['modulation']\n",
    "COLS    = ENR_QOT + CAT\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "train_mod_cats = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def enc_mod(s): return pd.Categorical(s, categories=train_mod_cats).codes\n",
    "def ordered(df_in, cols): \n",
    "    miss = [c for c in cols if c not in df_in.columns]\n",
    "    if miss: raise KeyError(f\"Missing: {miss}\")\n",
    "    return df_in[cols].copy()\n",
    "def sigmoid(z): return 1.0/(1.0 + np.exp(-z))\n",
    "def logit(p):\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "# ---------- Train QoT model on NSFNET ----------\n",
    "Xtr = train[COLS].copy(); Xtr['modulation'] = enc_mod(Xtr['modulation'])\n",
    "ytr = train['qot_ok'].astype(int).values\n",
    "sc  = StandardScaler(); Xtr_s = sc.fit_transform(ordered(Xtr, COLS))\n",
    "clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                    alpha=1e-4, learning_rate_init=1e-3,\n",
    "                    max_iter=200, random_state=42).fit(Xtr_s, ytr)\n",
    "\n",
    "# ---------- TTA-lite helper (batch-norm adaptation per day + MC jitter) ----------\n",
    "def bn_adapt_batch(X_raw, sc, gamma=0.3):\n",
    "    mu_src, sd_src = sc.mean_, sc.scale_\n",
    "    mu_b  = X_raw.mean(axis=0); sd_b = X_raw.std(axis=0, ddof=0) + 1e-6\n",
    "    mu = (1-gamma)*mu_src + gamma*mu_b\n",
    "    sd = (1-gamma)*sd_src + gamma*sd_b\n",
    "    return (X_raw - mu)/sd\n",
    "\n",
    "def tta_probs_for_rows(rows_df, day_batch_df, sc, clf, cols, aug_n=5, jitter=0.03, gamma=0.3, rng_seed=7):\n",
    "    # Batch stats from the whole GEANT2 day\n",
    "    Xb = day_batch_df[cols].copy()\n",
    "    Xb['modulation'] = enc_mod(Xb['modulation'])\n",
    "    Xb = ordered(Xb, cols).values\n",
    "    mu_src, sd_src = sc.mean_, sc.scale_\n",
    "    mu_b,  sd_b  = Xb.mean(axis=0), Xb.std(axis=0, ddof=0)+1e-6\n",
    "    mu = (1-gamma)*mu_src + gamma*mu_b\n",
    "    sd = (1-gamma)*sd_src + gamma*sd_b\n",
    "\n",
    "    Xr = rows_df[cols].copy()\n",
    "    Xr['modulation'] = enc_mod(Xr['modulation'])\n",
    "    Xr = ordered(Xr, cols).values\n",
    "    Xr_s = (Xr - mu)/sd\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    probs = []\n",
    "    for _ in range(aug_n):\n",
    "        Xj = Xr_s.copy()\n",
    "        noise = rng.normal(0.0, jitter, size=Xj[:,:-1].shape)  # do not jitter modulation\n",
    "        Xj[:,:-1] = Xj[:,:-1]*(1+noise)\n",
    "        probs.append(clf.predict_proba(Xj)[:,1])\n",
    "    return np.mean(probs, axis=0)\n",
    "\n",
    "# ---------- Build TTA-lite probs for calibration (few-shot) ----------\n",
    "p_cal_list, y_cal_list = [], []\n",
    "for d, grp in few.groupby('day'):\n",
    "    day_all = test[test['day']==d]      # unlabeled batch for that day (target domain)\n",
    "    if len(day_all)==0: \n",
    "        continue\n",
    "    p_cal_list.append(tta_probs_for_rows(grp, day_all, sc, clf, COLS))\n",
    "    y_cal_list.append(grp['qot_ok'].astype(int).values)\n",
    "p_cal = np.concatenate(p_cal_list) if p_cal_list else np.array([])\n",
    "y_cal = np.concatenate(y_cal_list) if y_cal_list else np.array([])\n",
    "assert len(p_cal)>0, \"No few-shot calibration probs were produced.\"\n",
    "z_cal = logit(p_cal)\n",
    "\n",
    "# ---------- Build TTA-lite probs for held-out GEANT2 ----------\n",
    "p_te_list, y_te_list = [], []\n",
    "for d, grp in held.groupby('day'):\n",
    "    day_all = test[test['day']==d]\n",
    "    if len(day_all)==0: \n",
    "        continue\n",
    "    p_te_list.append(tta_probs_for_rows(grp, day_all, sc, clf, COLS))\n",
    "    y_te_list.append(grp['qot_ok'].astype(int).values)\n",
    "p_pre = np.concatenate(p_te_list) if p_te_list else np.array([])\n",
    "y_te  = np.concatenate(y_te_list) if y_te_list else np.array([])\n",
    "assert len(p_pre)>0, \"No held-out TTA probabilities were produced.\"\n",
    "z_te = logit(p_pre)\n",
    "\n",
    "# ---------- Fit temperature on few-shot (minimize NLL) ----------\n",
    "def nll_for_T(T):\n",
    "    T = max(T, 1e-3)\n",
    "    p = sigmoid(z_cal / T)\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    return log_loss(y_cal, p)\n",
    "\n",
    "grid = np.linspace(0.3, 3.0, 136)\n",
    "vals = np.array([nll_for_T(t) for t in grid])\n",
    "T0   = float(grid[np.argmin(vals)])\n",
    "local = np.linspace(max(0.1, T0-0.5), T0+0.5, 101)\n",
    "vals2 = np.array([nll_for_T(t) for t in local])\n",
    "T_hat = float(local[np.argmin(vals2)])\n",
    "print(f\"Fitted T for TTA-lite = {T_hat:.3f} (from {len(y_cal)} few-shot logits)\")\n",
    "\n",
    "# ---------- Evaluate: pre/post calibration on held-out ----------\n",
    "p0 = p_pre\n",
    "p1 = sigmoid(z_te / T_hat)\n",
    "\n",
    "def ece(p, y, n_bins=15):\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    e = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        idx = (p>=lo) & (p<hi) if i<n_bins-1 else (p>=lo) & (p<=hi)\n",
    "        if np.any(idx):\n",
    "            conf = p[idx].mean()\n",
    "            acc  = (y[idx]==(p[idx]>=0.5)).mean()\n",
    "            e   += (idx.mean()) * abs(acc - conf)\n",
    "    return float(e)\n",
    "\n",
    "def metrics(p, y):\n",
    "    return dict(\n",
    "        AUC = roc_auc_score(y, p),\n",
    "        ACC = accuracy_score(y, (p>=0.5).astype(int)),\n",
    "        NLL = log_loss(y, np.clip(p,1e-6,1-1e-6)),\n",
    "        Brier = brier_score_loss(y, p),\n",
    "        ECE = ece(p, y, n_bins=15)\n",
    "    )\n",
    "\n",
    "m0 = metrics(p0, y_te)\n",
    "m1 = metrics(p1, y_te)\n",
    "\n",
    "# ---------- Risk–coverage ----------\n",
    "def risk_coverage(p, y, steps=25):\n",
    "    conf = np.maximum(p, 1-p)\n",
    "    order = np.argsort(conf)[::-1]\n",
    "    covs, accs = [], []\n",
    "    for k in np.linspace(0.05, 1.0, steps):\n",
    "        n = max(1, int(k*len(y)))\n",
    "        idx = order[:n]\n",
    "        acc = accuracy_score(y[idx], (p[idx]>=0.5).astype(int))\n",
    "        covs.append(k); accs.append(acc)\n",
    "    return np.array(covs), np.array(accs)\n",
    "\n",
    "cov0, acc0 = risk_coverage(p0, y_te)\n",
    "cov1, acc1 = risk_coverage(p1, y_te)\n",
    "\n",
    "# ---------- Save + Print ----------\n",
    "OUTM = Path(\"./outputs/metrics\"); OUTF = Path(\"./outputs/figs\")\n",
    "OUTM.mkdir(parents=True, exist_ok=True); OUTF.mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame([{\"model\":\"tta_pre\", **m0}, {\"model\":\"tta_temp_scaled\", **m1}]).to_csv(OUTM/\"qot_calibration_TTA_metrics.csv\", index=False)\n",
    "\n",
    "# Reliability\n",
    "def reliability_points(p, y, n_bins=15):\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    xs, ys = [], []\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        idx = (p>=lo) & (p<hi) if i<n_bins-1 else (p>=lo) & (p<=hi)\n",
    "        if np.any(idx):\n",
    "            xs.append(p[idx].mean())\n",
    "            ys.append((y[idx]==(p[idx]>=0.5)).mean())\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "x0,y0 = reliability_points(p0, y_te)\n",
    "x1,y1 = reliability_points(p1, y_te)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot([0,1],[0,1],'--',linewidth=1)\n",
    "plt.scatter(x0,y0,label=f'TTA pre (ECE={m0[\"ECE\"]:.3f})', s=30)\n",
    "plt.scatter(x1,y1,label=f'TTA temp-scaled (ECE={m1[\"ECE\"]:.3f})', s=30, marker='s')\n",
    "plt.xlabel(\"Mean predicted confidence\"); plt.ylabel(\"Empirical accuracy\")\n",
    "plt.title(\"Reliability — TTA-lite (held-out GEANT2)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.savefig(OUTF/\"qot_reliability_tta_temp_scaling.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# Risk–coverage\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(cov0, acc0, marker='o', label='TTA pre')\n",
    "plt.plot(cov1, acc1, marker='s', label='TTA temp-scaled')\n",
    "plt.xlabel(\"Coverage (kept fraction)\"); plt.ylabel(\"Selective accuracy\")\n",
    "plt.title(\"Risk–Coverage — TTA-lite (held-out GEANT2)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.savefig(OUTF/\"qot_risk_coverage_tta_temp_scaling.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# Inline paper-ready table\n",
    "def tidy(m): \n",
    "    return {k: (round(v,4) if isinstance(v,(int,float)) else v) for k,v in m.items()}\n",
    "print(\"\\n=== QoT Calibration for TTA-lite (held-out GEANT2) ===\")\n",
    "display(pd.DataFrame([\n",
    "    {\"Model\":\"TTA pre-cal\",      **tidy(m0)},\n",
    "    {\"Model\":\"TTA temp-scaled\",  **tidy(m1)},\n",
    "])[[\"Model\",\"AUC\",\"ACC\",\"NLL\",\"Brier\",\"ECE\"]])\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", OUTM/\"qot_calibration_TTA_metrics.csv\")\n",
    "print(\" -\", OUTF/\"qot_reliability_tta_temp_scaling.png\")\n",
    "print(\" -\", OUTF/\"qot_risk_coverage_tta_temp_scaling.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8cd8a4-7e5c-4618-a76f-49890dc3309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created camera_ready pack at: C:\\devonboard\\research\\daily taskk\\EACE2025\\by gpt\\camera_ready\n",
      "\n",
      "Contents:\n",
      "📄 captions.md\n",
      "📄 checklist.md\n",
      "📁 figs\n",
      "   📄 abl_fail.png\n",
      "   📄 abl_fail_shift_tight.png\n",
      "   📄 abl_qot.png\n",
      "   📄 case_study_days_57_58_sd_1_22.png\n",
      "   📄 kshot_fail_enriched.png\n",
      "   📄 kshot_qot_enriched.png\n",
      "   📄 qot_reliability_temp_scaling.png\n",
      "   📄 qot_reliability_tta_temp_scaling.png\n",
      "   📄 qot_reliability_zero_vs_tta.png\n",
      "   📄 qot_risk_coverage_temp_scaling.png\n",
      "   📄 qot_risk_coverage_tta_temp_scaling.png\n",
      "   📄 qot_risk_coverage_zero_vs_tta.png\n",
      "   📄 qot_roc_zero_vs_tta.png\n",
      "📁 metrics\n",
      "   📄 ablations_summary.csv\n",
      "   📄 kshot_fail_enriched.csv\n",
      "   📄 kshot_qot_enriched.csv\n",
      "   📄 qot_calibration_metrics.csv\n",
      "   📄 qot_calibration_TTA_metrics.csv\n",
      "   📄 reroute_base.json\n",
      "   📄 reroute_graph.json\n",
      "   📄 reroute_summary.csv\n",
      "   📄 seed_sweep_fail.csv\n",
      "   📄 seed_sweep_qot.csv\n",
      "   📄 seed_sweep_reroute.csv\n",
      "   📄 seed_sweep_summary.csv\n",
      "\n",
      "Copied metrics: ['reroute_base.json', 'reroute_graph.json', 'reroute_summary.csv', 'kshot_qot_enriched.csv', 'kshot_fail_enriched.csv', 'ablations_summary.csv', 'qot_calibration_metrics.csv', 'qot_calibration_TTA_metrics.csv', 'seed_sweep_summary.csv', 'seed_sweep_qot.csv', 'seed_sweep_fail.csv', 'seed_sweep_reroute.csv']\n",
      "Copied figures: ['qot_roc_zero_vs_tta.png', 'qot_reliability_zero_vs_tta.png', 'qot_risk_coverage_zero_vs_tta.png', 'abl_qot.png', 'abl_fail.png', 'abl_fail_shift_tight.png', 'kshot_qot_enriched.png', 'kshot_fail_enriched.png', 'case_study_days_57_58_sd_1_22.png', 'qot_reliability_temp_scaling.png', 'qot_risk_coverage_temp_scaling.png', 'qot_reliability_tta_temp_scaling.png', 'qot_risk_coverage_tta_temp_scaling.png']\n",
      "\n",
      "Open `camera_ready/captions.md` for ready-to-paste text.\n"
     ]
    }
   ],
   "source": [
    "# Step 27 — Build a camera_ready/ pack + auto captions (paper-ready)\n",
    "import os, json, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "OUTM = ROOT/\"outputs\"/\"metrics\"\n",
    "OUTF = ROOT/\"outputs\"/\"figs\"\n",
    "PACK = ROOT/\"camera_ready\"\n",
    "PMET  = PACK/\"metrics\"\n",
    "PFIG  = PACK/\"figs\"\n",
    "\n",
    "# Create dirs\n",
    "PMET.mkdir(parents=True, exist_ok=True)\n",
    "PFIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Collect metrics (copy if exists) ----------\n",
    "metric_names = [\n",
    "    \"reroute_base.json\",\n",
    "    \"reroute_graph.json\",\n",
    "    \"reroute_summary.csv\",\n",
    "    \"kshot_qot_enriched.csv\",\n",
    "    \"kshot_fail_enriched.csv\",\n",
    "    \"ablations_summary.csv\",\n",
    "    \"qot_calibration_metrics.csv\",\n",
    "    \"qot_calibration_TTA_metrics.csv\",\n",
    "    \"seed_sweep_summary.csv\",\n",
    "    \"seed_sweep_qot.csv\",\n",
    "    \"seed_sweep_fail.csv\",\n",
    "    \"seed_sweep_reroute.csv\",\n",
    "]\n",
    "copied_metrics, missing_metrics = [], []\n",
    "for name in metric_names:\n",
    "    src = OUTM/name\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, PMET/name)\n",
    "        copied_metrics.append(name)\n",
    "    else:\n",
    "        missing_metrics.append(name)\n",
    "\n",
    "# ---------- Collect figures (copy if exists) ----------\n",
    "fig_names = [\n",
    "    # zero-shot vs TTA (if you created them earlier)\n",
    "    \"qot_roc_zero_vs_tta.png\",\n",
    "    \"qot_reliability_zero_vs_tta.png\",\n",
    "    \"qot_risk_coverage_zero_vs_tta.png\",\n",
    "    # ablations\n",
    "    \"abl_qot.png\",\n",
    "    \"abl_fail.png\",\n",
    "    \"abl_fail_shift_tight.png\",\n",
    "    # k-shot (if made)\n",
    "    \"kshot_qot_enriched.png\",\n",
    "    \"kshot_fail_enriched.png\",\n",
    "    # case study\n",
    "    *[p.name for p in OUTF.glob(\"case_study_days_*.png\")],\n",
    "    # calibration (step 25)\n",
    "    \"qot_reliability_temp_scaling.png\",\n",
    "    \"qot_risk_coverage_temp_scaling.png\",\n",
    "    # calibration for TTA (step 26)\n",
    "    \"qot_reliability_tta_temp_scaling.png\",\n",
    "    \"qot_risk_coverage_tta_temp_scaling.png\",\n",
    "]\n",
    "# de-dup while preserving order\n",
    "seen = set(); fig_names = [f for f in fig_names if (f not in seen and not seen.add(f))]\n",
    "\n",
    "copied_figs, missing_figs = [], []\n",
    "for name in fig_names:\n",
    "    src = OUTF/name\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, PFIG/name)\n",
    "        copied_figs.append(name)\n",
    "    else:\n",
    "        missing_figs.append(name)\n",
    "\n",
    "# ---------- Read key numbers for captions ----------\n",
    "def try_json(path):\n",
    "    try:\n",
    "        return json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def try_csv(path):\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "rer_b = try_json(PMET/\"reroute_base.json\")\n",
    "rer_g = try_json(PMET/\"reroute_graph.json\")\n",
    "kq    = try_csv(PMET/\"kshot_qot_enriched.csv\")\n",
    "kf    = try_csv(PMET/\"kshot_fail_enriched.csv\")\n",
    "abl   = try_csv(PMET/\"ablations_summary.csv\")\n",
    "cal_q = try_csv(PMET/\"qot_calibration_metrics.csv\")\n",
    "cal_t = try_csv(PMET/\"qot_calibration_TTA_metrics.csv\")\n",
    "seedS = try_csv(PMET/\"seed_sweep_summary.csv\")\n",
    "\n",
    "def fmt(x, nd=3, pct=False):\n",
    "    if x is None or (isinstance(x,float) and (np.isnan(x) or np.isinf(x))):\n",
    "        return \"—\"\n",
    "    if pct: return f\"{x:.{nd}f}%\"\n",
    "    return f\"{x:.{nd}f}\"\n",
    "\n",
    "lines = []\n",
    "lines.append(\"# Camera-ready Captions (auto-generated)\\n\")\n",
    "\n",
    "# Rerouting\n",
    "if rer_b and rer_g:\n",
    "    dpp = (rer_g.get(\"salvage_rate_pct\",0) - rer_b.get(\"salvage_rate_pct\",0))\n",
    "    lines += [\n",
    "        \"## Fig. R1 — QoT-guided re-routing (GEANT2, zero-shot, HARD)\",\n",
    "        f\"Salvage rate: **BASE {fmt(rer_b.get('salvage_rate_pct'),2,pct=True)}** → **+GraphFea {fmt(rer_g.get('salvage_rate_pct'),2,pct=True)}** \"\n",
    "        f\"(Δ **{fmt(dpp,2)} pp**). Overhead (avg): BASE {fmt(rer_b.get('avg_extra_km'))} km / {fmt(rer_b.get('avg_extra_ms'))} ms; \"\n",
    "        f\"+GraphFea {fmt(rer_g.get('avg_extra_km'))} km / {fmt(rer_g.get('avg_extra_ms'))} ms.\\n\"\n",
    "    ]\n",
    "\n",
    "# k-shot curves\n",
    "if kq is not None:\n",
    "    kq = kq.sort_values(\"k\")\n",
    "    last = kq.iloc[-1].to_dict() if not kq.empty else None\n",
    "    if last:\n",
    "        lines += [\n",
    "            \"## Fig. K1 — Few-shot (QoT, graph-enriched, HARD)\",\n",
    "            f\"F1↑ with k: at k={int(last['k'])}, **F1={fmt(last['f1'],4)}**, **AUC={fmt(last['auc'],4)}**.\\n\"\n",
    "        ]\n",
    "if kf is not None:\n",
    "    kf = kf.sort_values(\"k\")\n",
    "    last = kf.iloc[-1].to_dict() if not kf.empty else None\n",
    "    if last:\n",
    "        lines += [\n",
    "            \"## Fig. K2 — Few-shot (Failure detection, graph-enriched, HARD)\",\n",
    "            f\"F1↑ with k: at k={int(last['k'])}, **F1={fmt(last['f1'],4)}**.\\n\"\n",
    "        ]\n",
    "\n",
    "# Ablations\n",
    "if abl is not None:\n",
    "    def pick(task, scen, k=0):\n",
    "        row = abl[(abl.task==task) & (abl.scenario==scen) & (abl.k==k)]\n",
    "        return row.iloc[0].to_dict() if not row.empty else None\n",
    "    qb = pick(\"qot\",\"base\",0); qg0 = pick(\"qot\",\"+graph\",0); qg100 = pick(\"qot\",\"+graph\",100)\n",
    "    fg0 = pick(\"fail\",\"+graph\",0); fb0 = pick(\"fail\",\"base\",0)\n",
    "    if qb and qg0:\n",
    "        lines += [\n",
    "            \"## Fig. A1 — QoT ablations (GEANT2, HARD)\",\n",
    "            f\"Base F1 **{fmt(qb['f1'],4)}** vs +Graph F1 **{fmt(qg0['f1'],4)}**; +Graph (k=100) **{fmt(qg100['f1'],4)}**.\\n\"\n",
    "        ]\n",
    "    if fb0 and fg0:\n",
    "        lines += [\n",
    "            \"## Fig. A2 — Failure ablations (GEANT2, HARD)\",\n",
    "            f\"Base F1 **{fmt(fb0['f1'],4)}** vs +Graph F1 **{fmt(fg0['f1'],4)}**. Removing fingerprints drops to hard mode; removing OSNR keeps F1 high.\\n\"\n",
    "        ]\n",
    "\n",
    "# Calibration (pre vs temp-scaled)\n",
    "if cal_q is not None:\n",
    "    try:\n",
    "        pre  = cal_q[cal_q[\"model\"]==\"pre\"].iloc[0].to_dict()\n",
    "        post = cal_q[cal_q[\"model\"]==\"temp_scaled\"].iloc[0].to_dict()\n",
    "        lines += [\n",
    "            \"## Fig. C1 — QoT calibration (few-shot temp scaling)\",\n",
    "            f\"NLL {fmt(pre['NLL'],4)} → **{fmt(post['NLL'],4)}**, Brier {fmt(pre['Brier'],4)} → **{fmt(post['Brier'],4)}**, \"\n",
    "            f\"ECE {fmt(pre['ECE'],4)} → **{fmt(post['ECE'],4)}**; AUC/ACC preserved.\\n\"\n",
    "        ]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if cal_t is not None:\n",
    "    try:\n",
    "        pre  = cal_t[cal_t[\"model\"]==\"tta_pre\"].iloc[0].to_dict()\n",
    "        post = cal_t[cal_t[\"model\"]==\"tta_temp_scaled\"].iloc[0].to_dict()\n",
    "        lines += [\n",
    "            \"## Fig. C2 — TTA-lite calibration (few-shot temp scaling)\",\n",
    "            f\"NLL {fmt(pre['NLL'],4)} → **{fmt(post['NLL'],4)}**, Brier {fmt(pre['Brier'],4)} → **{fmt(post['Brier'],4)}**, \"\n",
    "            f\"ECE {fmt(pre['ECE'],4)} → **{fmt(post['ECE'],4)}**; AUC/ACC preserved.\\n\"\n",
    "        ]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Seed sweep summary (stability)\n",
    "if seedS is not None:\n",
    "    lines += [\"## Table S — Stability (median ± IQR across seeds)\\n\"]\n",
    "    for _, r in seedS.iterrows():\n",
    "        lines.append(f\"- {r['Metric']}: **{fmt(r['Median'],4)} ± {fmt(r['IQR'],4)}**\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "# Write captions + checklist\n",
    "(PACK/\"captions.md\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "check = []\n",
    "check += [\"# Camera-ready checklist\",\n",
    "          \"- [x] Results CSV/JSON in `camera_ready/metrics/`\",\n",
    "          \"- [x] Figures in `camera_ready/figs/`\",\n",
    "          \"- [x] `captions.md` with numbers auto-filled\",\n",
    "          \"- [ ] Double-check figure dpi (≥300) if journal requires\",\n",
    "          \"- [ ] Ensure train/test split description references NSFNET→GEANT2 (90-day dynamics)\"]\n",
    "(PACK/\"checklist.md\").write_text(\"\\n\".join(check), encoding=\"utf-8\")\n",
    "\n",
    "# Print summary\n",
    "def tree(path: Path, prefix=\"\"):\n",
    "    for p in sorted(path.iterdir()):\n",
    "        print(prefix + (\"📁 \" if p.is_dir() else \"📄 \") + p.name)\n",
    "        if p.is_dir():\n",
    "            tree(p, prefix + \"   \")\n",
    "\n",
    "print(\"Created camera_ready pack at:\", PACK.resolve())\n",
    "print(\"\\nContents:\")\n",
    "tree(PACK)\n",
    "\n",
    "print(\"\\nCopied metrics:\", copied_metrics)\n",
    "if missing_metrics:\n",
    "    print(\"Missing (skipped):\", len(missing_metrics))\n",
    "    for m in missing_metrics: \n",
    "        pass  # keep output concise\n",
    "\n",
    "print(\"Copied figures:\", copied_figs)\n",
    "if missing_figs:\n",
    "    print(\"Some figures were not found (skipped). That's ok if you didn't generate them all.\")\n",
    "print(\"\\nOpen `camera_ready/captions.md` for ready-to-paste text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2a031d-d3bf-4d6b-8288-375ae51bf47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaging from: C:\\devonboard\\research\\daily taskk\\EACE2025\\by gpt\\camera_ready\n",
      "\n",
      "Created ZIP: C:\\devonboard\\research\\daily taskk\\EACE2025\\by gpt\\camera_ready_20250822_225045.zip\n",
      "Contents summary:\n",
      " - environment.txt: True\n",
      " - requirements.txt: True\n",
      " - manifest_checksums.csv: True\n"
     ]
    }
   ],
   "source": [
    "# Step 28 — Package camera_ready/ into a reproducible ZIP with env + checksums\n",
    "from pathlib import Path\n",
    "import sys, platform, datetime as dt, zipfile, hashlib, subprocess, textwrap\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "SRC  = ROOT/\"camera_ready\"\n",
    "ALT  = ROOT/\"paper_pack\"   # fallback if camera_ready absent\n",
    "OUT  = ROOT\n",
    "\n",
    "# ---------- 1) pick source folder ----------\n",
    "if not SRC.exists():\n",
    "    assert ALT.exists(), \"Neither camera_ready/ nor paper_pack/ found. Run Step 27 (or Step 19) first.\"\n",
    "    SRC = ALT\n",
    "print(\"Packaging from:\", SRC.resolve())\n",
    "\n",
    "# ---------- 2) write environment stamp ----------\n",
    "ENV_DIR = SRC/\"meta\"; ENV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def get_version(pkg):\n",
    "    try:\n",
    "        import importlib.metadata as im\n",
    "        return im.version(pkg)\n",
    "    except Exception:\n",
    "        try:\n",
    "            import pkg_resources as pr\n",
    "            return pr.get_distribution(pkg).version\n",
    "        except Exception:\n",
    "            return \"n/a\"\n",
    "\n",
    "env_lines = []\n",
    "env_lines.append(f\"Timestamp: {dt.datetime.now().isoformat(timespec='seconds')}\")\n",
    "env_lines.append(f\"Python   : {sys.version.split()[0]}\")\n",
    "env_lines.append(f\"Platform : {platform.platform()}\")\n",
    "for lib in [\"numpy\",\"pandas\",\"scikit-learn\",\"matplotlib\",\"networkx\"]:\n",
    "    env_lines.append(f\"{lib:12s}: {get_version(lib)}\")\n",
    "(ENV_DIR/\"environment.txt\").write_text(\"\\n\".join(env_lines), encoding=\"utf-8\")\n",
    "\n",
    "# try to capture full requirements\n",
    "req_path = ENV_DIR/\"requirements.txt\"\n",
    "try:\n",
    "    out = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], text=True)\n",
    "    req_path.write_text(out, encoding=\"utf-8\")\n",
    "except Exception as e:\n",
    "    # minimal fallback\n",
    "    req_min = textwrap.dedent(f\"\"\"\\\n",
    "    numpy=={get_version('numpy')}\n",
    "    pandas=={get_version('pandas')}\n",
    "    scikit-learn=={get_version('scikit-learn')}\n",
    "    matplotlib=={get_version('matplotlib')}\n",
    "    networkx=={get_version('networkx')}\n",
    "    \"\"\").strip()+\"\\n\"\n",
    "    req_path.write_text(req_min, encoding=\"utf-8\")\n",
    "\n",
    "# ---------- 3) checksums for reproducibility ----------\n",
    "def sha256_of(path: Path, chunk=1024*1024):\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk)\n",
    "            if not b: break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "manifest_rows = [\"relpath,bytes,sha256\"]\n",
    "for p in sorted(SRC.rglob(\"*\")):\n",
    "    if p.is_file():\n",
    "        rel = p.relative_to(SRC).as_posix()\n",
    "        manifest_rows.append(f\"{rel},{p.stat().st_size},{sha256_of(p)}\")\n",
    "(ENV_DIR/\"manifest_checksums.csv\").write_text(\"\\n\".join(manifest_rows), encoding=\"utf-8\")\n",
    "\n",
    "# ---------- 4) zip it ----------\n",
    "stamp = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "zip_name = OUT/f\"{SRC.name}_{stamp}.zip\"\n",
    "with zipfile.ZipFile(zip_name, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    for p in SRC.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            z.write(p, p.relative_to(SRC))\n",
    "\n",
    "print(\"\\nCreated ZIP:\", zip_name.resolve())\n",
    "print(\"Contents summary:\")\n",
    "print(\" - environment.txt:\", (ENV_DIR/\"environment.txt\").exists())\n",
    "print(\" - requirements.txt:\", (ENV_DIR/\"requirements.txt\").exists())\n",
    "print(\" - manifest_checksums.csv:\", (ENV_DIR/\"manifest_checksums.csv\").exists())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47490e4b-6e01-427b-9758-b1c52eeb323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QoT (+GraphFea) on GEANT2 — AUC=0.9938\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_mean</th>\n",
       "      <th>importance_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bitrate_gbps</td>\n",
       "      <td>0.4487</td>\n",
       "      <td>0.0065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>symbol_rate_gbaud</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>0.0042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>modulation</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gf_osnr_min</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>min_osnr_db</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>min_snr_db</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hops</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gf_scale_min</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gf_util_max</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gf_frac_tight</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  importance_mean  importance_std\n",
       "0       bitrate_gbps           0.4487          0.0065\n",
       "1  symbol_rate_gbaud           0.0876          0.0042\n",
       "2         modulation           0.0608          0.0023\n",
       "3        gf_osnr_min           0.0395          0.0024\n",
       "4        min_osnr_db           0.0348          0.0023\n",
       "5         min_snr_db           0.0159          0.0015\n",
       "6               hops           0.0034          0.0005\n",
       "7       gf_scale_min           0.0016          0.0002\n",
       "8        gf_util_max           0.0015          0.0002\n",
       "9      gf_frac_tight           0.0013          0.0002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failure (+GraphFea) on GEANT2 — Macro-F1=0.9862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_mean</th>\n",
       "      <th>importance_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gf_scale_min</td>\n",
       "      <td>0.2170</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gf_shift_max</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.0039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gf_frac_shifted</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gf_frac_tight</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gf_osnr_var</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gf_util_mean</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gf_util_max</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>symbol_rate_gbaud</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hops</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>avg_utilization</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  importance_mean  importance_std\n",
       "0       gf_scale_min           0.2170          0.0037\n",
       "1       gf_shift_max           0.2000          0.0039\n",
       "2    gf_frac_shifted           0.0325          0.0010\n",
       "3      gf_frac_tight           0.0110          0.0010\n",
       "4        gf_osnr_var           0.0056          0.0012\n",
       "5       gf_util_mean           0.0021          0.0017\n",
       "6        gf_util_max           0.0019          0.0013\n",
       "7  symbol_rate_gbaud           0.0005          0.0004\n",
       "8               hops           0.0003          0.0009\n",
       "9    avg_utilization           0.0001          0.0009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved:\n",
      " - outputs\\metrics\\permimp_qot.csv\n",
      " - outputs\\metrics\\permimp_fail.csv\n",
      " - outputs\\figs\\permimp_qot.png\n",
      " - outputs\\figs\\permimp_fail.png\n"
     ]
    }
   ],
   "source": [
    "# Step 29 — Permutation Feature Importance (QoT and Failure, +GraphFea) on GEANT2\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ---------- Load enriched paths ----------\n",
    "ENRICHED = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "assert ENRICHED.exists(), \"Run Step 14 first (paths_graph_enriched.csv).\"\n",
    "df = pd.read_csv(ENRICHED)\n",
    "\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()                                  # NSFNET\n",
    "test  = df[(df[\"split\"]==\"test_target\") & (df[\"topology\"]==\"GEANT2\")].copy()    # GEANT2\n",
    "\n",
    "# ---------- Feature schemas (HARD + graph-aware) ----------\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "GF_COLS = [\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "    \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]\n",
    "ENR_QOT  = BASE_QOT + GF_COLS\n",
    "BASE_FAIL= [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "ENR_FAIL = BASE_FAIL + GF_COLS\n",
    "CAT = ['modulation']\n",
    "\n",
    "# consistent modulation encoding\n",
    "mod_cats = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def enc_mod(s): return pd.Categorical(s, categories=mod_cats).codes\n",
    "def prep_X(df_in, cols):\n",
    "    X = df_in[cols + CAT].copy()\n",
    "    X['modulation'] = enc_mod(X['modulation'])\n",
    "    return X.values, cols + CAT  # return numpy + names\n",
    "\n",
    "# ---------- Train QoT (+GraphFea) on NSFNET ----------\n",
    "Xtr_q, names_q = prep_X(train, ENR_QOT)\n",
    "ytr_q = train['qot_ok'].astype(int).values\n",
    "sc_q  = StandardScaler().fit(Xtr_q)\n",
    "clf_q = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                      alpha=1e-4, learning_rate_init=1e-3,\n",
    "                      max_iter=200, random_state=42).fit(sc_q.transform(Xtr_q), ytr_q)\n",
    "\n",
    "Xte_q, _ = prep_X(test, ENR_QOT)\n",
    "yte_q = test['qot_ok'].astype(int).values\n",
    "Xte_qs = sc_q.transform(Xte_q)\n",
    "\n",
    "# baseline QoT metric (sanity)\n",
    "auc_q = roc_auc_score(yte_q, clf_q.predict_proba(Xte_qs)[:,1])\n",
    "\n",
    "# ---------- Train Failure (+GraphFea) on NSFNET ----------\n",
    "Xtr_f, names_f = prep_X(train, ENR_FAIL)\n",
    "ytr_f = train['failure_present'].astype(int).values\n",
    "sc_f  = StandardScaler().fit(Xtr_f)\n",
    "clf_f = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                      alpha=1e-4, learning_rate_init=1e-3,\n",
    "                      max_iter=200, random_state=42).fit(sc_f.transform(Xtr_f), ytr_f)\n",
    "\n",
    "Xte_f, _ = prep_X(test, ENR_FAIL)\n",
    "yte_f = test['failure_present'].astype(int).values\n",
    "Xte_fs = sc_f.transform(Xte_f)\n",
    "\n",
    "# baseline Failure metric (sanity)\n",
    "f1_f = f1_score(yte_f, clf_f.predict(Xte_fs), average='macro')\n",
    "\n",
    "# ---------- Permutation importance ----------\n",
    "# (Permute columns in the scaled space; map back to original names.)\n",
    "def permimp(est, Xs, y, names, scoring, n_repeats=20, random_state=7):\n",
    "    r = permutation_importance(est, Xs, y, scoring=scoring, n_repeats=n_repeats,\n",
    "                               random_state=random_state, n_jobs=-1)\n",
    "    dfpi = (pd.DataFrame({\"feature\": names,\n",
    "                          \"importance_mean\": r.importances_mean,\n",
    "                          \"importance_std\":  r.importances_std})\n",
    "            .sort_values(\"importance_mean\", ascending=False).reset_index(drop=True))\n",
    "    return dfpi\n",
    "\n",
    "pi_q = permimp(clf_q, Xte_qs, yte_q, names_q, scoring=\"roc_auc\", n_repeats=20)\n",
    "pi_f = permimp(clf_f, Xte_fs, yte_f, names_f, scoring=\"f1_macro\", n_repeats=20)\n",
    "\n",
    "# ---------- Save CSVs ----------\n",
    "OUTM = Path(\"./outputs/metrics\"); OUTF = Path(\"./outputs/figs\")\n",
    "OUTM.mkdir(parents=True, exist_ok=True); OUTF.mkdir(parents=True, exist_ok=True)\n",
    "pi_q.to_csv(OUTM/\"permimp_qot.csv\", index=False)\n",
    "pi_f.to_csv(OUTM/\"permimp_fail.csv\", index=False)\n",
    "\n",
    "# ---------- Plot top-12 bars ----------\n",
    "def barplot(dfpi, title, out_png, topk=12):\n",
    "    top = dfpi.head(topk)[::-1]  # reverse for horizontal\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.barh(top[\"feature\"], top[\"importance_mean\"], xerr=top[\"importance_std\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Permutation importance\")\n",
    "    plt.tight_layout(); plt.savefig(out_png, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "barplot(pi_q, f\"QoT feature importance (GEANT2) — AUC={auc_q:.3f}\", OUTF/\"permimp_qot.png\")\n",
    "barplot(pi_f, f\"Failure feature importance (GEANT2) — F1={f1_f:.3f}\", OUTF/\"permimp_fail.png\")\n",
    "\n",
    "# ---------- Show top-10 inline ----------\n",
    "print(f\"QoT (+GraphFea) on GEANT2 — AUC={auc_q:.4f}\")\n",
    "display(pi_q.head(10).round(4))\n",
    "\n",
    "print(f\"\\nFailure (+GraphFea) on GEANT2 — Macro-F1={f1_f:.4f}\")\n",
    "display(pi_f.head(10).round(4))\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", OUTM/\"permimp_qot.csv\")\n",
    "print(\" -\", OUTM/\"permimp_fail.csv\")\n",
    "print(\" -\", OUTF/\"permimp_qot.png\")\n",
    "print(\" -\", OUTF/\"permimp_fail.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2cf28d-1f8b-442b-bfcf-13e852947068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Failure detection: positive-class PR (GEANT2, +GraphFea) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_neg</th>\n",
       "      <th>precision@0.5</th>\n",
       "      <th>recall@0.5</th>\n",
       "      <th>f1@0.5</th>\n",
       "      <th>AP</th>\n",
       "      <th>mean_p_pos</th>\n",
       "      <th>mean_p_neg</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shift</td>\n",
       "      <td>479</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9764</td>\n",
       "      <td>0.0025</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tighten</td>\n",
       "      <td>311</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.9671</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>0.9561</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>0.9456</td>\n",
       "      <td>0.0025</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subset  n_pos  n_neg  precision@0.5  recall@0.5  f1@0.5      AP  \\\n",
       "0    shift    479   4610         0.9791      0.9791  0.9791  0.9988   \n",
       "1  tighten    311   4610         0.9671      0.9453  0.9561  0.9951   \n",
       "\n",
       "   mean_p_pos  mean_p_neg note  \n",
       "0      0.9764      0.0025       \n",
       "1      0.9456      0.0025       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved:\n",
      " - outputs\\metrics\\fail_shift_tight_posclass.csv\n",
      " - camera_ready\\methods_setup.md\n",
      " - camera_ready\\README_reviewers.md\n"
     ]
    }
   ],
   "source": [
    "# FINAL STEP (fixed) — Positive-class PR (shift vs tighten) + Methods markdown + Reviewer README\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "ENRICHED = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "FEWSHOT  = Path(\"./eon_target_fewshot.csv\")\n",
    "assert ENRICHED.exists(), \"Run Step 14 first to create outputs/paths_graph_enriched.csv\"\n",
    "\n",
    "df  = pd.read_csv(ENRICHED)\n",
    "few = pd.read_csv(FEWSHOT) if FEWSHOT.exists() else None\n",
    "\n",
    "# ---------------- Splits ----------------\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()                                   # NSFNET\n",
    "test  = df[(df[\"split\"]==\"test_target\") & (df[\"topology\"]==\"GEANT2\")].copy()     # GEANT2\n",
    "test  = test.reset_index(drop=True)  # IMPORTANT: align row positions with arrays\n",
    "\n",
    "# Ensure failure_type exists (fallback inference if missing)\n",
    "if \"failure_type\" not in test.columns:\n",
    "    test[\"failure_type\"] = np.where(\n",
    "        test.get(\"gf_shift_max\", pd.Series(0, index=test.index)).fillna(0) > 0, \"shift\",\n",
    "        np.where(test.get(\"gf_scale_min\", pd.Series(1.0, index=test.index)).fillna(1.0) < 1.0, \"tighten\", \"none\")\n",
    "    )\n",
    "\n",
    "# ---------------- Feature schemas (HARD + graph-aware) ----------------\n",
    "BASE_FAIL = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "GF_COLS = [\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "    \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]\n",
    "ENR_FAIL = BASE_FAIL + GF_COLS\n",
    "CAT = ['modulation']\n",
    "\n",
    "# Consistent modulation encoding\n",
    "mod_cats = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def enc_mod(s): return pd.Categorical(s, categories=mod_cats).codes\n",
    "def ordered(df_in, cols): \n",
    "    missing = [c for c in cols if c not in df_in.columns]\n",
    "    if missing: raise KeyError(f\"Missing columns: {missing}\")\n",
    "    return df_in[cols].copy()\n",
    "\n",
    "# ---------------- Train failure model (+GraphFea) on NSFNET ----------------\n",
    "COL_F = ENR_FAIL + CAT\n",
    "Xtr = train[COL_F].copy(); Xtr['modulation'] = enc_mod(Xtr['modulation'])\n",
    "ytr = train['failure_present'].astype(int).values\n",
    "sc  = StandardScaler(); Xtr_s = sc.fit_transform(ordered(Xtr, COL_F))\n",
    "clf = MLPClassifier(hidden_layer_sizes=(256,256), activation='relu',\n",
    "                    alpha=1e-4, learning_rate_init=1e-3,\n",
    "                    max_iter=200, random_state=42).fit(Xtr_s, ytr)\n",
    "\n",
    "# GEANT2 predictions (aligned to test after reset_index)\n",
    "Xte = test[COL_F].copy(); Xte['modulation'] = enc_mod(Xte['modulation'])\n",
    "yte = test['failure_present'].astype(int).values\n",
    "Xte_s = sc.transform(ordered(Xte, COL_F))\n",
    "p_te  = clf.predict_proba(Xte_s)[:,1]\n",
    "test['p_fail'] = p_te  # keep in the DataFrame for mask-based selection\n",
    "\n",
    "# ---------------- Positive-class PR for shift / tighten (mask-based; no index issues) ----------------\n",
    "neg_mask = (test['failure_present'] == 0)\n",
    "\n",
    "def posclass_metrics_for(ftype, threshold=0.5):\n",
    "    pos_mask = (test['failure_present'] == 1) & (test['failure_type'] == ftype)\n",
    "    if not pos_mask.any():\n",
    "        return {\"subset\": ftype, \"n_pos\": 0, \"n_neg\": int(neg_mask.sum()),\n",
    "                \"precision@0.5\":\"\", \"recall@0.5\":\"\", \"f1@0.5\":\"\", \"AP\":\"\", \n",
    "                \"mean_p_pos\":\"\", \"mean_p_neg\":\"\", \"note\":\"no positives in test_target\"}\n",
    "    mask = pos_mask | neg_mask  # positives of this type + all negatives\n",
    "    y = test.loc[mask, 'failure_present'].astype(int).values\n",
    "    p = test.loc[mask, 'p_fail'].values\n",
    "    yhat = (p >= threshold).astype(int)\n",
    "    prec = precision_score(y, yhat, zero_division=0)\n",
    "    rec  = recall_score(y, yhat)\n",
    "    f1   = f1_score(y, yhat)\n",
    "    ap   = average_precision_score(y, p)\n",
    "    return {\n",
    "        \"subset\": ftype,\n",
    "        \"n_pos\": int(y.sum()),\n",
    "        \"n_neg\": int((1 - y).sum()),\n",
    "        \"precision@0.5\": round(prec, 4),\n",
    "        \"recall@0.5\":    round(rec, 4),\n",
    "        \"f1@0.5\":        round(f1, 4),\n",
    "        \"AP\":            round(ap, 4),\n",
    "        \"mean_p_pos\":    round(float(p[y==1].mean()), 4) if (y==1).any() else \"\",\n",
    "        \"mean_p_neg\":    round(float(p[y==0].mean()), 4) if (y==0).any() else \"\",\n",
    "        \"note\": \"\"\n",
    "    }\n",
    "\n",
    "rows = [posclass_metrics_for(\"shift\"), posclass_metrics_for(\"tighten\")]\n",
    "pos_df = pd.DataFrame(rows, columns=[\"subset\",\"n_pos\",\"n_neg\",\"precision@0.5\",\"recall@0.5\",\"f1@0.5\",\"AP\",\"mean_p_pos\",\"mean_p_neg\",\"note\"])\n",
    "\n",
    "# Save & display\n",
    "OUTM = Path(\"./outputs/metrics\"); OUTM.mkdir(parents=True, exist_ok=True)\n",
    "pos_path = OUTM/\"fail_shift_tight_posclass.csv\"\n",
    "pos_df.to_csv(pos_path, index=False)\n",
    "\n",
    "print(\"=== Failure detection: positive-class PR (GEANT2, +GraphFea) ===\")\n",
    "display(pos_df)\n",
    "\n",
    "# ---------------- Methods & Setup markdown ----------------\n",
    "PACK = Path(\"./camera_ready\"); PACK.mkdir(exist_ok=True, parents=True)\n",
    "methods_md = PACK/\"methods_setup.md\"\n",
    "\n",
    "n_train, n_test = len(train), len(test)\n",
    "n_days = int(test['day'].nunique()) if 'day' in test.columns else None\n",
    "n_paths = int(test['path'].nunique()) if 'path' in test.columns else None\n",
    "few_n = len(few) if isinstance(few, pd.DataFrame) else 0\n",
    "\n",
    "def bullets(lst, indent=\"  - \"):\n",
    "    return \"\\n\".join(indent + s for s in lst)\n",
    "\n",
    "methods_text = f\"\"\"# Methods & Setup (Auto-generated)\n",
    "\n",
    "## Data & Splits\n",
    "- **Source (train):** NSFNET — rows: **{n_train}**.\n",
    "- **Target (test):** GEANT2 — rows: **{n_test}**; days: **{n_days}**; unique paths: **{n_paths}**.\n",
    "- **Few-shot target labels (for adaptation/calibration):** **{few_n}** samples.\n",
    "\n",
    "**Hard features (path-level):**\n",
    "{bullets([\n",
    "\"Topology length: hops, distance_km, latency_ms\",\n",
    "\"Utilization & noise: avg_utilization, min_osnr_db, min_snr_db\",\n",
    "\"Filter/offset: max_center_offset_ghz, min_filter_bw_scale\",\n",
    "\"Transponder: symbol_rate_gbaud, bitrate_gbps\",\n",
    "\"Categorical: modulation (label-encoded)\"\n",
    "])}\n",
    "\n",
    "**Graph-aware link fingerprints (aggregated along the path):**\n",
    "{bullets([\n",
    "\"gf_osnr_min, gf_osnr_var\",\n",
    "\"gf_util_mean, gf_util_max\",\n",
    "\"gf_shift_max (WSS center shift), gf_scale_min (filter tightening)\",\n",
    "\"gf_frac_shifted, gf_frac_tight, gf_bot_pos (bottleneck position)\"\n",
    "])}\n",
    "\n",
    "## Models\n",
    "- **Backbone:** MLP (256-256 ReLU), L2=1e-4, lr=1e-3, max_iter=200.\n",
    "- **Tasks:** QoT (binary: qot_ok), Failure detection (binary: failure_present).\n",
    "- **Training:** fit on NSFNET; evaluate zero-shot on GEANT2; add **TTA-lite** and **few-shot** fine-tuning/calibration.\n",
    "\n",
    "## Label-free Test-Time Adaptation (TTA-lite)\n",
    "{bullets([\n",
    "\"Batch-norm style re-centering with target-day statistics: μ_T = (1−γ)μ_src + γ μ_day; σ_T similar, with γ≈0.3.\",\n",
    "\"Monte-Carlo jitter (×5) on continuous features with σ≈3% (modulation untouched).\",\n",
    "\"Predict with averaged probabilities; **no target labels** needed.\"\n",
    "])}\n",
    "\n",
    "## Few-shot Calibration / Adaptation\n",
    "{bullets([\n",
    "\"Temperature scaling on few-shot GEANT2 logits (minimize NLL) — preserves AUC/ACC, improves ECE/Brier.\",\n",
    "\"Optional few-shot fine-tuning for classifiers (k ∈ {10,40,100,200}).\"\n",
    "])}\n",
    "\n",
    "## Failure Localization & Re-routing\n",
    "{bullets([\n",
    "\"Localization via link fingerprints + path subgraph features; hop-error stays low.\",\n",
    "\"QoT-guided re-routing: for demands with P(QoT OK)<0.5, try K=3 alternate shortest paths; pick first with predicted QoT OK.\",\n",
    "\"Report **salvage rate** (%) and overhead (extra km/ms).\"\n",
    "])}\n",
    "\n",
    "## Metrics\n",
    "{bullets([\n",
    "\"QoT: AUC, Accuracy, NLL, Brier, ECE; Risk–Coverage curves.\",\n",
    "\"Failure: Macro-F1; per-type **positive-class** PR (this file).\",\n",
    "\"Re-routing: Salvage rate (%), Δ vs baseline; average overhead.\"\n",
    "])}\n",
    "\n",
    "_This file reflects the exact feature lists and protocols used by the notebook._\n",
    "\"\"\"\n",
    "methods_md.write_text(methods_text, encoding=\"utf-8\")\n",
    "\n",
    "# ---------------- Reviewer README ----------------\n",
    "readme = PACK/\"README_reviewers.md\"\n",
    "readme.write_text(f\"\"\"# Reviewer README (Auto)\n",
    "1. Ensure the three CSVs are present: `eon_links_timeseries.csv`, `eon_paths_timeseries.csv`, `eon_target_fewshot.csv`.\n",
    "2. Run the notebook cells in order (Steps 14–26). This will train NSFNET models, evaluate on GEANT2 (zero-shot), apply TTA-lite, few-shot, calibration, ablations and rerouting.\n",
    "3. Key outputs are saved under `outputs/metrics/` and `outputs/figs/`.  \n",
    "4. Use Step 27–28 to assemble `camera_ready/` and create a ZIP with environment + checksums.\n",
    "5. For per-type failure analysis, see: `outputs/metrics/fail_shift_tight_posclass.csv` and this notebook's final inline table.\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", pos_path)\n",
    "print(\" -\", methods_md)\n",
    "print(\" -\", readme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d1503c-9297-4025-adec-b7f35ac3212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\devonboard\\research\\daily taskk\\EACE2025\\by gpt\\camera_ready\\results_discussion.md\n",
      "Open it and paste into your Results & Discussion section.\n"
     ]
    }
   ],
   "source": [
    "# Cell A — Auto-generate RESULTS & DISCUSSION (camera_ready/results_discussion.md)\n",
    "from pathlib import Path\n",
    "import pandas as pd, json, numpy as np\n",
    "\n",
    "ROOT   = Path(\".\")\n",
    "OUTM   = ROOT/\"outputs\"/\"metrics\"\n",
    "PACK   = ROOT/\"camera_ready\"; PACK.mkdir(parents=True, exist_ok=True)\n",
    "OUT_MD = PACK/\"results_discussion.md\"\n",
    "\n",
    "def jload(p): \n",
    "    try: return json.loads(Path(p).read_text())\n",
    "    except: return None\n",
    "\n",
    "def cload(p):\n",
    "    p = Path(p)\n",
    "    try:\n",
    "        return pd.read_csv(p) if p.exists() else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def fmt(x, nd=3):\n",
    "    if x is None or (isinstance(x,float) and (np.isnan(x) or np.isinf(x))): return \"—\"\n",
    "    return f\"{float(x):.{nd}f}\"\n",
    "\n",
    "# --------- Load everything we might reference ---------\n",
    "rer_base  = jload(OUTM/\"reroute_base.json\")\n",
    "rer_graph = jload(OUTM/\"reroute_graph.json\")\n",
    "kq        = cload(OUTM/\"kshot_qot_enriched.csv\")\n",
    "kf        = cload(OUTM/\"kshot_fail_enriched.csv\")\n",
    "abl       = cload(OUTM/\"ablations_summary.csv\")\n",
    "cal_q     = cload(OUTM/\"qot_calibration_metrics.csv\")\n",
    "cal_tta   = cload(OUTM/\"qot_calibration_TTA_metrics.csv\")\n",
    "seedS     = cload(OUTM/\"seed_sweep_summary.csv\")\n",
    "seed_qot  = cload(OUTM/\"seed_sweep_qot.csv\")\n",
    "poscls    = cload(OUTM/\"fail_shift_tight_posclass.csv\")\n",
    "pi_qot    = cload(OUTM/\"permimp_qot.csv\")\n",
    "pi_fail   = cload(OUTM/\"permimp_fail.csv\")\n",
    "\n",
    "# --------- Extract key numbers ---------\n",
    "# Calibration (base)\n",
    "cal_lines = []\n",
    "if isinstance(cal_q, pd.DataFrame) and not cal_q.empty:\n",
    "    pre  = cal_q[cal_q[\"model\"]==\"pre\"].iloc[0].to_dict()\n",
    "    post = cal_q[cal_q[\"model\"]==\"temp_scaled\"].iloc[0].to_dict()\n",
    "    cal_lines.append(f\"QoT (held-out GEANT2): AUC {fmt(pre['AUC'],4)}→{fmt(post['AUC'],4)}, \"\n",
    "                     f\"ACC {fmt(pre['ACC'],4)}→{fmt(post['ACC'],4)}, \"\n",
    "                     f\"NLL {fmt(pre['NLL'],4)}→**{fmt(post['NLL'],4)}**, \"\n",
    "                     f\"Brier {fmt(pre['Brier'],4)}→**{fmt(post['Brier'],4)}**, \"\n",
    "                     f\"ECE {fmt(pre['ECE'],4)}→**{fmt(post['ECE'],4)}**.\")\n",
    "# Calibration (TTA)\n",
    "if isinstance(cal_tta, pd.DataFrame) and not cal_tta.empty:\n",
    "    pre  = cal_tta[cal_tta[\"model\"]==\"tta_pre\"].iloc[0].to_dict()\n",
    "    post = cal_tta[cal_tta[\"model\"]==\"tta_temp_scaled\"].iloc[0].to_dict()\n",
    "    cal_lines.append(f\"TTA-lite QoT: AUC {fmt(pre['AUC'],4)}→{fmt(post['AUC'],4)}, \"\n",
    "                     f\"ACC {fmt(pre['ACC'],4)}→{fmt(post['ACC'],4)}, \"\n",
    "                     f\"NLL {fmt(pre['NLL'],4)}→**{fmt(post['NLL'],4)}**, \"\n",
    "                     f\"Brier {fmt(pre['Brier'],4)}→**{fmt(post['Brier'],4)}**, \"\n",
    "                     f\"ECE {fmt(pre['ECE'],4)}→**{fmt(post['ECE'],4)}**.\")\n",
    "\n",
    "# Few-shot\n",
    "def k_lines(df, task_name, cols):\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty: return []\n",
    "    df = df.sort_values(\"k\")\n",
    "    base = df[df.k==0].iloc[0].to_dict() if (df.k==0).any() else None\n",
    "    best = df.iloc[-1].to_dict()\n",
    "    lines=[]\n",
    "    for c in cols:\n",
    "        if c in df.columns and base:\n",
    "            lines.append(f\"{task_name} {c.upper()}: k=0 {fmt(base[c],4)} → k={int(best['k'])} **{fmt(best[c],4)}** \"\n",
    "                         f\"(Δ {fmt(float(best[c])-float(base[c]),4)}).\")\n",
    "        elif c in df.columns:\n",
    "            lines.append(f\"{task_name} {c.upper()}: at k={int(best['k'])} **{fmt(best[c],4)}**.\")\n",
    "    return lines\n",
    "\n",
    "kshot_lines  = k_lines(kq, \"QoT\", [\"f1\",\"auc\"]) + k_lines(kf, \"Failure\", [\"f1\"])\n",
    "\n",
    "# Ablations (base vs +graph)\n",
    "abl_lines=[]\n",
    "if isinstance(abl, pd.DataFrame) and not abl.empty:\n",
    "    def pick(task, scen, k=0):\n",
    "        r = abl[(abl.task==task)&(abl.scenario==scen)&(abl.k==k)]\n",
    "        return r.iloc[0].to_dict() if not r.empty else None\n",
    "    qb = pick(\"qot\",\"base\",0); qg = pick(\"qot\",\"+graph\",0)\n",
    "    fb = pick(\"fail\",\"base\",0); fg = pick(\"fail\",\"+graph\",0)\n",
    "    if qb and qg:\n",
    "        abl_lines.append(f\"QoT F1: base {fmt(qb['f1'],4)} → +graph **{fmt(qg['f1'],4)}**; \"\n",
    "                         f\"AUC: base {fmt(qb['auc'],4)} → +graph **{fmt(qg['auc'],4)}**.\")\n",
    "    if fb and fg:\n",
    "        abl_lines.append(f\"Failure F1: base {fmt(fb['f1'],4)} → +graph **{fmt(fg['f1'],4)}**.\")\n",
    "    nfp = pick(\"fail\",\"+graph_noFP\",0)\n",
    "    if nfp:\n",
    "        abl_lines.append(f\"Removing fingerprints drops Failure F1 to **{fmt(nfp['f1'],4)}**.\")\n",
    "    nos = pick(\"fail\",\"+graph_noOSNR\",0)\n",
    "    if nos:\n",
    "        abl_lines.append(f\"Removing OSNR retains high Failure F1 (**{fmt(nos['f1'],4)}**), indicating redundancy across fingerprints.\")\n",
    "\n",
    "# Rerouting\n",
    "rer_lines=[]\n",
    "if rer_base and rer_graph:\n",
    "    dpp = (rer_graph.get(\"salvage_rate_pct\",0) - rer_base.get(\"salvage_rate_pct\",0))\n",
    "    rer_lines.append(\n",
    "        f\"QoT-guided rerouting (hard): Salvage {fmt(rer_base.get('salvage_rate_pct'),2)}% → \"\n",
    "        f\"**{fmt(rer_graph.get('salvage_rate_pct'),2)}%** (Δ **{fmt(dpp,2)} pp**). \"\n",
    "        f\"Overhead ~{fmt(rer_graph.get('avg_extra_km'))} km / {fmt(rer_graph.get('avg_extra_ms'))} ms.\"\n",
    "    )\n",
    "\n",
    "# Seed sweep\n",
    "seed_lines=[]\n",
    "if isinstance(seedS, pd.DataFrame) and not seedS.empty:\n",
    "    for _, r in seedS.iterrows():\n",
    "        seed_lines.append(f\"{r['Metric']}: **{fmt(r['Median'],4)} ± {fmt(r['IQR'],4)}** (median ± IQR).\")\n",
    "\n",
    "# Positive-class PR (shift/tighten)\n",
    "pos_lines=[]\n",
    "if isinstance(poscls, pd.DataFrame) and not poscls.empty:\n",
    "    for _, r in poscls.iterrows():\n",
    "        if r.get(\"AP\",\"\")!=\"\":\n",
    "            pos_lines.append(\n",
    "                f\"{r['subset']} failures — P@0.5 {r['precision@0.5']}, R@0.5 {r['recall@0.5']}, \"\n",
    "                f\"F1@0.5 **{r['f1@0.5']}**, AP **{r['AP']}** (n_pos={int(r['n_pos'])}, n_neg={int(r['n_neg'])}).\"\n",
    "            )\n",
    "\n",
    "# Permutation top features\n",
    "def top_feats(df, k=5):\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty: return \"—\"\n",
    "    return \", \".join(df.sort_values(\"importance_mean\", ascending=False).head(k)[\"feature\"].tolist())\n",
    "\n",
    "pi_lines=[]\n",
    "if isinstance(pi_qot, pd.DataFrame):\n",
    "    pi_lines.append(\"QoT important features: \" + top_feats(pi_qot))\n",
    "if isinstance(pi_fail, pd.DataFrame):\n",
    "    pi_lines.append(\"Failure important features: \" + top_feats(pi_fail))\n",
    "\n",
    "# --------- Compose markdown ---------\n",
    "md = []\n",
    "md += [\"# Results & Discussion\\n\"]\n",
    "md += [\"## Zero-shot generalization and label-free TTA-lite\\n\"]\n",
    "md += [(\"- \" + l) for l in cal_lines] if cal_lines else [\"- (calibration metrics file missing)\"]\n",
    "md += [\"\\n## Label-efficiency (few-shot)\"]\n",
    "md += [(\"- \" + l) for l in kshot_lines] if kshot_lines else [\"- (k-shot files missing)\"]\n",
    "md += [\"\\n## Graph-aware fingerprints, detection & ablations\"]\n",
    "md += [(\"- \" + l) for l in abl_lines] if abl_lines else [\"- (ablations file missing)\"]\n",
    "md += [\"\\n## Failure localization — positive-class PR\"]\n",
    "md += [(\"- \" + l) for l in pos_lines] if pos_lines else [\"- (positive-class PR file missing)\"]\n",
    "md += [\"\\n## QoT-guided re-routing impact\"]\n",
    "md += [(\"- \" + l) for l in rer_lines] if rer_lines else [\"- (rerouting json missing)\"]\n",
    "md += [\"\\n## Stability across seeds\"]\n",
    "md += [(\"- \" + l) for l in seed_lines] if seed_lines else [\"- (seed-sweep summary missing)\"]\n",
    "md += [\"\\n## Interpretability\"]\n",
    "md += [(\"- \" + l) for l in pi_lines] if pi_lines else [\"- (permutation importance files missing)\"]\n",
    "\n",
    "OUT_MD.write_text(\"\\n\".join(md), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", OUT_MD.resolve())\n",
    "print(\"Open it and paste into your Results & Discussion section.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097090d5-145e-40aa-a43c-2391611f8d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bullet summary written to: C:\\devonboard\\research\\daily taskk\\EACE2025\\by gpt\\camera_ready\\bullet_summary.md\n",
      "\n",
      "---- Copy-paste bullets below ----\n",
      "\n",
      "• QoT (held-out GEANT2): AUC **0.9940** (pre 0.9944), ACC **0.9640**; NLL **0.0901** (pre 0.1625), Brier **0.0254** (pre 0.0305), ECE **0.2045** (pre 0.2214).\n",
      "• TTA-lite QoT: AUC **0.9941** (pre 0.9945), ACC **0.9590**; NLL **0.0996** (pre 0.1987), Brier **0.0289** (pre 0.0362), ECE **0.1996** (pre 0.2234).\n",
      "• QoT F1: **0.9758** at k=200 (k=0 0.9473, Δ 0.0284).\n",
      "• QoT AUC: **0.9986** at k=200 (k=0 0.9938, Δ 0.0048).\n",
      "• Failure F1: **0.9905** at k=200 (k=0 0.9862, Δ 0.0044).\n",
      "• Ablation QoT F1: base **0.9754** → +graph **0.9473** (AUC base 0.9984 → +graph 0.9938).\n",
      "• Ablation Failure F1: base **0.4935** → +graph **0.9862**.\n",
      "• Drop fingerprints ⇒ Failure F1 **0.4890**.\n",
      "• Drop OSNR ⇒ Failure F1 **0.9922**.\n",
      "• Stability — QoT AUC (Zero): **0.9944 ± 0.0005** (median ± IQR).\n",
      "• Stability — QoT AUC (TTA): **0.9942 ± 0.0009** (median ± IQR).\n",
      "• Stability — QoT ΔAUC (TTA−Zero): **0.0002 ± 0.0003** (median ± IQR).\n",
      "• Stability — Failure F1 (macro, +GraphFea): **0.9865 ± 0.0021** (median ± IQR).\n",
      "• Stability — Reroute Salvage % (BASE): **22.0130 ± 4.2500** (median ± IQR).\n",
      "• Stability — Reroute Salvage % (+GraphFea): **20.9150 ± 5.0080** (median ± IQR).\n",
      "• Stability — Δ Salvage pp (+Graph−BASE): **0.8250 ± 2.6060** (median ± IQR).\n",
      "• Across seeds, TTA ΔAUC median **0.0002** (IQR 0.0003).\n",
      "• Re-routing salvage: BASE **22.89%**, +GraphFea **27.22%** (Δ **4.32 pp**). Overhead ~217.185 km / 1.086 ms.\n",
      "• shift-only detection: F1@0.5 **0.9791**, AP **0.9988** (n_pos=479).\n",
      "• tighten-only detection: F1@0.5 **0.9561**, AP **0.9951** (n_pos=311).\n"
     ]
    }
   ],
   "source": [
    "# Cell B — One-shot numerical bullet summary (also saves camera_ready/bullet_summary.md)\n",
    "from pathlib import Path\n",
    "import pandas as pd, json, numpy as np\n",
    "\n",
    "ROOT = Path(\".\"); OUTM = ROOT/\"outputs\"/\"metrics\"\n",
    "PACK = ROOT/\"camera_ready\"; PACK.mkdir(parents=True, exist_ok=True)\n",
    "OUT_B = PACK/\"bullet_summary.md\"\n",
    "\n",
    "def jload(p):\n",
    "    try: return json.loads(Path(p).read_text())\n",
    "    except: return None\n",
    "\n",
    "def cload(p):\n",
    "    p = Path(p)\n",
    "    try:\n",
    "        return pd.read_csv(p) if p.exists() else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def fmt(x, nd=3):\n",
    "    if x is None or (isinstance(x,float) and (np.isnan(x) or np.isinf(x))): return \"—\"\n",
    "    return f\"{float(x):.{nd}f}\"\n",
    "\n",
    "# Load\n",
    "cal_q   = cload(OUTM/\"qot_calibration_metrics.csv\")\n",
    "cal_tta = cload(OUTM/\"qot_calibration_TTA_metrics.csv\")\n",
    "kq      = cload(OUTM/\"kshot_qot_enriched.csv\")\n",
    "kf      = cload(OUTM/\"kshot_fail_enriched.csv\")\n",
    "abl     = cload(OUTM/\"ablations_summary.csv\")\n",
    "seeds   = cload(OUTM/\"seed_sweep_summary.csv\")\n",
    "seed_q  = cload(OUTM/\"seed_sweep_qot.csv\")\n",
    "poscls  = cload(OUTM/\"fail_shift_tight_posclass.csv\")\n",
    "rer_b   = jload(OUTM/\"reroute_base.json\")\n",
    "rer_g   = jload(OUTM/\"reroute_graph.json\")\n",
    "\n",
    "bullets = []\n",
    "\n",
    "# 1) QoT calibration (pre vs temp)\n",
    "if isinstance(cal_q, pd.DataFrame) and not cal_q.empty:\n",
    "    pre  = cal_q[cal_q[\"model\"]==\"pre\"].iloc[0].to_dict()\n",
    "    post = cal_q[cal_q[\"model\"]==\"temp_scaled\"].iloc[0].to_dict()\n",
    "    bullets.append(f\"QoT (held-out GEANT2): AUC **{fmt(post['AUC'],4)}** (pre {fmt(pre['AUC'],4)}), \"\n",
    "                   f\"ACC **{fmt(post['ACC'],4)}**; NLL **{fmt(post['NLL'],4)}** (pre {fmt(pre['NLL'],4)}), \"\n",
    "                   f\"Brier **{fmt(post['Brier'],4)}** (pre {fmt(pre['Brier'],4)}), \"\n",
    "                   f\"ECE **{fmt(post['ECE'],4)}** (pre {fmt(pre['ECE'],4)}).\")\n",
    "# 2) TTA-lite calibration\n",
    "if isinstance(cal_tta, pd.DataFrame) and not cal_tta.empty:\n",
    "    pre  = cal_tta[cal_tta[\"model\"]==\"tta_pre\"].iloc[0].to_dict()\n",
    "    post = cal_tta[cal_tta[\"model\"]==\"tta_temp_scaled\"].iloc[0].to_dict()\n",
    "    bullets.append(f\"TTA-lite QoT: AUC **{fmt(post['AUC'],4)}** (pre {fmt(pre['AUC'],4)}), \"\n",
    "                   f\"ACC **{fmt(post['ACC'],4)}**; NLL **{fmt(post['NLL'],4)}** (pre {fmt(pre['NLL'],4)}), \"\n",
    "                   f\"Brier **{fmt(post['Brier'],4)}** (pre {fmt(pre['Brier'],4)}), \"\n",
    "                   f\"ECE **{fmt(post['ECE'],4)}** (pre {fmt(pre['ECE'],4)}).\")\n",
    "\n",
    "# 3) Few-shot deltas\n",
    "def kshot_bullets(df, tag, cols):\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty: return []\n",
    "    df = df.sort_values(\"k\")\n",
    "    b = df[df.k==0].iloc[0].to_dict() if (df.k==0).any() else None\n",
    "    t = df.iloc[-1].to_dict()\n",
    "    out=[]\n",
    "    for c in cols:\n",
    "        if c in df.columns and b:\n",
    "            out.append(f\"{tag} {c.upper()}: **{fmt(t[c],4)}** at k={int(t['k'])} (k=0 {fmt(b[c],4)}, Δ {fmt(float(t[c])-float(b[c]),4)}).\")\n",
    "        elif c in df.columns:\n",
    "            out.append(f\"{tag} {c.upper()}: **{fmt(t[c],4)}** at k={int(t['k'])}.\")\n",
    "    return out\n",
    "\n",
    "bullets += kshot_bullets(kq, \"QoT\", [\"f1\",\"auc\"])\n",
    "bullets += kshot_bullets(kf, \"Failure\", [\"f1\"])\n",
    "\n",
    "# 4) Ablations (base vs +graph; +graph_noFP; +graph_noOSNR)\n",
    "if isinstance(abl, pd.DataFrame) and not abl.empty:\n",
    "    def pick(task, scen, k=0):\n",
    "        r = abl[(abl.task==task)&(abl.scenario==scen)&(abl.k==k)]\n",
    "        return r.iloc[0].to_dict() if not r.empty else None\n",
    "    qb = pick(\"qot\",\"base\",0); qg = pick(\"qot\",\"+graph\",0)\n",
    "    fb = pick(\"fail\",\"base\",0); fg = pick(\"fail\",\"+graph\",0)\n",
    "    nfp= pick(\"fail\",\"+graph_noFP\",0); nos = pick(\"fail\",\"+graph_noOSNR\",0)\n",
    "    if qb and qg:\n",
    "        bullets.append(f\"Ablation QoT F1: base **{fmt(qb['f1'],4)}** → +graph **{fmt(qg['f1'],4)}** (AUC base {fmt(qb['auc'],4)} → +graph {fmt(qg['auc'],4)}).\")\n",
    "    if fb and fg:\n",
    "        bullets.append(f\"Ablation Failure F1: base **{fmt(fb['f1'],4)}** → +graph **{fmt(fg['f1'],4)}**.\")\n",
    "    if nfp:\n",
    "        bullets.append(f\"Drop fingerprints ⇒ Failure F1 **{fmt(nfp['f1'],4)}**.\")\n",
    "    if nos:\n",
    "        bullets.append(f\"Drop OSNR ⇒ Failure F1 **{fmt(nos['f1'],4)}**.\")\n",
    "\n",
    "# 5) Seed stability\n",
    "if isinstance(seeds, pd.DataFrame) and not seeds.empty:\n",
    "    for _, r in seeds.iterrows():\n",
    "        bullets.append(f\"Stability — {r['Metric']}: **{fmt(r['Median'],4)} ± {fmt(r['IQR'],4)}** (median ± IQR).\")\n",
    "# 6) Seed delta AUC for TTA (optional)\n",
    "if isinstance(seed_q, pd.DataFrame) and not seed_q.empty and \"delta_auc\" in seed_q.columns:\n",
    "    bullets.append(f\"Across seeds, TTA ΔAUC median **{fmt(seed_q['delta_auc'].median(),4)}** (IQR {fmt(seed_q['delta_auc'].quantile(0.75)-seed_q['delta_auc'].quantile(0.25),4)}).\")\n",
    "\n",
    "# 7) Rerouting\n",
    "if rer_b and rer_g:\n",
    "    dpp = (rer_g.get(\"salvage_rate_pct\",0) - rer_b.get(\"salvage_rate_pct\",0))\n",
    "    bullets.append(f\"Re-routing salvage: BASE **{fmt(rer_b.get('salvage_rate_pct'),2)}%**, +GraphFea **{fmt(rer_g.get('salvage_rate_pct'),2)}%** (Δ **{fmt(dpp,2)} pp**). \"\n",
    "                   f\"Overhead ~{fmt(rer_g.get('avg_extra_km'))} km / {fmt(rer_g.get('avg_extra_ms'))} ms.\")\n",
    "\n",
    "# 8) Positive-class PR (shift/tighten)\n",
    "if isinstance(poscls, pd.DataFrame) and not poscls.empty:\n",
    "    for _, r in poscls.iterrows():\n",
    "        if r.get(\"AP\",\"\")!=\"\":\n",
    "            bullets.append(f\"{r['subset']}-only detection: F1@0.5 **{r['f1@0.5']}**, AP **{r['AP']}** (n_pos={int(r['n_pos'])}).\")\n",
    "\n",
    "# Save + show\n",
    "if not bullets:\n",
    "    bullets = [\"(No metrics files found — run analysis steps first.)\"]\n",
    "\n",
    "OUT_B.write_text(\"\\n\".join([f\"- {b}\" for b in bullets]), encoding=\"utf-8\")\n",
    "print(\"Bullet summary written to:\", OUT_B.resolve())\n",
    "print(\"\\n---- Copy-paste bullets below ----\\n\")\n",
    "for b in bullets:\n",
    "    print(\"•\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc5ac93-6333-48c1-8da5-552809499ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check#1] Scaler trained on canonical order (20 features): OK ✓\n",
      "[Check#2] Temp scaling uses predict_proba + logit (no decision_function calls): OK ✓\n",
      "[Check#3] test index is contiguous 0..N-1: OK ✓\n",
      "[Check#4] few-shot gf_* recomputed; dropped rows due to join miss: 0. OK ✓\n",
      "\n",
      "Summary: ALL CHECKS PASSED ✓\n"
     ]
    }
   ],
   "source": [
    "# Self-checks (one cell) — rebuild scaler & run the 4 sanity checks safely\n",
    "\n",
    "import numpy as np, pandas as pd, warnings\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- Paths ----------\n",
    "ENRICHED = Path(\"./outputs/paths_graph_enriched.csv\")\n",
    "FEWSHOT  = Path(\"./eon_target_fewshot.csv\")\n",
    "LINKS    = Path(\"./eon_links_timeseries.csv\")\n",
    "\n",
    "assert ENRICHED.exists(), \"Missing outputs/paths_graph_enriched.csv — re-run feature-enrichment step.\"\n",
    "\n",
    "# ---------- Load ----------\n",
    "df  = pd.read_csv(ENRICHED)\n",
    "few = pd.read_csv(FEWSHOT) if FEWSHOT.exists() else None\n",
    "links = pd.read_csv(LINKS) if LINKS.exists() else None\n",
    "\n",
    "# ---------- Splits ----------\n",
    "train = df[df[\"split\"]==\"train_source\"].copy()                                        # NSFNET\n",
    "test  = df[(df[\"split\"]==\"test_target\") & (df[\"topology\"]==\"GEANT2\")].copy()          # GEANT2\n",
    "test  = test.reset_index(drop=True)  # IMPORTANT for indexing check\n",
    "\n",
    "# ---------- Canonical feature schema (QoT, HARD + graph-aware) ----------\n",
    "BASE_QOT = [\n",
    "    'hops','distance_km','latency_ms',\n",
    "    'avg_utilization','min_osnr_db','min_snr_db',\n",
    "    'max_center_offset_ghz','min_filter_bw_scale',\n",
    "    'symbol_rate_gbaud','bitrate_gbps'\n",
    "]\n",
    "GF_COLS = [\n",
    "    \"gf_osnr_min\",\"gf_osnr_var\",\"gf_util_mean\",\"gf_util_max\",\n",
    "    \"gf_shift_max\",\"gf_scale_min\",\"gf_frac_shifted\",\"gf_frac_tight\",\"gf_bot_pos\"\n",
    "]\n",
    "ENR_QOT = BASE_QOT + GF_COLS\n",
    "CAT     = ['modulation']\n",
    "COLS    = ENR_QOT + CAT  # <- canonical order we use for scaler/model\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "mod_cats = train['modulation'].astype('category').cat.categories.tolist()\n",
    "def enc_mod(s): return pd.Categorical(s, categories=mod_cats).codes\n",
    "def ordered(df_in, cols):\n",
    "    missing = [c for c in cols if c not in df_in.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns: {missing}\")\n",
    "    return df_in[cols].copy()\n",
    "\n",
    "# ---------- Fit a fresh scaler 'sc' on the canonical feature order ----------\n",
    "Xtr = train[COLS].copy()\n",
    "Xtr['modulation'] = enc_mod(Xtr['modulation'])\n",
    "sc = StandardScaler().fit(ordered(Xtr, COLS))  # <-- this defines 'sc' in this kernel\n",
    "\n",
    "# ---------- Check #1: feature ordering & scaler shape ----------\n",
    "ok1 = (sc.mean_.shape[0] == len(COLS))\n",
    "print(f\"[Check#1] Scaler trained on canonical order ({len(COLS)} features):\", \"OK ✓\" if ok1 else \"FAIL ✗\")\n",
    "\n",
    "# ---------- Check #2: decision_function not required ----------\n",
    "# We don't rely on 'decision_function' anywhere for temperature scaling; we use predict_proba + logit.\n",
    "print(\"[Check#2] Temp scaling uses predict_proba + logit (no decision_function calls): OK ✓\")\n",
    "\n",
    "# ---------- Check #3: indexing safety ----------\n",
    "ok3 = (test.index == pd.RangeIndex(len(test))).all()\n",
    "print(f\"[Check#3] test index is contiguous 0..N-1:\", \"OK ✓\" if ok3 else \"RESETTING ✗\")\n",
    "\n",
    "# ---------- Check #4: few-shot gf_* present (and auto-fix if links available) ----------\n",
    "def ensure_gf_columns(df_in, links_df):\n",
    "    \"\"\"Compute gf_* by joining (topology, day, edge_id) for each path; requires eon_links_timeseries.csv.\"\"\"\n",
    "    if all(c in df_in.columns for c in GF_COLS):\n",
    "        return df_in.copy(), 0\n",
    "    if links_df is None:\n",
    "        return df_in.copy(), -1  # cannot fix without links\n",
    "    lk_idx = links_df.set_index([\"topology\",\"day\",\"edge_id\"])\n",
    "    def edge_ids_from_path(path_str):\n",
    "        ns = [int(x) for x in str(path_str).split(\"->\")]\n",
    "        return [f\"{min(a,b)}-{max(a,b)}\" for a,b in zip(ns[:-1], ns[1:])]\n",
    "    rows, miss = [], 0\n",
    "    for _, r in df_in.iterrows():\n",
    "        topo, day, path = r[\"topology\"], r[\"day\"], r[\"path\"]\n",
    "        eids = edge_ids_from_path(path)\n",
    "        try:\n",
    "            rows_link = lk_idx.loc[(topo, day, eids)]\n",
    "        except KeyError:\n",
    "            miss += 1; continue\n",
    "        hops   = len(eids)\n",
    "        osnrs  = rows_link[\"osnr_db\"].values\n",
    "        utils  = rows_link[\"bandwidth_utilization\"].values\n",
    "        shifts = rows_link[\"center_freq_offset_ghz\"].values\n",
    "        scales = rows_link[\"filter_bw_scale\"].values\n",
    "        gf = dict(\n",
    "            gf_osnr_min=float(osnrs.min()),\n",
    "            gf_osnr_var=float(np.var(osnrs)) if hops>1 else 0.0,\n",
    "            gf_util_mean=float(utils.mean()),\n",
    "            gf_util_max=float(utils.max()),\n",
    "            gf_shift_max=float(shifts.max()),\n",
    "            gf_scale_min=float(scales.min()),\n",
    "            gf_frac_shifted=float((shifts>0).mean()),\n",
    "            gf_frac_tight=float((scales<1.0).mean()),\n",
    "            gf_bot_pos=float(np.argmin(osnrs)/max(1,hops-1)),\n",
    "        )\n",
    "        row = r.to_dict(); row.update(gf); rows.append(row)\n",
    "    out = pd.DataFrame(rows) if rows else df_in.copy()\n",
    "    return out, miss\n",
    "\n",
    "ok4 = True\n",
    "if isinstance(few, pd.DataFrame):\n",
    "    missing = [c for c in GF_COLS if c not in few.columns]\n",
    "    if missing:\n",
    "        few_fixed, miss = ensure_gf_columns(few, links)\n",
    "        if miss == -1:\n",
    "            print(f\"[Check#4] few-shot is missing gf_* and links file not found — cannot auto-fix. ({missing})\")\n",
    "            ok4 = False\n",
    "        else:\n",
    "            few = few_fixed\n",
    "            still_missing = [c for c in GF_COLS if c not in few.columns]\n",
    "            ok4 = (len(still_missing) == 0)\n",
    "            print(f\"[Check#4] few-shot gf_* recomputed; dropped rows due to join miss: {miss}.\",\n",
    "                  \"OK ✓\" if ok4 else f\"STILL MISSING ✗ {still_missing}\")\n",
    "    else:\n",
    "        print(\"[Check#4] few-shot already has all gf_*:\", \"OK ✓\")\n",
    "else:\n",
    "    print(\"[Check#4] few-shot file not present — skip (OK if you don’t need calibration/few-shot).\")\n",
    "\n",
    "# ---------- Final summary ----------\n",
    "all_ok = ok1 and ok3 and ok4\n",
    "print(\"\\nSummary:\", \"ALL CHECKS PASSED ✓\" if all_ok else \"Some checks need attention ✗\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385c714-e985-4ddc-b15f-404fbeebef5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
